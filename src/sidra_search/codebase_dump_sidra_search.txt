Project structure for '/c/Users/Galaxy/LEVI/projects/sidra-database/src/sidra_search':
===============================================================================
  __init__.py
  __main__.py
  cli/__init__.py
  cli/__main__.py
  config.py
  db/__init__.py
  db/base_schema.py
  db/migrations.py
  db/search_schema.py
  db/session.py
  ingest/__init__.py
  ingest/bulk.py
  ingest/ingest_table.py
  ingest/links.py
  net/__init__.py
  net/api_client.py
  net/embedding_client.py
  search/__init__.py
  search/coverage.py
  search/fuzzy3gram.py
  search/normalize.py
  search/tables.py
  search/title_rank.py
  smoke.sh
  util/__init__.py



###############################################################################
### FILE: __init__.py
###############################################################################
"""sidra_search — table-centric search (titles + var/class links + fuzzy)."""

from .db.migrations import apply_search_schema, get_search_schema_version  # noqa: F401
from .db.base_schema import apply_base_schema  # noqa: F401

__all__ = [
    "apply_base_schema",
    "apply_search_schema",
    "get_search_schema_version",
]



###############################################################################
### FILE: __main__.py
###############################################################################
# sidra_search/__main__.py
from .cli import main
if __name__ == "__main__":
    main()



###############################################################################
### FILE: cli/__init__.py
###############################################################################
from __future__ import annotations

import argparse
import asyncio
import json
from dataclasses import asdict
from typing import Sequence  # <- added
from array import array
import orjson
import hashlib

from ..config import get_settings
from ..db.session import ensure_full_schema, sqlite_session
from ..ingest.ingest_table import ingest_table
from ..ingest.bulk import ingest_by_coverage
from ..ingest.links import build_links_for_table
from ..net.embedding_client import EmbeddingClient
from ..search.tables import SearchArgs, search_tables
from ..search.fuzzy3gram import reset_cache
from ..db.session import sqlite_session
from ..ingest.ingest_table import _canonical_table_text  # reuse identical text

def _print_json(obj) -> None:
    print(json.dumps(obj, ensure_ascii=False, indent=2))


# ---------------------------
# DB admin
# ---------------------------
def _cmd_db_migrate(args: argparse.Namespace) -> None:
    ensure_full_schema()
    print("Database schema ensured (base + search).")


def _cmd_db_stats(args: argparse.Namespace) -> None:
    ensure_full_schema()
    with sqlite_session() as conn:
        counts = {}
        def c(table: str) -> int:
            try:
                return int(conn.execute(f"SELECT COUNT(*) FROM {table}").fetchone()[0])
            except Exception:
                return 0

        counts["agregados"] = c("agregados")
        counts["variables"] = c("variables")
        counts["classifications"] = c("classifications")
        counts["categories"] = c("categories")
        counts["periods"] = c("periods")
        counts["agregados_levels"] = c("agregados_levels")
        counts["localities"] = c("localities")
        counts["name_keys"] = c("name_keys")
        counts["link_var"] = c("link_var")
        counts["link_class"] = c("link_class")
        counts["link_cat"] = c("link_cat")
        counts["link_var_class"] = c("link_var_class")
        counts["table_titles_fts"] = c("table_titles_fts")
        try:
            counts["embeddings_agregado"] = int(
                conn.execute(
                    "SELECT COUNT(*) FROM embeddings WHERE entity_type='agregado'"
                ).fetchone()[0]
            )
        except Exception:
            counts["embeddings_agregado"] = 0

    _print_json(counts)


# ---------------------------
# Ingest
# ---------------------------
def _cmd_ingest(args: argparse.Namespace) -> None:
    ensure_full_schema()
    async def run():
        for tid in args.table_ids:
            try:
                await ingest_table(int(tid))
                print(f"ingested {tid}")
            except Exception as exc:
                print(f"failed {tid}: {exc}")
    asyncio.run(run())


def _cmd_ingest_coverage(args: argparse.Namespace) -> None:
    ensure_full_schema()
    report = asyncio.run(
        ingest_by_coverage(
            coverage=args.coverage,
            subject_contains=args.subject_contains,
            survey_contains=args.survey_contains,
            limit=args.limit,
            concurrency=args.concurrent,
            probe_concurrent=args.probe_concurrent,
        )
    )
    _print_json(asdict(report))


# ---------------------------
# Index / links
# ---------------------------
def _all_table_ids() -> list[int]:
    with sqlite_session() as conn:
        rows = conn.execute("SELECT id FROM agregados ORDER BY id").fetchall()
        return [int(r[0]) for r in rows]

def _cmd_build_links(args: argparse.Namespace) -> None:
    ensure_full_schema()
    table_ids = _all_table_ids() if args.all else args.table_ids
    if not table_ids:
        print("No table IDs provided.")
        return
    for tid in table_ids:
        c = build_links_for_table(int(tid))
        print(f"{tid}: vars={c.vars} classes={c.classes} cats={c.cats} var×class={c.var_class}")
    reset_cache()
    print("fuzzy cache reset")


# ---------------------------
# Search
# ---------------------------
def _cmd_search_tables(args: argparse.Namespace) -> None:
    ensure_full_schema()
    sargs = SearchArgs(
        title=args.title,
        vars=tuple(args.var or ()),
        classes=tuple(args.cls or ()),
        coverage=args.coverage,
        limit=max(1, args.limit),
        allow_fuzzy=not args.no_fuzzy,
        var_th=float(args.var_th),
        class_th=float(args.class_th),
        semantic=bool(args.semantic),
        debug_fuzzy=bool(args.debug_fuzzy),
    )
    emb = EmbeddingClient() if args.semantic else None
    hits = asyncio.run(search_tables(sargs, embedding_client=emb))
    if args.json:
        _print_json([
            {
                "id": h.table_id,
                "title": h.title,
                "period_start": h.period_start,
                "period_end": h.period_end,
                "n3": h.n3,
                "n6": h.n6,
                "why": h.why,
                "score": h.score,
                "rrf_score": h.rrf_score,
                "struct_score": h.struct_score,
            }
            for h in hits
        ])
        return
    if not hits:
        print("No results.")
        return
    with sqlite_session() as _conn:
        for h in hits:
            period = ""
            if h.period_start or h.period_end:
                if h.period_start and h.period_end and h.period_start != h.period_end:
                    period = f" | {h.period_start}–{h.period_end}"
                else:
                    period = f" | {h.period_start or h.period_end}"
            cov = f" | N3={h.n3} N6={h.n6}" if (h.n3 or h.n6) else ""
            print(f"{h.table_id}: {h.title}{period}{cov}")

            if args.show_classes:
                names = [r[0] for r in _conn.execute(
                    "SELECT nome FROM classifications WHERE agregado_id=? ORDER BY id LIMIT 3", (h.table_id,)
                ).fetchall()]
                if names:
                    print("  classes:", "; ".join(names))

            if args.explain and h.why:
                print("  matches:", " ".join(f"[{w}]" for w in h.why))
                print(f"  score={h.score:.3f} (struct={h.struct_score:.3f}, rrf={h.rrf_score:.3f})")


# ---------------------------
# Embed
# ---------------------------

def _vec_to_blob(vec):
    arr = array("f", (float(x) for x in vec))
    return arr.tobytes()

def _sha256_text(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()

def _cmd_embed_titles(args: argparse.Namespace) -> None:
    ensure_full_schema()
    s = get_settings()
    model = args.model or s.embedding_model
    only_missing = bool(args.only_missing)
    limit = args.limit if args.limit and args.limit > 0 else None

    emb = EmbeddingClient()

    # Iterate agregados and upsert embeddings if missing/stale
    with sqlite_session() as conn:
        rows = conn.execute(
            "SELECT id, raw_json FROM agregados ORDER BY id"
        ).fetchall()

    count = 0
    updated = 0
    for r in rows:
        if limit is not None and count >= limit:
            break
        tid = int(r["id"])
        try:
            md = orjson.loads(r["raw_json"])
        except Exception:
            continue

        text = _canonical_table_text(md)
        if not text.strip():
            continue

        text_hash = _sha256_text(text)

        # Check existing
        with sqlite_session() as conn:
            cur = conn.execute(
                "SELECT text_hash FROM embeddings WHERE entity_type='agregado' AND entity_id=? AND model=?",
                (str(tid), model),
            ).fetchone()
            existing_hash = cur["text_hash"] if cur else None

        if existing_hash == text_hash:
            count += 1
            continue
        if only_missing and existing_hash is not None:
            count += 1
            continue

        # Compute vector
        try:
            vec = emb.embed_text(text, model=model)
        except Exception as exc:
            print(f"embed failed for {tid}: {exc}")
            count += 1
            continue

        # Upsert
        with sqlite_session() as conn:
            with conn:
                conn.execute(
                    """
                    INSERT OR REPLACE INTO embeddings(
                      entity_type, entity_id, agregado_id, text_hash, model, dimension, vector, created_at
                    ) VALUES(?,?,?,?,?,?,?,datetime('now'))
                    """,
                    (
                        "agregado",
                        str(tid),
                        tid,
                        text_hash,
                        model,
                        len(vec),
                        _vec_to_blob(vec),
                    ),
                )
        updated += 1
        count += 1

    print(f"embed-titles: scanned={count}, updated={updated}, model={model}")



def build_parser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(prog="sidra-search", description="Table-centric search & ingestion")
    sub = p.add_subparsers(dest="cmd")

    # db
    dbmig = sub.add_parser("db", help="Database utilities")
    dbsub = dbmig.add_subparsers(dest="db_cmd")

    db_migrate = dbsub.add_parser("migrate", help="Apply base + search schema")
    db_migrate.set_defaults(func=_cmd_db_migrate)

    db_stats = dbsub.add_parser("stats", help="Show row counts for key tables")
    db_stats.set_defaults(func=_cmd_db_stats)

    # ingest single/many
    ig = sub.add_parser("ingest", help="Ingest one or more table IDs")
    ig.add_argument("table_ids", type=int, nargs="+")
    ig.set_defaults(func=_cmd_ingest)

    # ingest by coverage discovery
    ic = sub.add_parser("ingest-coverage", help="Discover by coverage and ingest")
    ic.add_argument("--coverage", required=True, help='Boolean expr, e.g. "N3 OR (N6>=5000)"')
    ic.add_argument("--subject-contains", dest="subject_contains", help="Filter catalog by subject name substring")
    ic.add_argument("--survey-contains", dest="survey_contains", help="Filter catalog by survey name substring")
    ic.add_argument("--limit", type=int, default=None)
    ic.add_argument("--concurrent", type=int, default=8, help="ingestion concurrency")
    ic.add_argument("--probe-concurrent", type=int, default=None, help="coverage probe concurrency (defaults to --concurrent)")
    ic.set_defaults(func=_cmd_ingest_coverage)

    # build links
    bl = sub.add_parser("build-links", help="(Re)build link indexes for tables")
    bl.add_argument("table_ids", type=int, nargs="*", help="Specific table IDs")
    bl.add_argument("--all", action="store_true", help="Process all ingested tables")
    bl.set_defaults(func=_cmd_build_links)

    # search
    st = sub.add_parser("search", help="Search tables by facets and title")
    st.add_argument("--title", dest="title", help="free-text title query")
    st.add_argument("--var", dest="var", action="append", help="variable name (repeatable)")
    st.add_argument("--class", dest="cls", action="append", help='class name or "Class:Category" (repeatable)')
    st.add_argument("--coverage", help="boolean coverage expr, e.g. '(N6>=5000) AND (N3>=27)'")
    st.add_argument("--limit", type=int, default=20)
    st.add_argument("--no-fuzzy", action="store_true")
    st.add_argument("--var-th", type=float, default=0.74)
    st.add_argument("--class-th", type=float, default=0.78)
    st.add_argument("--semantic", action="store_true", help="use semantic title ranking (requires embeddings)")
    st.add_argument("--explain", action="store_true", help="print match rationale and scores")
    st.add_argument("--json", action="store_true")
    st.add_argument("--show-classes", action="store_true", help="List up to 3 classification names for each hit")
    st.add_argument("--debug-fuzzy", action="store_true", help="Print fuzzy expansions and candidate counts")
    st.set_defaults(func=_cmd_search_tables)

    # embeddings backfill (ADD BEFORE RETURN)
    et = sub.add_parser("embed-titles", help="(Re)embed table titles (idempotent)")
    et.add_argument("--model", help="Embedding model name (defaults to settings)")
    et.add_argument("--only-missing", action="store_true", help="Skip rows that already have an embedding, even if text changed")
    et.add_argument("--limit", type=int, default=None, help="Limit number of tables processed")
    et.set_defaults(func=_cmd_embed_titles)

    return p

def main(argv: Sequence[str] | None = None) -> None:
    parser = build_parser()
    args = parser.parse_args(list(argv) if argv is not None else None)

    # Support nested: "db migrate"/"db stats"
    if getattr(args, "cmd", None) == "db" and not hasattr(args, "func"):
        parser.parse_args(["db", "-h"])
        return

    if not hasattr(args, "func"):
        parser.print_help()
        return
    args.func(args)


if __name__ == "__main__":
    main()



###############################################################################
### FILE: cli/__main__.py
###############################################################################
from . import main

if __name__ == "__main__":
    main()



###############################################################################
### FILE: config.py
###############################################################################
from __future__ import annotations

import os
from dataclasses import dataclass
from functools import lru_cache


@dataclass(frozen=True)
class Settings:
    # HTTP
    sidra_base_url: str = "https://servicodados.ibge.gov.br/api/v3/agregados"
    request_timeout: float = 30.0
    request_retries: int = 3
    user_agent: str = "sidra-search/0.1"

    # DB
    database_timeout: float = 60.0
    municipality_national_threshold: int = 5000

    # Embeddings (optional, for title semantics)
    embedding_api_url: str = "http://127.0.0.1:1234/v1/embeddings"
    embedding_model: str = "text-embedding-qwen3-embedding-0.6b@f16"

    # Features
    enable_titles_fts: bool = True
    enable_title_embeddings: bool = True


def _env(name: str) -> str | None:
    for key in (name, name.upper(), name.lower()):
        if key in os.environ:
            return os.environ[key]
    return None


@lru_cache(maxsize=1)
def get_settings() -> Settings:
    d = Settings().__dict__.copy()

    # strings
    for f in ("sidra_base_url", "user_agent", "embedding_api_url", "embedding_model"):
        v = _env(f"SIDRA_SEARCH_{f.upper()}")
        if v:
            d[f] = v

    # numerics/bools
    def _float(env, key):
        v = _env(env)
        if v:
            try: d[key] = float(v)
            except: pass

    def _int(env, key):
        v = _env(env)
        if v:
            try: d[key] = int(v)
            except: pass

    def _bool(env, key):
        v = _env(env)
        if v is not None:
            d[key] = v not in ("0", "false", "False", "")

    _float("SIDRA_SEARCH_REQUEST_TIMEOUT", "request_timeout")
    _float("SIDRA_SEARCH_DATABASE_TIMEOUT", "database_timeout")
    _int("SIDRA_SEARCH_REQUEST_RETRIES", "request_retries")
    _int("SIDRA_SEARCH_MUNICIPALITY_NATIONAL_THRESHOLD", "municipality_national_threshold")
    _bool("SIDRA_SEARCH_ENABLE_TITLES_FTS", "enable_titles_fts")
    _bool("SIDRA_SEARCH_ENABLE_TITLE_EMBEDDINGS", "enable_title_embeddings")

    return Settings(**d)



###############################################################################
### FILE: db/base_schema.py
###############################################################################
from __future__ import annotations

TABLE_STATEMENTS: tuple[str, ...] = (
    """
    CREATE TABLE IF NOT EXISTS agregados (
        id INTEGER PRIMARY KEY,
        nome TEXT NOT NULL,
        pesquisa TEXT,
        assunto TEXT,
        url TEXT,
        freq TEXT,
        periodo_inicio TEXT,
        periodo_fim TEXT,
        raw_json BLOB NOT NULL,
        fetched_at TEXT NOT NULL,
        municipality_locality_count INTEGER DEFAULT 0,
        covers_national_municipalities INTEGER DEFAULT 0
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS agregados_levels (
        agregado_id INTEGER NOT NULL,
        level_id TEXT NOT NULL,
        level_name TEXT,
        level_type TEXT NOT NULL,
        locality_count INTEGER DEFAULT 0,
        PRIMARY KEY (agregado_id, level_id, level_type),
        FOREIGN KEY (agregado_id) REFERENCES agregados(id)
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS variables (
        id          INTEGER NOT NULL,
        agregado_id INTEGER NOT NULL,
        nome        TEXT NOT NULL,
        unidade     TEXT,
        sumarizacao TEXT,
        text_hash   TEXT NOT NULL,
        PRIMARY KEY (agregado_id, id),
        FOREIGN KEY (agregado_id) REFERENCES agregados(id)
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS classifications (
        id INTEGER NOT NULL,
        agregado_id INTEGER NOT NULL,
        nome TEXT NOT NULL,
        sumarizacao_status INTEGER,
        sumarizacao_excecao TEXT,
        PRIMARY KEY (agregado_id, id),
        FOREIGN KEY (agregado_id) REFERENCES agregados(id)
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS categories (
        agregado_id INTEGER NOT NULL,
        classification_id INTEGER NOT NULL,
        categoria_id INTEGER NOT NULL,
        nome TEXT NOT NULL,
        unidade TEXT,
        nivel INTEGER,
        text_hash TEXT NOT NULL,
        PRIMARY KEY (agregado_id, classification_id, categoria_id),
        FOREIGN KEY (agregado_id, classification_id) REFERENCES classifications(agregado_id, id)
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS periods (
        agregado_id INTEGER NOT NULL,
        periodo_id TEXT NOT NULL,
        literals TEXT NOT NULL,
        modificacao TEXT,
        periodo_ord INTEGER,
        periodo_kind TEXT,
        PRIMARY KEY (agregado_id, periodo_id),
        FOREIGN KEY (agregado_id) REFERENCES agregados(id)
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS localities (
        agregado_id INTEGER NOT NULL,
        level_id TEXT NOT NULL,
        locality_id TEXT NOT NULL,
        nome TEXT NOT NULL,
        PRIMARY KEY (agregado_id, level_id, locality_id),
        FOREIGN KEY (agregado_id, level_id) REFERENCES agregados_levels(agregado_id, level_id)
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS ingestion_log (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        agregado_id INTEGER NOT NULL,
        stage TEXT NOT NULL,
        status TEXT NOT NULL,
        detail TEXT,
        run_at TEXT NOT NULL
    )
    """,
)

INDEX_STATEMENTS: tuple[str, ...] = (
    "CREATE INDEX IF NOT EXISTS idx_variables_agregado ON variables(agregado_id)",
    "CREATE INDEX IF NOT EXISTS idx_categories_agregado ON categories(agregado_id, classification_id)",
    "CREATE INDEX IF NOT EXISTS idx_localities_agregado ON localities(agregado_id, level_id)",
    "CREATE UNIQUE INDEX IF NOT EXISTS u_agregados_levels_pair ON agregados_levels(agregado_id, level_id)",
    "CREATE INDEX IF NOT EXISTS idx_periods_agregado_ord ON periods(agregado_id, periodo_ord)",
)

def apply_base_schema(connection) -> None:
    cur = connection.cursor()
    for stmt in TABLE_STATEMENTS:
        cur.execute(stmt)
    for stmt in INDEX_STATEMENTS:
        cur.execute(stmt)
    connection.commit()



###############################################################################
### FILE: db/migrations.py
###############################################################################
from __future__ import annotations

import sqlite3

from .search_schema import apply_search_schema as _apply_schema

SEARCH_SCHEMA_VERSION = 4
KEY = "sidra_search_schema_version"

def _ensure_meta(connection: sqlite3.Connection) -> None:
    connection.execute("""
        CREATE TABLE IF NOT EXISTS meta_kv(
          key TEXT PRIMARY KEY,
          value TEXT NOT NULL
        )
    """)

def get_search_schema_version(connection: sqlite3.Connection) -> int:
    _ensure_meta(connection)
    cur = connection.execute("SELECT value FROM meta_kv WHERE key = ?", (KEY,))
    row = cur.fetchone()
    return int(row[0]) if row else 0

def bump_search_schema_version(connection: sqlite3.Connection, to_version: int) -> None:
    _ensure_meta(connection)
    connection.execute(
        "INSERT INTO meta_kv(key,value) VALUES(?,?) "
        "ON CONFLICT(key) DO UPDATE SET value=excluded.value",
        (KEY, str(to_version)),
    )

def apply_search_schema(connection: sqlite3.Connection) -> None:
    current = get_search_schema_version(connection)
    if current >= SEARCH_SCHEMA_VERSION:
        return
    with connection:
        _apply_schema(connection)
        bump_search_schema_version(connection, SEARCH_SCHEMA_VERSION)



###############################################################################
### FILE: db/search_schema.py
###############################################################################
from __future__ import annotations

DDL: tuple[str, ...] = (
    # meta kv (shared)
    """
    CREATE TABLE IF NOT EXISTS meta_kv(
      key TEXT PRIMARY KEY,
      value TEXT NOT NULL
    )
    """,
    # link keys
    """
    CREATE TABLE IF NOT EXISTS name_keys (
      kind TEXT NOT NULL,     -- 'var' | 'class' | 'cat'
      key  TEXT NOT NULL,     -- normalized
      raw  TEXT NOT NULL,     -- original
      UNIQUE(kind, key, raw)
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS link_var (
      var_key    TEXT NOT NULL,
      table_id   INTEGER NOT NULL,
      variable_id INTEGER NOT NULL,
      UNIQUE(var_key, table_id, variable_id)
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS link_class (
      class_key TEXT NOT NULL,
      table_id  INTEGER NOT NULL,
      class_id  INTEGER NOT NULL,
      UNIQUE(class_key, table_id, class_id)
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS link_cat (
      class_key  TEXT NOT NULL,
      cat_key    TEXT NOT NULL,
      table_id   INTEGER NOT NULL,
      class_id   INTEGER NOT NULL,
      category_id INTEGER NOT NULL,
      UNIQUE(class_key, cat_key, table_id, class_id, category_id)
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS link_var_class (
      var_key   TEXT NOT NULL,
      class_key TEXT NOT NULL,
      table_id  INTEGER NOT NULL,
      variable_id INTEGER NOT NULL,
      class_id    INTEGER NOT NULL,
      UNIQUE(var_key, class_key, table_id, variable_id, class_id)
    )
    """,
    # FTS for titles (no extra index allowed on virtual table)
    """
    CREATE VIRTUAL TABLE IF NOT EXISTS table_titles_fts
    USING fts5(table_id UNINDEXED, title, survey, subject, tokenize='unicode61')
    """,
    # Embeddings table (generic, reused for table titles)
    """
    CREATE TABLE IF NOT EXISTS embeddings (
      entity_type TEXT NOT NULL,   -- 'agregado'
      entity_id   TEXT NOT NULL,   -- table_id as text
      agregado_id INTEGER,         -- optional convenience (same as entity_id)
      text_hash   TEXT NOT NULL,
      model       TEXT NOT NULL,
      dimension   INTEGER NOT NULL,
      vector      BLOB NOT NULL,
      created_at  TEXT NOT NULL,
      PRIMARY KEY (entity_type, entity_id, model)
    )
    """,
)

INDEXES: tuple[str, ...] = (
    "CREATE INDEX IF NOT EXISTS idx_link_var_key   ON link_var(var_key)",
    "CREATE INDEX IF NOT EXISTS idx_link_class_key ON link_class(class_key)",
    "CREATE INDEX IF NOT EXISTS idx_link_cat_keys  ON link_cat(class_key, cat_key)",
    "CREATE INDEX IF NOT EXISTS idx_link_var_class ON link_var_class(var_key, class_key)",
    "CREATE INDEX IF NOT EXISTS idx_embeddings_agregado ON embeddings(agregado_id, model)",
)

def apply_search_schema(connection) -> None:
    cur = connection.cursor()
    for stmt in DDL:
        cur.execute(stmt)
    for stmt in INDEXES:
        cur.execute(stmt)
    connection.commit()



###############################################################################
### FILE: db/session.py
###############################################################################
from __future__ import annotations

import os
import sqlite3
from contextlib import contextmanager
from pathlib import Path
from typing import Iterator

from ..config import get_settings
from .base_schema import apply_base_schema
from .migrations import apply_search_schema


def _has_base_tables(path: Path) -> bool:
    if not path.exists():
        return False
    try:
        conn = sqlite3.connect(path)
        cur = conn.execute(
            "SELECT name FROM sqlite_master WHERE type='table' AND name IN ('agregados','variables')"
        )
        names = {r[0] for r in cur.fetchall()}
        return "agregados" in names and "variables" in names
    except sqlite3.Error:
        return False
    finally:
        try: conn.close()
        except: pass


def _resolve_db_path() -> Path:
    env = os.getenv("SIDRA_DATABASE_PATH")
    if env:
        p = Path(env).expanduser().resolve()
        p.parent.mkdir(parents=True, exist_ok=True)
        return p
    # default
    p = Path("sidra.db").resolve()
    p.parent.mkdir(parents=True, exist_ok=True)
    return p


_DB_PATH: Path | None = None

def get_database_path() -> Path:
    global _DB_PATH
    if _DB_PATH is None:
        _DB_PATH = _resolve_db_path()
    return _DB_PATH


def create_connection() -> sqlite3.Connection:
    s = get_settings()
    conn = sqlite3.connect(
        get_database_path(),
        timeout=max(float(s.database_timeout), 30.0),
        check_same_thread=False,
    )
    conn.row_factory = sqlite3.Row

    # Safety + performance (no semantic change):
    conn.execute("PRAGMA foreign_keys=ON")
    conn.execute("PRAGMA journal_mode=WAL")
    conn.execute("PRAGMA synchronous=NORMAL")  # durable enough, already in code
    conn.execute(f"PRAGMA busy_timeout = {int(max(s.database_timeout, 60.0) * 1000)}")

    # Added: speed up big batch inserts & temp btrees
    # cache_size: negative => value is in KiB (here ~128 MiB)
    conn.execute("PRAGMA cache_size = -131072")
    conn.execute("PRAGMA temp_store = MEMORY")
    # Use mmap when supported (safe fallback if OS/FS refuses)
    try:
        conn.execute("PRAGMA mmap_size = 268435456")  # 256 MiB
    except Exception:
        pass

    return conn


@contextmanager
def sqlite_session() -> Iterator[sqlite3.Connection]:
    conn = create_connection()
    try:
        yield conn
    finally:
        conn.close()


def ensure_full_schema() -> None:
    conn = create_connection()
    try:
        apply_base_schema(conn)
        apply_search_schema(conn)
        conn.commit()
    finally:
        conn.close()



###############################################################################
### FILE: ingest/bulk.py
###############################################################################
from __future__ import annotations

import asyncio
from dataclasses import dataclass, field
from typing import Any, Iterable, Sequence

from ..net.api_client import SidraApiClient
from ..db.session import sqlite_session, ensure_full_schema
from .ingest_table import ingest_table
from ..search.coverage import parse_coverage_expr, extract_levels, eval_coverage
from ..config import get_settings
from ..search.normalize import normalize_basic

@dataclass(frozen=True)
class CatalogEntry:
    id: int
    nome: str | None
    pesquisa: str | None
    pesquisa_id: int | None
    assunto: str | None
    assunto_id: int | None
    periodicidade: Any
    nivel_territorial: dict[str, list[str]]
    level_hints: frozenset[str] = frozenset()

    @property
    def level_codes(self) -> set[str]:
        codes: set[str] = set()
        for vals in self.nivel_territorial.values():
            for v in vals:
                codes.add(str(v).upper())
        codes.update(self.level_hints)
        return codes

def _normalize_levels(payload: Any) -> dict[str, list[str]]:
    result: dict[str, list[str]] = {}
    if isinstance(payload, dict):
        for k, vals in payload.items():
            arr = vals if isinstance(vals, list) else [vals]
            out = []
            for it in arr:
                if isinstance(it, str):
                    out.append(it.upper())
                elif isinstance(it, dict):
                    code = it.get("codigo") or it.get("nivel") or it.get("id")
                    if isinstance(code, str): out.append(code.upper())
            if out: result[str(k)] = out
    return result

def _extract_subject_fields(survey: Any, ag: Any) -> tuple[str | None, int | None]:
    """
    Try hard to extract a human-readable subject name and id
    from either the table (agregado) or the survey object.
    Handles: plain str, nested dict {'nome','id'}, and a few common aliases.
    """
    def _name_and_id(obj: Any) -> tuple[str | None, int | None]:
        if not isinstance(obj, dict):
            return None, None
        v = obj.get("assunto")
        # variante 1: nested object
        if isinstance(v, dict):
            name = v.get("nome") or v.get("name")
            sid  = v.get("id") or obj.get("idAssunto")
            return (name if isinstance(name, str) else None,
                    int(sid) if isinstance(sid, (int, str)) and str(sid).isdigit() else None)
        # variante 2: plain string
        if isinstance(v, str):
            sid = obj.get("idAssunto")
            return v, int(sid) if isinstance(sid, (int, str)) and str(sid).isdigit() else None
        # variante 3: loose aliases some payloads use
        for k in ("assuntoNome", "assunto_nome", "assuntoDescricao", "assunto_descricao"):
            w = obj.get(k)
            if isinstance(w, str):
                sid = obj.get("idAssunto")
                return w, int(sid) if isinstance(sid, (int, str)) and str(sid).isdigit() else None
        return None, None

    # Prefer table-level, fallback to survey-level
    a_name, a_id = _name_and_id(ag)
    if a_name:
        return a_name, a_id
    s_name, s_id = _name_and_id(survey)
    return s_name, s_id

async def fetch_catalog_entries(
    *, client: SidraApiClient | None = None, subject_id: int | None = None,
    periodicity: str | None = None, levels: Sequence[str] | None = None
) -> list[CatalogEntry]:
    own = False
    if client is None:
        client = SidraApiClient()
        own = True
    normalized_levels = [c.upper() for c in levels or [] if c]
    try:
        catalog = await client.fetch_catalog(subject_id=subject_id, periodicity=periodicity, levels=normalized_levels or None)
    finally:
        if own:
            await client.close()

    out: list[CatalogEntry] = []
    if not isinstance(catalog, list): return out
    for survey in catalog:
        ags = survey.get("agregados") if isinstance(survey, dict) else None
        if not isinstance(ags, list): continue
        for ag in ags:
            if not isinstance(ag, dict): continue
            assunto_name, assunto_id = _extract_subject_fields(survey, ag)

            entry = CatalogEntry(
                id = int(ag.get("id")),
                nome = ag.get("nome") or ag.get("tabela"),
                pesquisa = survey.get("pesquisa") or survey.get("nome"),
                pesquisa_id = survey.get("idPesquisa") or survey.get("id"),
                assunto = assunto_name,
                assunto_id = assunto_id,
                periodicidade = survey.get("periodicidade"),
                nivel_territorial = _normalize_levels(ag.get("nivelTerritorial")),
                level_hints = frozenset(c.upper() for c in normalized_levels),
            )
            out.append(entry)
    return out

def filter_catalog_entries(
    entries: Sequence[CatalogEntry], *,
    require_any_levels: Iterable[str] | None = None,
    require_all_levels: Iterable[str] | None = None,
    exclude_levels: Iterable[str] | None = None,
    subject_contains: str | None = None,
    survey_contains: str | None = None,
) -> list[CatalogEntry]:
    any_levels = {c.upper() for c in require_any_levels or ()}
    all_levels = {c.upper() for c in require_all_levels or ()}
    excluded  = {c.upper() for c in exclude_levels or ()}
    subj_q = normalize_basic(subject_contains) if subject_contains else None
    surv_q = normalize_basic(survey_contains) if survey_contains else None

    out: list[CatalogEntry] = []
    for e in entries:
        codes = e.level_codes
        if any_levels and not (codes & any_levels): continue
        if all_levels and not all_levels.issubset(codes): continue
        if excluded and (codes & excluded): continue
        subj_val = normalize_basic(e.assunto or "")
        surv_val = normalize_basic(e.pesquisa or "")
        if subj_q and subj_q not in subj_val: continue
        if surv_q and surv_q not in surv_val: continue
        out.append(e)
    return out


@dataclass
class BulkReport:
    discovered_ids: list[int] = field(default_factory=list)
    scheduled_ids: list[int] = field(default_factory=list)
    skipped_existing: list[int] = field(default_factory=list)
    ingested_ids: list[int] = field(default_factory=list)
    failed: list[tuple[int, str]] = field(default_factory=list)

async def _probe_counts_for_levels(
    client: SidraApiClient,
    table_id: int,
    levels: Iterable[str],
) -> tuple[dict[str, int], dict[str, list]]:
    """
    Fetch locality lists per level for a table and return:
      - counts: {LEVEL: count}
      - payloads: {LEVEL: list_of_localities}  (for reuse during ingest)
    Levels are normalized to UPPERCASE strings. Non-list responses are treated as empty lists.
    """
    lvls = sorted({str(l).upper() for l in levels if l})
    if not lvls:
        return {}, {}

    async def one(lvl: str) -> tuple[str, list]:
        try:
            payload = await client.fetch_localities(table_id, lvl)
            if isinstance(payload, list):
                return (lvl, payload)
            try:
                return (lvl, list(payload))
            except Exception:
                return (lvl, [])
        except Exception:
            return (lvl, [])

    results = await asyncio.gather(*(one(l) for l in lvls))
    payloads: dict[str, list] = {k: v for (k, v) in results}
    counts: dict[str, int] = {k: (len(v) if isinstance(v, list) else 0) for k, v in payloads.items()}
    return counts, payloads


# Use local DB counts (agregados_levels) for specific levels, if present
def _db_counts_for_levels(table_id: int, levels: Iterable[str]) -> dict[str, int]:
    lvls = [str(l).upper() for l in levels if l]
    if not lvls:
        return {}
    with sqlite_session() as conn:
        placeholders = ",".join("?" for _ in lvls)
        sql = (
            f"SELECT level_id, COALESCE(locality_count,0) AS c "
            f"FROM agregados_levels "
            f"WHERE agregado_id=? AND UPPER(level_id) IN ({placeholders})"
        )
        rows = conn.execute(sql, (table_id, *lvls)).fetchall()
    return {str(r["level_id"]).upper(): int(r["c"]) for r in rows}


async def _subject_gate_with_metadata(
    *,
    entries: list["CatalogEntry"],
    subject_contains: str,
    client: SidraApiClient,
    need: int | None,
    parallel: int,
) -> list["CatalogEntry"]:
    """
    Keep only entries whose SUBJECT matches `subject_contains`.
    Prefer catalog subject; otherwise check local DB; otherwise fetch /{id}/metadados.
    Bounded concurrency, early-stop, and progress ticks.
    """
    q = normalize_basic(subject_contains)
    if not q:
        return entries

    total = len(entries)
    kept: list[CatalogEntry] = []
    done = 0

    print(f"[subject] filtering {total} entries by assunto~{subject_contains!r} (parallel={parallel})")

    async def check_one(e: CatalogEntry) -> tuple[CatalogEntry, bool]:
        # 0) DB cache (already ingested)
        with sqlite_session() as conn:
            row = conn.execute("SELECT assunto FROM agregados WHERE id=?", (e.id,)).fetchone()
        if row and row[0]:
            subj_db = normalize_basic(str(row[0] or ""))
            if subj_db and (q in subj_db):
                return e, True
            # DB had a subject and it didn't match → skip network
            return e, False

        # 1) catalog-provided subject
        subj_cat = normalize_basic(e.assunto or "")
        if subj_cat and (q in subj_cat):
            return e, True

        # 2) fallback: metadata
        per_table_timeout = max(10.0, 2.0 * float(get_settings().request_timeout))
        try:
            md = await asyncio.wait_for(client.fetch_metadata(e.id), timeout=per_table_timeout)
        except Exception:
            return e, False

        subj = md.get("assunto")
        if isinstance(subj, dict):
            subj = subj.get("nome")
        subj = normalize_basic(str(subj or ""))
        return e, (q in subj)

    sem = asyncio.Semaphore(max(1, int(parallel)))
    it = iter(entries)
    pending: set[asyncio.Task] = set()

    async def schedule_next() -> bool:
        try:
            e = next(it)
        except StopIteration:
            return False
        async def _task():
            async with sem:
                return await check_one(e)
        t = asyncio.create_task(_task())
        pending.add(t)
        return True

    # prime
    for _ in range(parallel):
        if not await schedule_next():
            break

    # drain with progress + early-stop
    while pending:
        done_set, pending = await asyncio.wait(pending, return_when=asyncio.FIRST_COMPLETED)
        for t in done_set:
            e, ok = await t
            done += 1
            if ok:
                kept.append(e)
            if (done % 10 == 0) or (need and len(kept) >= need):
                pct = (done * 100) // max(1, total)
                print(f"\r[subject] checked={done}/{total} kept={len(kept)} ({pct}%)", end="", flush=True)
            if need and len(kept) >= need:
                pending.clear()
                break
        while len(pending) < parallel:
            more = await schedule_next()
            if not more:
                break
    print()  # newline

    return kept


async def ingest_by_coverage(
    *,
    coverage: str,
    subject_contains: str | None = None,
    survey_contains: str | None = None,
    limit: int | None = None,
    concurrency: int = 8,
    probe_concurrent: int | None = None,
) -> BulkReport:
    """
    Discover tables that satisfy the coverage expression and ingest them.

    - Catalog fetch pruned by mentioned levels (cheap).
    - Optional text narrowing on survey (cheap).
    - Optional subject narrowing that falls back to metadata (bounded + early stop).
    - Coverage probe (bounded + early stop).
    """
    ensure_full_schema()
    report = BulkReport()
    
    # Cache per-table locality payloads for levels we probed (e.g., N3/N6)
    prefetched_localities: dict[int, dict[str, list]] = {}

    # Parse coverage and extract hinted levels
    try:
        cov_ast = parse_coverage_expr(coverage)
    except Exception as exc:
        raise RuntimeError(f"Invalid --coverage expression: {coverage!r}: {exc}") from exc
    level_hints = extract_levels(cov_ast)

    need = max(0, int(limit)) if (limit is not None and limit >= 0) else None
    par_probe = max(1, int(probe_concurrent or concurrency))

    async with SidraApiClient() as client:
        # 1) Catalog fetch with hinted levels (server-side pruning)
        entries = await fetch_catalog_entries(client=client, levels=sorted(level_hints) or None)

        # 2) Survey narrowing (accent-insensitive, already implemented in filter_catalog_entries)
        entries = filter_catalog_entries(
            entries,
            require_any_levels=None,
            require_all_levels=None,
            exclude_levels=None,
            subject_contains=None,           # handled below via metadata fallback
            survey_contains=survey_contains,
        )
        if not entries:
            return report

        # 3) Subject narrowing with metadata fallback (bounded, early-stop)
        if subject_contains:
            # probe a bit more than 'need' so the coverage step still has room to filter
            subj_need = need if need is not None else None
            entries = await _subject_gate_with_metadata(
                entries=entries,
                subject_contains=subject_contains,
                client=client,
                need=subj_need,
                parallel=par_probe,
            )
            if not entries:
                return report

        total = len(entries)
        print(f"[probe] candidates={total}, levels={sorted(level_hints) or []}, parallel={par_probe}")

        # 4) Coverage probe (bounded, early-stop)
        kept: list[CatalogEntry] = []
        pending: set[asyncio.Task] = set()
        it = iter(entries)

        async def schedule_next() -> bool:
            try:
                e = next(it)
            except StopIteration:
                return False

            check_levels = (level_hints & e.level_codes) if level_hints else set()
            if not check_levels:
                # Nothing to check for this table per coverage expression → accept (no network)
                async def _one_accept():
                    return (e, True, {})
                pending.add(asyncio.create_task(_one_accept()))
                return True

            # 1) Try local DB counts first (only if table already ingested)
            db_counts = await asyncio.to_thread(_db_counts_for_levels, e.id, check_levels)
            have_all = bool(db_counts) and all(l.upper() in db_counts for l in check_levels)
            if have_all:
                ok_local = eval_coverage(cov_ast, db_counts)
                async def _one_db():
                    return (e, ok_local, {})
                pending.add(asyncio.create_task(_one_db()))
                return True

            # 2) Fallback: live probe from API (also returns payloads we can reuse)
            async def _one_net(e_: CatalogEntry) -> tuple[CatalogEntry, bool, dict[str, list]]:
                per_table_timeout = max(10.0, 2.0 * float(get_settings().request_timeout))
                try:
                    counts, payloads = await asyncio.wait_for(
                        _probe_counts_for_levels(client, e_.id, check_levels),
                        timeout=per_table_timeout,
                    )
                    ok = eval_coverage(cov_ast, counts)
                    return e_, ok, (payloads if ok else {})
                except Exception:
                    return e_, False, {}

            pending.add(asyncio.create_task(_one_net(e)))
            return True

        # prime
        for _ in range(par_probe):
            if not await schedule_next():
                break

        probed = 0
        while pending:
            done, pending = await asyncio.wait(pending, return_when=asyncio.FIRST_COMPLETED)
            for t in done:
                probed += 1
                e, ok, payloads = await t
                if ok:
                    kept.append(e)
                    if payloads:
                        prefetched_localities[e.id] = payloads
                if (probed % 10 == 0) or (need and len(kept) >= need):
                    pct = (probed * 100) // max(1, total)
                    inflight = len(pending)
                    print(
                        f"\r[probe] done={probed}/{total} (accepted={len(kept)} • {pct}%) | inflight={inflight}",
                        end="",
                        flush=True,
                    )
                if need and len(kept) >= need:
                    pending.clear()
                    break

            while len(pending) < par_probe:
                more = await schedule_next()
                if not more:
                    break

        print()  # newline after progress

        if need is not None:
            kept = kept[:need]
        report.discovered_ids = [e.id for e in kept]

        # 5) Skip existing + ingest (+ include historical failures)
        with sqlite_session() as conn:
            existing = {int(r[0]) for r in conn.execute("SELECT id FROM agregados")}
            past_failed = {
                int(r[0])
                for r in conn.execute(
                    "SELECT DISTINCT agregado_id FROM ingestion_log WHERE status='error'"
                )
            }

        # schedule = new-kept (not existing)  ∪  (historical failures not existing)
        new_kept = [e.id for e in kept if e.id not in existing]
        hist_retry = sorted(past_failed - existing)

        to_do = list(dict.fromkeys([*new_kept, *hist_retry]))  # preserve order, dedupe

        report.discovered_ids = [e.id for e in kept]  # unchanged semantics
        report.scheduled_ids = list(to_do)
        report.skipped_existing = [e.id for e in kept if e.id in existing]

        if not to_do:
            return report

        print(f"[ingest] {len(to_do)} tables (parallel={concurrency})")
        sem = asyncio.Semaphore(max(1, concurrency))

        async def worker(tid: int) -> None:
            async with sem:
                # Per-table retry with short backoff (other tasks continue)
                delays = (1.0, 3.0, 7.0)  # seconds
                attempt = 0
                while True:
                    try:
                        await ingest_table(tid)
                        report.ingested_ids.append(tid)
                        print(f"  ingested {tid}")
                        return
                    except Exception as exc:
                        msg = str(exc)
                        attempt += 1

                        # Persistent server-side 500? Fail fast after 2 tries.
                        if "statusCode\":500" in msg or " 500:" in msg:
                            if attempt >= 2:
                                report.failed.append((tid, msg[:200]))
                                print(f"  failed {tid}: {exc} (permanent 500?)")
                                return

                        if attempt > len(delays):
                            report.failed.append((tid, msg[:200]))
                            print(f"  failed {tid}: {exc}")
                            return

                        backoff = delays[attempt - 1]
                        print(f"  retry {tid} in {backoff:.0f}s (attempt {attempt+1})")
                        await asyncio.sleep(backoff)


        await asyncio.gather(*(worker(t) for t in to_do))

    return report



###############################################################################
### FILE: ingest/ingest_table.py
###############################################################################
from __future__ import annotations

import asyncio
import hashlib
from array import array
from datetime import datetime, timezone
from typing import Any, Sequence

import orjson

from ..config import get_settings
from ..db.session import sqlite_session
from ..db.migrations import apply_search_schema
from ..db.base_schema import apply_base_schema
from ..ingest.links import build_links_for_table
from ..net.api_client import SidraApiClient
from ..net.embedding_client import EmbeddingClient
from ..search.fuzzy3gram import reset_cache

ISO = "%Y-%m-%dT%H:%M:%SZ"


def _hash_fields(*vals: object) -> str:
    # Robust: stringify, replace None with "", and hash a stable joined string
    return _sha256_text("||".join("" if v is None else str(v) for v in vals))

def _now() -> str:
    return datetime.now(timezone.utc).strftime(ISO)


def _json(obj: Any) -> bytes:
    return orjson.dumps(obj)


def _json_text(obj: Any) -> str:
    return orjson.dumps(obj).decode("utf-8")


def _sha256_text(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()


def _period_to_ord_kind(periodo_id: Any) -> tuple[int | None, str]:
    """
    YYYY -> (YYYY00, 'Y')
    YYYYMM -> (YYYYMM, 'YM')
    YYYYMMDD -> (YYYYMMDD, 'YMD')
    otherwise -> (None, 'UNK')
    """
    s = str(periodo_id or "").strip()
    digits = "".join(ch for ch in s if ch.isdigit())
    if len(digits) == 4:
        return int(digits + "00"), "Y"
    if len(digits) == 6:
        return int(digits), "YM"
    if len(digits) == 8:
        return int(digits), "YMD"
    return None, "UNK"


def _canonical_table_text(md: dict[str, Any]) -> str:
    periodicidade = md.get("periodicidade") or {}
    freq = periodicidade.get("frequencia")
    inicio = periodicidade.get("inicio")
    fim = periodicidade.get("fim")

    if inicio and fim:
        period_line = f"Period: {inicio} - {fim}" if inicio != fim else f"Period: {inicio}"
    elif inicio or fim:
        period_line = f"Period: {inicio or fim}"
    else:
        period_line = None

    nivel = md.get("nivelTerritorial") or {}
    level_parts: list[str] = []
    if isinstance(nivel, dict):
        for level_type in sorted(nivel):
            codes = nivel.get(level_type) or []
            if codes:
                level_parts.append(f"{level_type}: {', '.join(str(c) for c in codes)}")

    lines = [
        f"Table {str(md.get('id'))}: {str(md.get('nome') or '').strip()}".strip(),
        f"Survey: {str(md.get('pesquisa') or '')}" if md.get("pesquisa") else None,
        f"Subject: {str(md.get('assunto') or '')}" if md.get("assunto") else None,
        f"Frequency: {str(freq)}" if freq else None,
        period_line,
        f"Territorial levels: {'; '.join(level_parts)}" if level_parts else None,
        f"URL: {str(md.get('URL') or '')}" if md.get("URL") else None,
    ]
    # every yielded piece is a str; ignore falsy items
    return "\n".join([s for s in lines if isinstance(s, str) and s])



def _vec_to_blob(vec: Sequence[float]) -> bytes:
    arr = array("f", (float(x) for x in vec))
    return arr.tobytes()


async def _ensure_embeddings(table_id: int, text: str, *, embedding_client: EmbeddingClient) -> None:
    if not text.strip():
        return
    text_hash = _sha256_text(text)
    model = embedding_client.model

    def _read_existing() -> str | None:
        with sqlite_session() as conn:
            cur = conn.execute(
                "SELECT text_hash FROM embeddings "
                "WHERE entity_type='agregado' AND entity_id=? AND model=?",
                (str(table_id), model),
            ).fetchone()
            return cur["text_hash"] if cur else None

    existing_hash = await asyncio.to_thread(_read_existing)
    if existing_hash == text_hash:
        return

    # compute vector
    vector = await asyncio.to_thread(embedding_client.embed_text, text, model=model)
    dim = len(vector)

    def _write():
        with sqlite_session() as conn:
            with conn:
                conn.execute(
                    """
                    INSERT OR REPLACE INTO embeddings(
                      entity_type, entity_id, agregado_id, text_hash, model, dimension, vector, created_at
                    ) VALUES(?,?,?,?,?,?,?,?)
                    """,
                    (
                        "agregado",
                        str(table_id),
                        table_id,
                        text_hash,
                        model,
                        dim,
                        _vec_to_blob(vector),
                        _now(),
                    ),
                )

    await asyncio.to_thread(_write)


async def ingest_table(
    table_id: int,
    *,
    client: SidraApiClient | None = None,
    embedding_client: EmbeddingClient | None = None,
    build_links: bool = True,
    prefetched_localities: dict[str, list] | None = None,  # optional reuse from probe
) -> None:
    """
    Fetch metadata from SIDRA and persist into base + search schemas.
    Also refresh the table title FTS row and (optionally) a title embedding.
    """
    # ensure schema
    with sqlite_session() as conn:
        apply_base_schema(conn)
        apply_search_schema(conn)
        conn.commit()

    own_client = False
    if client is None:
        client = SidraApiClient()
        own_client = True

    try:
        # --- fetch payloads
        try:
            md = await client.fetch_metadata(table_id)
        except Exception:
            _log_ingestion(table_id, "error", "api:metadata")
            raise
        try:
            periods = await client.fetch_periods(table_id)
        except Exception:
            _log_ingestion(table_id, "error", "api:periods")
            raise

        # --- derive levels + localities (count + names)
        nivel_groups = md.get("nivelTerritorial") or {}
        if not isinstance(nivel_groups, dict):
            nivel_groups = {}

        settings = get_settings()
        municipality_count = 0
        level_rows: list[tuple[int, str, str | None, str, int]] = []
        locality_rows: list[tuple[int, str, str | None, str | None]] = []

        # NOTE:
        # - `prefetched_localities` is a dict like {"N3": [...], "N6": [...]} if passed by the caller.
        # - Keys we store/reuse are UPPERCASE to avoid case mismatches.
        for level_type, codes in (nivel_groups or {}).items():
            if not codes:
                continue
            # Be defensive: ensure we can iterate even if API returned a scalar.
            try:
                code_list = list(codes)
            except TypeError:
                code_list = [codes]

            for code in code_list:
                code_s = str(code)
                code_key = code_s.upper()

                # Prefer the probe's cached payload for this level (e.g., N3/N6).
                payload: list | None = None
                if prefetched_localities and isinstance(prefetched_localities.get(code_key), list):
                    payload = prefetched_localities[code_key]
                else:
                    try:
                        payload = await client.fetch_localities(table_id, code_s)
                    except Exception:
                        payload = []
                    if not isinstance(payload, list):
                        try:
                            payload = list(payload)
                        except Exception:
                            payload = []

                # Count first (coverage uses counts only)
                count = len(payload)
                if code_key == "N6":
                    municipality_count = max(municipality_count, count)

                # Try to extract a representative level_name (same as before)
                level_name = None
                if payload:
                    try:
                        node = payload[0].get("nivel") if isinstance(payload[0], dict) else None
                        if isinstance(node, dict):
                            level_name = node.get("nome")
                    except Exception:
                        level_name = None

                # Record per-level counts
                level_rows.append((table_id, code_s, level_name, str(level_type), count))

                # Persist exact membership (unchanged behavior)
                if payload:
                    for loc in payload:
                        if isinstance(loc, dict):
                            lid = loc.get("id")
                            lname = loc.get("nome")
                        else:
                            lid = None
                            lname = None
                        locality_rows.append((table_id, code_s, lid, lname))


        covers_nat = (
            1
            if (municipality_count >= max(0, int(settings.municipality_national_threshold)))
            else 0
        )

        # variables
        variables = md.get("variaveis") or []
        var_rows: list[tuple[Any, int, Any, Any, str, str]] = []
        for v in variables:
            var_rows.append(
                (
                    v.get("id"),
                    table_id,
                    v.get("nome"),
                    v.get("unidade"),
                    _json_text(v.get("sumarizacao", [])),
                    _hash_fields(v.get("id"), v.get("nome"), v.get("unidade")),   # <— here
                )
            )

        # classifications + categories
        class_rows: list[tuple[int, int, str | None, int, str]] = []
        cat_rows: list[tuple[int, int, int, str | None, str | None, Any, str]] = []
        for cl in md.get("classificacoes", []) or []:
            cid = cl.get("id")
            class_rows.append(
                (
                    cid,
                    table_id,
                    cl.get("nome"),
                    1 if (cl.get("sumarizacao", {}).get("status")) else 0,
                    _json_text(cl.get("sumarizacao", {}).get("excecao", [])),
                )
            )
            for cat in cl.get("categorias", []) or []:
                cat_rows.append(
                    (
                        table_id,
                        cid,
                        cat.get("id"),
                        cat.get("nome"),
                        cat.get("unidade"),
                        cat.get("nivel"),
                        _hash_fields(cid, cat.get("id"), cat.get("nome"), cat.get("unidade")),  # <— here
                    )
                )

        # periods
        period_rows: list[tuple[int, str, str, Any, int | None, str]] = []
        for p in periods or []:
            pid = p.get("id") if isinstance(p, dict) else p
            literals = p.get("literals", [pid]) if isinstance(p, dict) else [pid]
            modificacao = p.get("modificacao") if isinstance(p, dict) else None
            ord_val, kind = _period_to_ord_kind(pid)
            period_rows.append((table_id, str(pid), _json_text(literals), modificacao, ord_val, kind))

        # --- write to DB (replace child rows)
        fetched_at = _now()

        with sqlite_session() as conn:
            apply_base_schema(conn)
            apply_search_schema(conn)

            conn.execute("BEGIN")
            try:
                # parent: agregados header
                conn.execute(
                    """
                    INSERT OR REPLACE INTO agregados(
                      id, nome, pesquisa, assunto, url, freq, periodo_inicio, periodo_fim,
                      raw_json, fetched_at, municipality_locality_count, covers_national_municipalities
                    ) VALUES(?,?,?,?,?,?,?,?,?,?,?,?)
                    """,
                    (
                        md.get("id"),
                        md.get("nome"),
                        md.get("pesquisa"),
                        md.get("assunto"),
                        md.get("URL"),
                        (md.get("periodicidade") or {}).get("frequencia"),
                        (md.get("periodicidade") or {}).get("inicio"),
                        (md.get("periodicidade") or {}).get("fim"),
                        _json(md),
                        fetched_at,
                        municipality_count,
                        covers_nat,
                    ),
                )

                # purge children → insert fresh
                conn.execute("DELETE FROM localities WHERE agregado_id=?", (table_id,))
                conn.execute("DELETE FROM agregados_levels WHERE agregado_id=?", (table_id,))
                conn.execute("DELETE FROM categories WHERE agregado_id=?", (table_id,))
                conn.execute("DELETE FROM classifications WHERE agregado_id=?", (table_id,))
                conn.execute("DELETE FROM variables WHERE agregado_id=?", (table_id,))
                conn.execute("DELETE FROM periods WHERE agregado_id=?", (table_id,))

                if level_rows:
                    conn.executemany(
                        """
                        INSERT OR REPLACE INTO agregados_levels(
                          agregado_id, level_id, level_name, level_type, locality_count
                        ) VALUES(?,?,?,?,?)
                        """,
                        level_rows,
                    )
                if var_rows:
                    conn.executemany(
                        """
                        INSERT OR REPLACE INTO variables(
                          id, agregado_id, nome, unidade, sumarizacao, text_hash
                        ) VALUES(?,?,?,?,?,?)
                        """,
                        var_rows,
                    )
                if class_rows:
                    conn.executemany(
                        """
                        INSERT OR REPLACE INTO classifications(
                          id, agregado_id, nome, sumarizacao_status, sumarizacao_excecao
                        ) VALUES(?,?,?,?,?)
                        """,
                        class_rows,
                    )
                if cat_rows:
                    conn.executemany(
                        """
                        INSERT OR REPLACE INTO categories(
                          agregado_id, classification_id, categoria_id, nome, unidade, nivel, text_hash
                        ) VALUES(?,?,?,?,?,?,?)
                        """,
                        cat_rows,
                    )
                if period_rows:
                    conn.executemany(
                        """
                        INSERT OR REPLACE INTO periods(
                          agregado_id, periodo_id, literals, modificacao, periodo_ord, periodo_kind
                        ) VALUES(?,?,?,?,?,?)
                        """,
                        period_rows,
                    )
                if locality_rows:
                    conn.executemany(
                        """
                        INSERT OR REPLACE INTO localities(
                          agregado_id, level_id, locality_id, nome
                        ) VALUES(?,?,?,?)
                        """,
                        locality_rows,
                    )

                # refresh FTS for title/survey/subject
                conn.execute("DELETE FROM table_titles_fts WHERE table_id=?", (table_id,))
                conn.execute(
                    "INSERT INTO table_titles_fts(table_id, title, survey, subject) VALUES(?,?,?,?)",
                    (
                        table_id,
                        md.get("nome") or "",
                        md.get("pesquisa") or "",
                        md.get("assunto") or "",
                    ),
                )

                # log
                conn.execute(
                    "INSERT INTO ingestion_log(agregado_id, stage, status, detail, run_at) VALUES(?,?,?,?,?)",
                    (table_id, "metadata", "success", None, fetched_at),
                )

                conn.commit()
            except Exception:
                conn.rollback()
                _log_ingestion(table_id, "error", "db:write")
                raise

        # build name→table link indexes
        if build_links:
            await asyncio.to_thread(build_links_for_table, table_id)
            # make new names visible to in-process searches
            reset_cache()  # ADD

        # optional embeddings for title text
        if get_settings().enable_title_embeddings:
            try:
                embedding_client = embedding_client or EmbeddingClient()
                text = _canonical_table_text(md)
                await _ensure_embeddings(table_id, text, embedding_client=embedding_client)
            except Exception:
                # don't break ingestion if embeddings fail
                pass

    finally:
        if own_client:
            await client.close()


def _log_ingestion(table_id: int, status: str, detail: str) -> None:
    with sqlite_session() as conn:
        with conn:
            conn.execute(
                """
                INSERT INTO ingestion_log(agregado_id, stage, status, detail, run_at)
                VALUES(?,?,?,?,?)
                """,
                (table_id, "metadata", status, detail, _now()),
            )



###############################################################################
### FILE: ingest/links.py
###############################################################################
from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, List, Tuple

from ..db.session import create_connection
from ..db.migrations import apply_search_schema
from ..search.normalize import normalize_basic


@dataclass
class LinkCounts:
    vars: int = 0
    classes: int = 0
    cats: int = 0
    var_class: int = 0


def _delete_links_for_table(conn, table_id: int) -> None:
    with conn:
        conn.execute("DELETE FROM link_var WHERE table_id = ?", (table_id,))
        conn.execute("DELETE FROM link_class WHERE table_id = ?", (table_id,))
        conn.execute("DELETE FROM link_cat WHERE table_id = ?", (table_id,))
        conn.execute("DELETE FROM link_var_class WHERE table_id = ?", (table_id,))


def _infer_var_class_pairs(metadata_json: dict) -> List[Tuple[int, int]]:
    """
    Optional hook: try to infer actual applicable {variable_id, class_id} pairs from metadata.
    For now, return empty (fallback to cross-product).
    """
    return []


def build_links_for_table(table_id: int) -> LinkCounts:
    conn = create_connection()
    try:
        apply_search_schema(conn)

        # load rows
        var_rows = conn.execute(
            "SELECT id, nome FROM variables WHERE agregado_id = ? ORDER BY id", (table_id,)
        ).fetchall()
        class_rows = conn.execute(
            "SELECT id, nome FROM classifications WHERE agregado_id = ? ORDER BY id", (table_id,)
        ).fetchall()
        cat_rows = conn.execute(
            """
            SELECT classification_id, categoria_id, nome
            FROM categories
            WHERE agregado_id = ?
            ORDER BY classification_id, categoria_id
            """,
            (table_id,),
        ).fetchall()
        if not var_rows and not class_rows:
            return LinkCounts()

        cats_by_class: Dict[int, List[Tuple[int, str]]] = {}
        for cr in cat_rows:
            cats_by_class.setdefault(int(cr["classification_id"]), []).append((int(cr["categoria_id"]), cr["nome"]))

        _delete_links_for_table(conn, table_id)

        with conn:
            # variables
            for vr in var_rows:
                v_id = int(vr["id"]); raw = str(vr["nome"] or ""); key = normalize_basic(raw)
                if not key: continue
                conn.execute("INSERT OR IGNORE INTO name_keys(kind,key,raw) VALUES(?,?,?)", ("var", key, raw))
                conn.execute(
                    "INSERT OR IGNORE INTO link_var(var_key, table_id, variable_id) VALUES(?,?,?)",
                    (key, table_id, v_id),
                )

            # classes & cats
            for cl in class_rows:
                c_id = int(cl["id"]); raw = str(cl["nome"] or ""); key = normalize_basic(raw)
                if not key: continue
                conn.execute("INSERT OR IGNORE INTO name_keys(kind,key,raw) VALUES(?,?,?)", ("class", key, raw))
                conn.execute(
                    "INSERT OR IGNORE INTO link_class(class_key, table_id, class_id) VALUES(?,?,?)",
                    (key, table_id, c_id),
                )
                for cat_id, cat_raw in cats_by_class.get(c_id, []):
                    cat_key = normalize_basic(str(cat_raw or ""))
                    if not cat_key: continue
                    conn.execute("INSERT OR IGNORE INTO name_keys(kind,key,raw) VALUES(?,?,?)", ("cat", cat_key, cat_raw))
                    conn.execute(
                        "INSERT OR IGNORE INTO link_cat(class_key, cat_key, table_id, class_id, category_id) VALUES(?,?,?,?,?)",
                        (key, cat_key, table_id, c_id, int(cat_id)),
                    )

            # var×class pairs — cross-product fallback
            inferred = set(_infer_var_class_pairs({}))  # reserved for future use
            if inferred:
                to_pairs = inferred
            else:
                to_pairs = {(int(vr["id"]), int(cl["id"])) for vr in var_rows for cl in class_rows}

            for v_id, c_id in to_pairs:
                v_key = normalize_basic(str(next((vr["nome"] for vr in var_rows if int(vr["id"])==v_id), "")))
                c_key = normalize_basic(str(next((cr["nome"] for cr in class_rows if int(cr["id"])==c_id), "")))
                if not v_key or not c_key: continue
                conn.execute(
                    "INSERT OR IGNORE INTO link_var_class(var_key, class_key, table_id, variable_id, class_id) VALUES(?,?,?,?,?)",
                    (v_key, c_key, table_id, v_id, c_id),
                )

        c_var = conn.execute("SELECT COUNT(*) FROM link_var WHERE table_id=?", (table_id,)).fetchone()[0]
        c_cls = conn.execute("SELECT COUNT(*) FROM link_class WHERE table_id=?", (table_id,)).fetchone()[0]
        c_cat = conn.execute("SELECT COUNT(*) FROM link_cat WHERE table_id=?", (table_id,)).fetchone()[0]
        c_vc  = conn.execute("SELECT COUNT(*) FROM link_var_class WHERE table_id=?", (table_id,)).fetchone()[0]
        return LinkCounts(c_var, c_cls, c_cat, c_vc)
    finally:
        conn.close()



###############################################################################
### FILE: net/api_client.py
###############################################################################
from __future__ import annotations

import asyncio
from typing import Any, Mapping, Self

import httpx
import orjson
from tenacity import retry, retry_if_exception_type, stop_after_attempt, wait_exponential

from ..config import get_settings


class SidraApiError(RuntimeError):
    pass


class SidraApiClient:
    def __init__(self, *, base_url: str | None = None, timeout: float | None = None) -> None:
        s = get_settings()
        # Explicit timeouts so slow servers can't stall forever on read/close
        t = float(timeout or s.request_timeout)
        http_timeout = httpx.Timeout(connect=t, read=t, write=t, pool=t)
        # Keep connections bounded (Windows sockets behave better this way)
        limits = httpx.Limits(max_connections=50, max_keepalive_connections=20)

        self._client = httpx.AsyncClient(
            base_url=base_url or s.sidra_base_url,
            timeout=http_timeout,
            headers={"User-Agent": s.user_agent},
            limits=limits,
        )

    async def __aenter__(self) -> Self: return self
    async def __aexit__(self, *_): await self.close()
    async def close(self) -> None: await self._client.aclose()

    @retry(
        stop=stop_after_attempt(get_settings().request_retries),
        wait=wait_exponential(multiplier=1, min=1, max=10),
        retry=retry_if_exception_type((httpx.TransportError, SidraApiError)),
        reraise=True,
    )
    async def _get_json(self, path: str, params: Mapping[str, Any] | None = None) -> Any:
        r = await self._client.get(path, params=params)
        if r.status_code in (429,) or r.status_code >= 500:
            raise SidraApiError(f"SIDRA API failed {r.status_code}: {r.text[:200]}")
        if r.status_code >= 400:
            raise RuntimeError(f"SIDRA API failed {r.status_code}: {r.text[:200]}")
        return orjson.loads(r.content)

    async def fetch_metadata(self, agregado_id: int) -> Any:
        return await self._get_json(f"/{agregado_id}/metadados")

    async def fetch_periods(self, agregado_id: int) -> Any:
        return await self._get_json(f"/{agregado_id}/periodos")

    async def fetch_localities(self, agregado_id: int, level: str) -> Any:
        return await self._get_json(f"/{agregado_id}/localidades/{level}")

    async def fetch_catalog(self, *, subject_id: int | None = None, periodicity: str | None = None,
                            levels: list[str] | None = None) -> Any:
        params: dict[str, Any] = {}
        if subject_id is not None: params["assunto"] = subject_id
        if periodicity: params["periodicidade"] = periodicity
        if levels:
            lv = [c.upper() for c in levels if c]
            if lv: params["nivel"] = "|".join(lv)
        return await self._get_json("", params=params or None)


def fetch_metadata_sync(agregado_id: int) -> Any:
    async def _r() -> Any:
        async with SidraApiClient() as c:
            return await c.fetch_metadata(agregado_id)
    return asyncio.run(_r())


__all__ = ["SidraApiClient", "SidraApiError", "fetch_metadata_sync"]



###############################################################################
### FILE: net/embedding_client.py
###############################################################################
from __future__ import annotations

from typing import Iterable, Sequence

import httpx
import orjson

from ..config import get_settings


class EmbeddingClient:
    def __init__(self, *, base_url: str | None = None, model: str | None = None, timeout: float | None = None) -> None:
        s = get_settings()
        self._base_url = base_url or s.embedding_api_url
        self._model = model or s.embedding_model
        self._timeout = timeout or s.request_timeout
        self._headers = {"Content-Type": "application/json", "User-Agent": s.user_agent}

    @property
    def model(self) -> str: return self._model

    def embed_text(self, text: str, *, model: str | None = None) -> Sequence[float]:
        payload = {"model": model or self._model, "input": text}
        r = httpx.post(self._base_url, content=orjson.dumps(payload), headers=self._headers, timeout=self._timeout)
        r.raise_for_status()
        data = r.json()
        return data["data"][0]["embedding"]

    def embed_batch(self, texts: Iterable[str], *, model: str | None = None) -> list[Sequence[float]]:
        payload = {"model": model or self._model, "input": list(texts)}
        r = httpx.post(self._base_url, content=orjson.dumps(payload), headers=self._headers, timeout=self._timeout)
        r.raise_for_status()
        data = r.json()
        return [item["embedding"] for item in data["data"]]



###############################################################################
### FILE: search/coverage.py
###############################################################################
from __future__ import annotations
from dataclasses import dataclass
from typing import Any, Iterator

@dataclass(frozen=True)
class _Tok:
    kind: str
    value: str
    pos: int

def _tokens(s: str) -> Iterator[_Tok]:
    i, n = 0, len(s)
    while i < n:
        ch = s[i]
        if ch.isspace(): i += 1; continue
        if ch in '()':
            yield _Tok('LP' if ch=='(' else 'RP', ch, i); i += 1; continue
        if i+1<n:
            two = s[i:i+2]
            if two in ('>=','<=','==','!=','&&'): yield _Tok('OP' if two!='&&' else 'AND', two, i); i += 2; continue
        if ch in ('<','>','='): yield _Tok('OP', ch, i); i += 1; continue
        if ch.isalpha() or ch=='_':
            start=i; i+=1
            while i<n and (s[i].isalnum() or s[i]=='_'): i+=1
            word = s[start:i].upper()
            if word in ('AND','OR','NOT'): yield _Tok(word, word, start)
            else: yield _Tok('ID', word, start)
            continue
        if ch.isdigit():
            start=i; i+=1
            while i<n and s[i].isdigit(): i+=1
            yield _Tok('NUM', s[start:i], start); continue
        if ch=='|':
            if i+1<n and s[i+1]=='|': yield _Tok('OR','||',i); i+=2; continue
        if ch=='!': yield _Tok('NOT','!',i); i+=1; continue
        raise SyntaxError(f"Unexpected {ch!r} at {i}")
    yield _Tok('EOF','',n)

@dataclass(frozen=True)
class _Cmp: op: str; ident: str; number: int
@dataclass(frozen=True)
class _Not: node: Any
@dataclass(frozen=True)
class _And: left: Any; right: Any
@dataclass(frozen=True)
class _Or: left: Any; right: Any

class _Parser:
    def __init__(self, text: str) -> None:
        self._it = iter(_tokens(text)); self.cur = next(self._it)
    def _eat(self, kind: str) -> _Tok:
        if self.cur.kind != kind: raise SyntaxError(f"Expected {kind}, got {self.cur.kind} at {self.cur.pos}")
        t = self.cur; self.cur = next(self._it); return t
    def parse(self) -> Any:
        n = self._expr()
        if self.cur.kind != 'EOF': raise SyntaxError(f"Unexpected {self.cur.kind} at {self.cur.pos}")
        return n
    def _expr(self) -> Any:
        n = self._and()
        while self.cur.kind == 'OR':
            self._eat('OR'); n = _Or(n, self._and())
        return n
    def _and(self) -> Any:
        n = self._unary()
        while self.cur.kind == 'AND':
            self._eat('AND'); n = _And(n, self._unary())
        return n
    def _unary(self) -> Any:
        if self.cur.kind=='NOT': self._eat('NOT'); return _Not(self._unary())
        return self._primary()
    def _primary(self) -> Any:
        if self.cur.kind=='LP':
            self._eat('LP'); n=self._expr(); self._eat('RP'); return n
        return self._cmp()
    def _cmp(self) -> Any:
        ident = self._eat('ID').value
        # Allow bare identifiers like "N3" (shorthand for "N3 >= 1")
        if self.cur.kind != 'OP':
            return _Cmp('>=', ident, 1)
        op = self._eat('OP').value
        if op == '=':
            op = '=='
        number = int(self._eat('NUM').value)
        return _Cmp(op, ident, number)

def parse_coverage_expr(text: str) -> Any: return _Parser(text).parse()

def extract_levels(node: Any) -> set[str]:
    out: set[str] = set()
    def _w(n: Any) -> None:
        if isinstance(n, _Cmp): out.add(n.ident)
        elif isinstance(n, _Not): _w(n.node)
        elif isinstance(n, (_And,_Or)): _w(n.left); _w(n.right)
    _w(node)
    return out      

def eval_coverage(node: Any, counts: dict[str,int]) -> bool:
    def _cmp(op: str, a: int, b: int) -> bool:
        return {'>=':a>=b,'>':a>b,'<=':a<=b,'<':a<b,'==':a==b,'!=':a!=b}[op]
    def _ev(n: Any) -> bool:
        if isinstance(n,_Cmp): return _cmp(n.op, int(counts.get(n.ident,0)), n.number)
        if isinstance(n,_Not): return not _ev(n.node)
        if isinstance(n,_And): return _ev(n.left) and _ev(n.right)
        if isinstance(n,_Or): return _ev(n.left) or _ev(n.right)
        raise TypeError(f"Bad node {n}")
    return _ev(node)



###############################################################################
### FILE: search/fuzzy3gram.py
###############################################################################
# src/sidra_search/search/fuzzy3gram.py
from __future__ import annotations

from typing import Dict, List, Tuple, Literal

from rapidfuzz import fuzz, process

from ..db.session import create_connection
from ..db.migrations import apply_search_schema
from .normalize import normalize_basic

# In-RAM corpus of normalized names by kind
# kind: "var" or "class"
_CORPUS: Dict[str, List[str]] = {"var": [], "class": []}
_BUILT = False


def reset_cache() -> None:
    """Clear the in-RAM corpus so new names become visible in the same process."""
    global _BUILT
    _BUILT = False
    _CORPUS["var"].clear()
    _CORPUS["class"].clear()


def _build() -> None:
    """Build the list of normalized names for variables and classifications."""
    global _BUILT
    if _BUILT:
        return
    conn = create_connection()
    try:
        apply_search_schema(conn)
        # Pull *normalized* names from DB and unique them
        var_keys = [normalize_basic(r[0]) for r in conn.execute("SELECT DISTINCT nome FROM variables")]
        class_keys = [normalize_basic(r[0]) for r in conn.execute("SELECT DISTINCT nome FROM classifications")]

        _CORPUS["var"] = sorted({k for k in var_keys if k})
        _CORPUS["class"] = sorted({k for k in class_keys if k})
        _BUILT = True
    finally:
        conn.close()


def _rf_score(a: str, b: str, **kwargs) -> float:
    """
    Blend a few RapidFuzz scorers (all 0..100) to cover:
      - substrings (partial_ratio),
      - bag-of-words overlap (token_set_ratio),
      - general fuzz (WRatio).
    """
    if not a or not b:
        return 0.0
    if a == b:
        return 100.0
    s2 = fuzz.token_set_ratio(a, b)   # multi-token overlap, order-insensitive
    s1 = fuzz.partial_ratio(a, b)     # substring-ish
    s3 = fuzz.WRatio(a, b)            # robust overall
    return 0.5 * s2 + 0.3 * s1 + 0.2 * s3


def similar_keys(
    kind: Literal["var", "class"],
    query_raw: str,
    *,
    threshold: float,
    top_k: int = 10,
) -> List[Tuple[str, float]]:
    assert kind in ("var", "class")
    _build()

    q = normalize_basic(query_raw)
    if not q:
        return []

    choices = _CORPUS[kind]
    if not choices:
        return []

    # Adaptive threshold: for short one-token queries (e.g., "pessoal"),
    # allow a bit more fuzz so it can hit "pessoas", etc.
    tok_count = len(q.split())
    qlen = len(q.replace(" ", ""))
    eff_th = float(threshold)
    if tok_count == 1 and qlen <= 8:
        eff_th = min(eff_th, 0.72)  # relax to 0.72 if default is higher
    cutoff = max(0, min(100, int(round(eff_th * 100.0))))


    # Pass 1 — strict (user cutoff)
    results = process.extract(
        q, choices, scorer=_rf_score, limit=max(10, top_k * 5), score_cutoff=cutoff
    )

    # Pass 2 — if empty, slightly relax (−12 pts ~ 0.12)
    if not results and len(q) <= 10:
        results = process.extract(
            q, choices, scorer=_rf_score, limit=max(10, top_k * 5), score_cutoff=max(0, cutoff - 12)
        )
        
    # Pass 2.5 — single-token: compare the query against individual tokens in each key.
    # This pulls in long names containing a near-match token (e.g., "pessoas" vs "pessoal").
    if not results and tok_count == 1:
        hits = []
        for key in choices:
            tokens = key.replace("-", " ").split()
            best = 0
            for t in tokens:
                s = fuzz.ratio(q, t)
                if s > best:
                    best = s
                if best >= 86:  # short-circuit when very close
                    break
            # tolerate near misses like "pessoal"↔"pessoas"
            if best >= max(68, cutoff - 15):
                hits.append((key, best))
        hits.sort(key=lambda x: x[1], reverse=True)
        results = [(k, s, None) for (k, s) in hits[: max(10, top_k * 5)]]


    # Pass 3 — if still empty, try a robust single scorer (WRatio), no cutoff
    if not results:
        results = process.extract(
            q, choices, scorer=fuzz.WRatio, limit=max(10, top_k * 5)
        )
        

    out: List[Tuple[str, float]] = [(key, float(score) / 100.0) for (key, score, _idx) in results]

    # Last resort — exact substring if still nothing
    if not out:
        rel = [(key, len(q) / max(1, len(key))) for key in choices if q in key]
        rel.sort(key=lambda x: x[1], reverse=True)
        out = rel[:top_k]

    return out[:top_k]



###############################################################################
### FILE: search/normalize.py
###############################################################################
# src/sidra_search/search/normalize.py
from __future__ import annotations

import re
import unicodedata

_WS = re.compile(r"\s+")
# Keep hyphen as a token-forming char per plan; drop other punctuation.
_PUNCT = re.compile(r"[^\w\s-]", re.UNICODE)

def normalize_basic(s: str | None) -> str:
    if not s:
        return ""
    s = unicodedata.normalize("NFKD", s)
    s = "".join(ch for ch in s if not unicodedata.combining(ch))
    s = s.lower().replace("\u00a0", " ")
    # normalize whitespace around hyphens
    s = _WS.sub(" ", s).strip()
    s = _PUNCT.sub(" ", s)
    s = _WS.sub(" ", s).strip()
    # collapse repeated hyphens/spaces like " -  - "
    s = re.sub(r"\s*-\s*", "-", s)
    s = _WS.sub(" ", s).strip()
    return s



###############################################################################
### FILE: search/tables.py
###############################################################################
# src/sidra_search/search/tables.py
from __future__ import annotations

import asyncio
from dataclasses import dataclass
from typing import Dict, Iterable, List, Optional, Sequence, Set, Tuple

from ..db.session import create_connection
from ..db.migrations import apply_search_schema
from ..search.normalize import normalize_basic
from ..search.coverage import parse_coverage_expr, extract_levels, eval_coverage
from ..search.fuzzy3gram import similar_keys
from ..search.title_rank import rrf
from ..net.embedding_client import EmbeddingClient
from ..config import get_settings


@dataclass(frozen=True)
class TableHit:
    table_id: int
    title: str
    period_start: str | None
    period_end: str | None
    n3: int
    n6: int
    why: List[str]
    score: float
    rrf_score: float
    struct_score: float


@dataclass(frozen=True)
class SearchArgs:
    title: str | None
    vars: Tuple[str, ...]
    classes: Tuple[str, ...]   # "Class" or "Class:Category"
    coverage: str | None
    limit: int
    allow_fuzzy: bool
    var_th: float
    class_th: float
    semantic: bool    # use embeddings for titles
    debug_fuzzy: bool  # NEW


def _split_class(spec: str) -> Tuple[str, Optional[str]]:
    if ":" in spec:
        a, b = spec.split(":", 1)
        return a.strip(), b.strip()
    return spec.strip(), None


def _fts_query(text: str) -> str:
    toks = [t for t in normalize_basic(text).split() if t]
    return " ".join(toks)


def _union_tables_for_keys(conn, table: str, col: str, keys: Iterable[str]) -> Set[int]:
    ids: Set[int] = set()
    keys = [k for k in set(keys) if k]
    if not keys:
        return ids
    for key in keys:
        rows = conn.execute(f"SELECT DISTINCT table_id FROM {table} WHERE {col} = ?", (key,)).fetchall()
        ids |= {int(r[0]) for r in rows}
    return ids


def _intersect_groups_union_inside(conn, table: str, col: str, groups: List[Set[str]]) -> Set[int]:
    """
    For classes-only queries: for each group (expansions for a requested class),
    take the union of tables, then intersect across groups.
    """
    result: Optional[Set[int]] = None
    for g in groups:
        u = _union_tables_for_keys(conn, table, col, g)
        result = u if result is None else (result & u)
        if not result:
            return set()
    return result or set()


def _tables_for_class_cat_multi(conn, groups_and_cat: List[Tuple[Set[str], str]]) -> Set[int]:
    """
    Intersect across pairs. For each pair, we accept any class_key from the group's expansions,
    but the category_key is strict (per plan).
    """
    result: Optional[Set[int]] = None
    for class_keys, cat_key in groups_and_cat:
        if not class_keys or not cat_key:
            return set()
        u: Set[int] = set()
        for ck in class_keys:
            rows = conn.execute(
                "SELECT DISTINCT table_id FROM link_cat WHERE class_key=? AND cat_key=?",
                (ck, cat_key),
            ).fetchall()
            u |= {int(r[0]) for r in rows}
        result = u if result is None else (result & u)
        if not result:
            return set()
    return result or set()


def _select_in(conn, sql_prefix: str, items: Sequence[str], *prefix_params) -> List[tuple]:
    """
    Helper to run `sql_prefix ... IN (?)` safely with dynamic placeholders.
    Returns rows as tuples.
    """
    if not items:
        return []
    placeholders = ",".join("?" for _ in items)
    sql = f"{sql_prefix} ({placeholders})"
    cur = conn.execute(sql, (*prefix_params, *items))
    return [tuple(r) for r in cur.fetchall()]


async def search_tables(
    args: SearchArgs,
    *,
    embedding_client: EmbeddingClient | None = None,
) -> List[TableHit]:
    conn = create_connection()
    try:
        apply_search_schema(conn)

        # ---------- Expand inputs (with scores) ----------
        # Vars
        var_cand_score: Dict[str, float] = {}
        var_exact: Set[str] = set()
        for v in args.vars or ():
            ek = normalize_basic(v)
            if ek:
                var_cand_score[ek] = max(var_cand_score.get(ek, 0.0), 1.0)  # exact=1.0
                var_exact.add(ek)
            if args.allow_fuzzy:
                for k, s in similar_keys("var", v, threshold=args.var_th, top_k=12):
                    var_cand_score[k] = max(var_cand_score.get(k, 0.0), float(s))

        # Classes (grouped by request)
        class_groups: List[Dict[str, float]] = []  # one dict per requested class
        class_exact: List[str] = []                # exact key per group (may be "")
        class_cat_per_group: List[Optional[str]] = []  # strict category (normalized) or None
        for spec in args.classes or ():
            raw_class, raw_cat = _split_class(spec)
            ck_exact = normalize_basic(raw_class)
            catk = normalize_basic(raw_cat) if raw_cat else None

            group: Dict[str, float] = {}
            if ck_exact:
                group[ck_exact] = 1.0
            if args.allow_fuzzy and raw_class:
                for k, s in similar_keys("class", raw_class, threshold=args.class_th, top_k=12):
                    group[k] = max(group.get(k, 0.0), float(s))

            class_groups.append(group)
            class_exact.append(ck_exact or "")
            class_cat_per_group.append(catk)

        # ---------- Inspect fuzzy expansions (debug) ----------
        if args.debug_fuzzy:
            top_vars = sorted(var_cand_score.items(), key=lambda x: x[1], reverse=True)[:12]
            print("[fuzzy] var:", ", ".join(f"{k}:{s:.2f}" for k,s in top_vars) or "(none)")
            for i, grp in enumerate(class_groups):
                top_cls = sorted(grp.items(), key=lambda x: x[1], reverse=True)[:12]
                print(f"[fuzzy] class[{i}]:", ", ".join(f"{k}:{s:.2f}" for k,s in top_cls) or "(none)")


        # ---------- Initial candidate tables ----------
        candidates: Optional[Set[int]] = None

        # If there are var candidates: union of tables with any var candidate
        if var_cand_score:
            ids = _union_tables_for_keys(conn, "link_var", "var_key", var_cand_score.keys())
            candidates = ids if candidates is None else (candidates & ids)

            # Intersect early with class presence if classes were requested
            if class_groups:
                groups_sets = [set(g.keys()) for g in class_groups if g]
                if groups_sets:
                    cls_ids = _intersect_groups_union_inside(conn, "link_class", "class_key", groups_sets)
                    candidates = candidates & cls_ids
                    if not candidates:
                        return []

                
        # If classes-only: intersect across groups (union inside)
        if not var_cand_score and class_groups:
            groups_sets = [set(g.keys()) for g in class_groups if g]
            ids = _intersect_groups_union_inside(conn, "link_class", "class_key", groups_sets)
            candidates = ids if candidates is None else (candidates & ids)

        # Class:Category constraints (class fuzzy, category strict)
        class_cat_groups = [
            (set(class_groups[i].keys()), catk) for i, catk in enumerate(class_cat_per_group) if catk
        ]
        if class_cat_groups:
            ids = _tables_for_class_cat_multi(conn, class_cat_groups)
            candidates = ids if candidates is None else (candidates & ids)

        # If nothing structural given: all tables
        if candidates is None:
            rows = conn.execute("SELECT id FROM agregados").fetchall()
            candidates = {int(r[0]) for r in rows}

        if not candidates:
            return []

        # ---------- Coverage filter (post-structural) ----------
        if args.coverage:
            try:
                ast = parse_coverage_expr(args.coverage)
            except Exception:
                ast = None
            if ast:
                needed = extract_levels(ast)
                keep: Set[int] = set()
                for tid in candidates:
                    rows = conn.execute(
                        "SELECT level_id, locality_count FROM agregados_levels WHERE agregado_id=?",
                        (tid,),
                    ).fetchall()
                    counts = {str(r["level_id"]).upper(): int(r["locality_count"] or 0) for r in rows}
                    if eval_coverage(ast, counts):
                        keep.add(tid)
                candidates = keep
                if not candidates:
                    return []

        # ---------- Title ranking (lexical + semantic) ----------
        lexical_ranks: Dict[int, int] = {}
        semantic_ranks: Dict[int, int] = {}

        if args.title and get_settings().enable_titles_fts:
            q = _fts_query(args.title)
            if q:
                rows = conn.execute(
                    "SELECT table_id FROM table_titles_fts WHERE table_titles_fts MATCH ?",
                    (q,),
                ).fetchall()
                order: List[int] = []
                seen: Set[int] = set()
                for r in rows:
                    t = int(r[0])
                    if t in candidates and t not in seen:
                        seen.add(t); order.append(t)
                    if len(order) >= args.limit * 5:
                        break
                lexical_ranks = {tid: idx + 1 for idx, tid in enumerate(order)}

        if args.title and args.semantic and get_settings().enable_title_embeddings:
            emb = embedding_client or EmbeddingClient()
            try:
                # keyword-only parameter "model", so call with a lambda/partial
                qvec = await asyncio.to_thread(lambda: emb.embed_text(args.title, model=emb.model))
            except Exception:
                qvec = None

            if qvec:
                ordered = sorted(candidates)
                placeholders = ",".join("?" for _ in ordered)
                sql = (
                    f"SELECT entity_id, dimension, vector "
                    f"FROM embeddings WHERE entity_type='agregado' AND model=? AND entity_id IN ({placeholders})"
                )
                cur = conn.execute(sql, (emb.model, *ordered))

                from array import array
                import math

                def to_vec(blob, dim):
                    arr = array("f"); arr.frombytes(blob); v = list(arr)
                    return v[:dim] if dim and len(v) > dim else v

                def cosine(a: Sequence[float], b: Sequence[float]) -> float:
                    if not a or not b or len(a) != len(b): return 0.0
                    dot = sum(x*y for x,y in zip(a,b))
                    na = math.sqrt(sum(x*x for x in a))
                    nb = math.sqrt(sum(x*x for x in b))
                    return 0.0 if na==0 or nb==0 else dot/(na*nb)

                sims: List[Tuple[int, float]] = []
                for row in cur.fetchall():
                    tid = int(row["entity_id"])
                    vec = to_vec(row["vector"], int(row["dimension"]))
                    sim = cosine(qvec, vec)
                    if sim > 0:
                        sims.append((tid, sim))
                sims.sort(key=lambda x: x[1], reverse=True)
                top = [tid for (tid, _s) in sims[: args.limit * 5]]
                semantic_ranks = {tid: idx + 1 for idx, tid in enumerate(top)}


        rrf_scores = rrf({**lexical_ranks, **semantic_ranks}, k=60.0)

        # ---------- Structural scoring with fuzzy pairing ----------
        # Pre-materialize for speed per table:
        var_candidates = list(var_cand_score.keys())
        class_groups_list = [list(g.keys()) for g in class_groups]

        def struct_for_table(tid: int) -> Tuple[float, List[str]]:
            why: List[str] = []

            # Quickly list var keys present in this table among our candidates
            present_vars = set(k for (k, ) in _select_in(
                conn,
                "SELECT var_key FROM link_var WHERE table_id=? AND var_key IN",
                var_candidates, tid
            ))

            # If query has var(s), require at least one present; else classes-only path
            if var_candidates and not present_vars:
                return 0.0, []

            # If classes-only, we don't need pairing against a var
            if not var_candidates and class_groups:
                score_cls = 0.0
                for gi, grp in enumerate(class_groups):
                    present_any = set(k for (k, ) in _select_in(
                        conn,
                        "SELECT class_key FROM link_class WHERE table_id=? AND class_key IN",
                        class_groups_list[gi], tid
                    ))
                    if not present_any:
                        return 0.0, []
                    # take best cosine in this group
                    best_ck = max(present_any, key=lambda k: class_groups[gi].get(k, 0.0))
                    score_cls += class_groups[gi].get(best_ck, 0.0)
                    exact = class_exact[gi]
                    if best_ck == exact and exact:
                        why.append(f'class="{best_ck}"')
                    else:
                        why.append(f'class≈"{best_ck}"')
                s_classes = (score_cls / max(1, len(class_groups)))
                return (0.35 * s_classes, why)  # small structure score without a var

            # With var(s): choose the BEST variable that satisfies all class groups
            best_struct = 0.0
            best_why: List[str] = []

            for vk in present_vars or set(["__no_var__"]):
                if vk == "__no_var__":  # defensive
                    continue

                # For each class group, find the best class_key that:
                #  - pairs with this var (link_var_class)
                #  - and (if that group has a category) has link_cat(class_key, cat_key)
                cls_picks: List[Tuple[str, float, bool]] = []  # (ck, score, is_exact)
                ok_all = True
                for gi, grp in enumerate(class_groups):
                    if not grp:
                        continue
                    group_keys = list(grp.keys())

                    # category constraint (strict) narrows allowed class keys
                    if class_cat_per_group[gi]:
                        allowed = set(k for (k, ) in _select_in(
                            conn,
                            "SELECT class_key FROM link_cat WHERE table_id=? AND cat_key=? AND class_key IN",
                            group_keys, tid, class_cat_per_group[gi]
                        ))
                    else:
                        allowed = set(group_keys)

                    if not allowed:
                        ok_all = False; break

                    # pairing constraint: only class_keys that pair with this var in this table
                    paired = set(k for (k, ) in _select_in(
                        conn,
                        "SELECT class_key FROM link_var_class WHERE table_id=? AND var_key=? AND class_key IN",
                        list(allowed), tid, vk
                    ))
                    if not paired:
                        ok_all = False; break

                    # pick the best scoring class key in this group
                    best_ck = max(paired, key=lambda k: grp.get(k, 0.0))
                    cls_picks.append((best_ck, grp.get(best_ck, 0.0), best_ck == class_exact[gi]))

                if not ok_all:
                    continue

                s_var = var_cand_score.get(vk, 0.0)
                s_classes = (sum(s for (_ck, s, _ex) in cls_picks) / max(1, len(cls_picks))) if cls_picks else 0.0
                exact_boost = (0.10 if vk in var_exact else 0.0) + 0.05 * sum(1 for (_ck, _s, ex) in cls_picks if ex)
                struct = 0.55 * s_var + 0.35 * s_classes + exact_boost

                # Build WHY for this candidate
                why_vk = [f'var{"=" if vk in var_exact else "≈"}"{vk}"']
                for ck, _s, ex in cls_picks:
                    why_vk.append(f'class{"=" if ex else "≈"}"{ck}"')
                if cls_picks:
                    why_vk.append("var×class")

                if struct > best_struct:
                    best_struct, best_why = struct, why_vk

            return best_struct, best_why

        # ---------- Assemble hits ----------
        hits: List[TableHit] = []
        for tid in candidates:
            row = conn.execute(
                "SELECT id, nome, periodo_inicio, periodo_fim FROM agregados WHERE id=?",
                (tid,),
            ).fetchone()
            if not row:
                continue

            n3r = conn.execute(
                "SELECT COALESCE(locality_count,0) FROM agregados_levels WHERE agregado_id=? AND level_id='N3'",
                (tid,),
            ).fetchone()
            n6r = conn.execute(
                "SELECT COALESCE(locality_count,0) FROM agregados_levels WHERE agregado_id=? AND level_id='N6'",
                (tid,),
            ).fetchone()
            n3 = int(n3r[0]) if n3r else 0
            n6 = int(n6r[0]) if n6r else 0

            struct, why = struct_for_table(tid)
            rrf_score = float(rrf_scores.get(tid, 0.0))
            final = 0.75 * struct + 0.25 * rrf_score  # per plan

            hits.append(
                TableHit(
                    table_id=int(row["id"]),
                    title=str(row["nome"] or ""),
                    period_start=row["periodo_inicio"],
                    period_end=row["periodo_fim"],
                    n3=n3,
                    n6=n6,
                    why=why,
                    score=final,
                    rrf_score=rrf_score,
                    struct_score=struct,
                )
            )

        hits.sort(key=lambda h: h.score, reverse=True)
        return hits[: max(1, int(args.limit))]
    finally:
        conn.close()



###############################################################################
### FILE: search/title_rank.py
###############################################################################
from __future__ import annotations
from typing import Mapping, Dict

def rrf(ranks: Mapping[int, int], k: float = 60.0) -> Dict[int, float]:
    return {k_: 1.0 / (k + r) for k_, r in ranks.items()}



###############################################################################
### FILE: smoke.sh
###############################################################################
#!/usr/bin/env bash
set -euo pipefail

echo "== sanity: pick DB in repo root =="
export SIDRA_DATABASE_PATH="$(pwd)/sidra.db"
python -m sidra_search.cli db stats

echo
echo "== A) subject/survey filters =="
python -m sidra_search.cli ingest-coverage \
  --coverage "N3 OR (N6>=5000)" \
  --survey-contains "Censo" \
  --subject-contains "Agro" \
  --limit 10

python -m sidra_search.cli ingest-coverage \
  --coverage "N3 OR (N6>=5000)" \
  --survey-contains "Censo" \
  --limit 10

echo
echo "== B) link counts after ingest (should be nonzero) =="
python -m sidra_search.cli build-links --all
python -m sidra_search.cli db stats
sqlite3 "$SIDRA_DATABASE_PATH" "SELECT COUNT(*) FROM link_var;"
sqlite3 "$SIDRA_DATABASE_PATH" "SELECT COUNT(*) FROM link_class;"
sqlite3 "$SIDRA_DATABASE_PATH" "SELECT COUNT(*) FROM link_var_class;"

echo
echo "== C) verify known var×class pairs exist at SQL level =="
sqlite3 "$SIDRA_DATABASE_PATH" "
SELECT DISTINCT table_id
FROM link_var_class
WHERE var_key LIKE '%pessoas%' AND class_key='sexo';"

sqlite3 "$SIDRA_DATABASE_PATH" "
SELECT DISTINCT table_id
FROM link_var_class
WHERE var_key LIKE '%pessoas%' AND var_key LIKE '%indigen%'
  AND class_key LIKE '%ensino%';"

echo
echo "== D) search: single-token typo should succeed (token-level merge) =="
python -m sidra_search.cli search --var "pessoal" --class "sexo" --limit 10 --explain --debug-fuzzy

echo
echo "== E) search: multi-token misspelling still ok (via blended scorer) =="
python -m sidra_search.cli search --var "pssoas" --class "sexo" --limit 10 --explain --debug-fuzzy

echo
echo "== F) search: classes-only should work (no crash) =="
python -m sidra_search.cli search --class "sexo" --limit 10 --explain

echo
echo "== G) search: Class:Category strict filter =="
# Try common category 'Branca' under 'Cor ou raça' (if present in your ingest)
python -m sidra_search.cli search --class "Cor ou raça:Branca" --limit 10 --explain || true

echo
echo "== H) title FTS (lexical) =="
python -m sidra_search.cli search --title "frequencia escolar indígenas" --limit 10 --explain

echo
echo "== I) coverage filter on search results (should only show N6 >= 5000) =="
python -m sidra_search.cli search \
  --var "pessoas" --class "sexo" \
  --coverage "(N6>=5000)" \
  --limit 10 --explain

echo
echo "== J) debug mismatch drill: show top fuzzy keys directly =="
# Ensure we actually see 'pessoas' among var fuzzy candidates for 'pessoal'
python -m sidra_search.cli search --var "pessoal" --limit 3 --debug-fuzzy

echo
echo "== K) negative test: impossible coverage expression -> handled error =="
set +e
python -m sidra_search.cli ingest-coverage --coverage "N6>>=5" --limit 1 2>&1 | sed -n '1,5p'
set -e

echo
echo "== L) FTS row per table guaranteed =="
sqlite3 "$SIDRA_DATABASE_PATH" "SELECT COUNT(*), COUNT(DISTINCT table_id) FROM table_titles_fts;"

echo
echo "== M) pairing sanity: pick a returned table and show its classes =="
# Replace 10058 with any ID from earlier results:
sqlite3 "$SIDRA_DATABASE_PATH" "SELECT DISTINCT class_key FROM link_class WHERE table_id=10058 ORDER BY 1 LIMIT 20;"

echo
echo "== All tests ran. Scan the outputs above for: =="
echo "  - No 'No results.' where a pair exists in SQL"
echo "  - Fuzzy lists for 'pessoal' include at least one 'pessoas ...' key"
echo "  - Class-only search prints results (no TypeError)"
echo "  - Coverage-constrained search shows N6 >= 5000 in lines"



###############################################################################
### FILE: util/__init__.py
###############################################################################
from __future__ import annotations

import hashlib
from datetime import datetime, timezone

ISO = "%Y-%m-%dT%H:%M:%SZ"

def utcnow_iso() -> str:
    return datetime.now(timezone.utc).strftime(ISO)

def sha256_text(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()



