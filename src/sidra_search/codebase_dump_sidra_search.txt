Project structure for '/c/Users/Galaxy/LEVI/projects/sidra-database/src/sidra_search':
===============================================================================
  __init__.py
  __main__.py
  cli/__init__.py
  cli/__main__.py
  config.py
  db/__init__.py
  db/base_schema.py
  db/migrations.py
  db/search_schema.py
  db/session.py
  ingest/__init__.py
  ingest/bulk.py
  ingest/ingest_table.py
  ingest/links.py
  net/__init__.py
  net/api_client.py
  net/embedding_client.py
  search/__init__.py
  search/coverage.py
  search/fuzzy3gram.py
  search/normalize.py
  search/tables.py
  search/title_rank.py
  search/where_eval.py
  search/where_expr.py
  smoke.sh
  util/__init__.py



###############################################################################
### FILE: __init__.py
###############################################################################
"""sidra_search — table-centric search (titles + var/class links + fuzzy)."""

from .db.migrations import apply_search_schema, get_search_schema_version  # noqa: F401
from .db.base_schema import apply_base_schema  # noqa: F401

__all__ = [
    "apply_base_schema",
    "apply_search_schema",
    "get_search_schema_version",
]



###############################################################################
### FILE: __main__.py
###############################################################################
# sidra_search/__main__.py
from .cli import main
if __name__ == "__main__":
    main()



###############################################################################
### FILE: cli/__init__.py
###############################################################################
from __future__ import annotations

import argparse
import asyncio
import hashlib
import json
import sys
from array import array
from dataclasses import asdict
from typing import Sequence

import orjson

from ..config import get_settings
from ..db.session import ensure_full_schema, sqlite_session
from ..ingest.bulk import ingest_by_coverage
from ..ingest.ingest_table import _canonical_table_text  # reuse identical text
from ..ingest.ingest_table import ingest_table
from ..ingest.links import build_links_for_table
from ..net.embedding_client import EmbeddingClient
from ..search.fuzzy3gram import reset_cache
from ..search.tables import SearchArgs, search_tables
from ..search.where_expr import parse_where_expr


CLI_MANUAL = """\
sidra-search manual
====================

Common commands:
  python -m sidra_search.cli db migrate
      Ensure both base and search schemas are present in the local SQLite database.

  python -m sidra_search.cli ingest-coverage --coverage "N3 OR (N6>=5000)" --limit 10
      Discover tables that satisfy a coverage expression (boolean logic over N-level counts),
      probing SIDRA as needed, and ingest them locally. Combine with --survey-contains or
      --subject-contains to narrow the catalog textually.

  python -m sidra_search.cli build-links --all
      Rebuild search link tables (variables, classifications, categories) for every ingested table.

  python -m sidra_search.cli embed-titles --only-missing
      Refresh semantic embeddings for table titles. Required before using --semantic search.

  python -m sidra_search.cli search --q 'title~"taxa" AND (N6>=5000)'
      Search with the unified boolean query language (facets + coverage). Combine with --semantic
      to blend embeddings with lexical and structural ranking when TITLE literals are present.
      SURVEY and SUBJECT terms act as catalog filters only. For categories, use cat~"Nome" to
      match any class containing that category, or cat~"Class::Nome" to require the exact
      class/category pair. "Contains" (~) checks use normalized, accent-stripped substring
      comparisons, while internal prefilters on VAR/CLASS/CAT link keys stay exact for precision.
      Add --explain to show match reasons.

Legacy flags like --title/--var/--class/--coverage are still accepted but translate into --q behind the scenes.

Flags worth noting:
  --json            Output structured JSON for scripting.
  --debug-fuzzy     Inspect fuzzy expansions for variables/classes during search.
  --no-fuzzy        Disable fuzzy expansions (exact key matching only).

Environment hints:
  SIDRA_SEARCH_ENABLE_TITLE_EMBEDDINGS=1 enables semantic title ranking once embeddings exist.
  SIDRA_DATABASE_PATH can be set to relocate the SQLite database file.

Run `python -m sidra_search.cli <command> --help` for command-specific options.
"""

def _print_json(obj) -> None:
    print(json.dumps(obj, ensure_ascii=False, indent=2))


# ---------------------------
# Manual / help
# ---------------------------
def _cmd_manual(_args: argparse.Namespace) -> None:
    print(CLI_MANUAL)


# ---------------------------
# DB admin
# ---------------------------
def _cmd_db_migrate(args: argparse.Namespace) -> None:
    ensure_full_schema()
    print("Database schema ensured (base + search).")


def _cmd_db_stats(args: argparse.Namespace) -> None:
    ensure_full_schema()
    with sqlite_session() as conn:
        counts = {}
        def c(table: str) -> int:
            try:
                return int(conn.execute(f"SELECT COUNT(*) FROM {table}").fetchone()[0])
            except Exception:
                return 0

        counts["agregados"] = c("agregados")
        counts["variables"] = c("variables")
        counts["classifications"] = c("classifications")
        counts["categories"] = c("categories")
        counts["periods"] = c("periods")
        counts["agregados_levels"] = c("agregados_levels")
        counts["localities"] = c("localities")
        counts["name_keys"] = c("name_keys")
        counts["link_var"] = c("link_var")
        counts["link_class"] = c("link_class")
        counts["link_cat"] = c("link_cat")
        counts["link_var_class"] = c("link_var_class")
        counts["table_titles_fts"] = c("table_titles_fts")
        try:
            counts["embeddings_agregado"] = int(
                conn.execute(
                    "SELECT COUNT(*) FROM embeddings WHERE entity_type='agregado'"
                ).fetchone()[0]
            )
        except Exception:
            counts["embeddings_agregado"] = 0

    _print_json(counts)


# ---------------------------
# Ingest
# ---------------------------
def _cmd_ingest(args: argparse.Namespace) -> None:
    ensure_full_schema()
    async def run():
        for tid in args.table_ids:
            try:
                await ingest_table(int(tid))
                print(f"ingested {tid}")
            except Exception as exc:
                print(f"failed {tid}: {exc}")
    asyncio.run(run())


def _cmd_ingest_coverage(args: argparse.Namespace) -> None:
    ensure_full_schema()
    report = asyncio.run(
        ingest_by_coverage(
            coverage=args.coverage,
            subject_contains=args.subject_contains,
            survey_contains=args.survey_contains,
            limit=args.limit,
            concurrency=args.concurrent,
            probe_concurrent=args.probe_concurrent,
        )
    )
    _print_json(asdict(report))


# ---------------------------
# Index / links
# ---------------------------
def _all_table_ids() -> list[int]:
    with sqlite_session() as conn:
        rows = conn.execute("SELECT id FROM agregados ORDER BY id").fetchall()
        return [int(r[0]) for r in rows]

def _cmd_build_links(args: argparse.Namespace) -> None:
    ensure_full_schema()
    table_ids = _all_table_ids() if args.all else args.table_ids
    if not table_ids:
        print("No table IDs provided.")
        return
    for tid in table_ids:
        c = build_links_for_table(int(tid))
        print(f"{tid}: vars={c.vars} classes={c.classes} cats={c.cats} var×class={c.var_class}")
    reset_cache()
    print("fuzzy cache reset")


# ---------------------------
# Search
# ---------------------------
def _quote_literal(text: str) -> str:
    return text.replace("\\", "\\\\").replace('"', "\\\"")


def _legacy_query(args: argparse.Namespace) -> str | None:
    parts: list[str] = []
    if args.title:
        parts.append(f'title~"{_quote_literal(args.title)}"')
    for v in args.var or ():
        parts.append(f'var~"{_quote_literal(v)}"')
    for spec in args.cls or ():
        raw = spec.strip()
        if not raw:
            continue
        if ":" in raw:
            class_part, cat_part = raw.split(":", 1)
            class_part = class_part.strip()
            cat_part = cat_part.strip()
        else:
            class_part, cat_part = raw, ""
        if class_part:
            parts.append(f'class~"{_quote_literal(class_part)}"')
        if class_part and cat_part:
            combo = f"{class_part}::{cat_part}"
            parts.append(f'cat~"{_quote_literal(combo)}"')
    if args.coverage:
        parts.append(f"({args.coverage})")
    if args.survey_contains:
        parts.append(f'survey~"{_quote_literal(args.survey_contains)}"')
    if args.subject_contains:
        parts.append(f'subject~"{_quote_literal(args.subject_contains)}"')
    if parts:
        return " AND ".join(parts)
    return None


def _cmd_search_tables(args: argparse.Namespace) -> None:
    ensure_full_schema()

    q_expr = args.q.strip() if args.q and args.q.strip() else None
    legacy_expr = _legacy_query(args)
    if legacy_expr:
        print("NOTE: --title/--var/--class/--coverage flags are deprecated; use --q instead.")
        q_expr = f"({q_expr}) AND ({legacy_expr})" if q_expr else legacy_expr

    where = None
    if q_expr:
        try:
            where = parse_where_expr(q_expr)
        except Exception as exc:  # pragma: no cover - user input error
            print(f"invalid query (--q): {exc}")
            sys.exit(1)

    sargs = SearchArgs(
        q=q_expr,
        where=where,
        limit=max(1, args.limit),
        allow_fuzzy=not args.no_fuzzy,
        var_th=float(args.var_th),
        class_th=float(args.class_th),
        semantic=bool(args.semantic),
        debug_fuzzy=bool(args.debug_fuzzy),
    )

    emb = EmbeddingClient() if args.semantic else None
    hits = asyncio.run(search_tables(sargs, embedding_client=emb))
    if args.json:
        _print_json([
            {
                "id": h.table_id,
                "title": h.title,
                "period_start": h.period_start,
                "period_end": h.period_end,
                "n3": h.n3,
                "n6": h.n6,
                "why": h.why,
                "score": h.score,
                "rrf_score": h.rrf_score,
                "struct_score": h.struct_score,
            }
            for h in hits
        ])
        return
    if not hits:
        print("No results.")
        return
    with sqlite_session() as _conn:
        for h in hits:
            period = ""
            if h.period_start or h.period_end:
                if h.period_start and h.period_end and h.period_start != h.period_end:
                    period = f" | {h.period_start}–{h.period_end}"
                else:
                    period = f" | {h.period_start or h.period_end}"
            cov = f" | N3={h.n3} N6={h.n6}" if (h.n3 or h.n6) else ""
            print(f"{h.table_id}: {h.title}{period}{cov}")

            if args.show_classes:
                names = [r[0] for r in _conn.execute(
                    "SELECT nome FROM classifications WHERE agregado_id=? ORDER BY id LIMIT 3", (h.table_id,)
                ).fetchall()]
                if names:
                    print("  classes:", "; ".join(names))

            if args.explain and h.why:
                print("  matches:", " ".join(f"[{w}]" for w in h.why))
                print(f"  score={h.score:.3f} (struct={h.struct_score:.3f}, rrf={h.rrf_score:.3f})")


# ---------------------------
# Embed
# ---------------------------

def _vec_to_blob(vec):
    arr = array("f", (float(x) for x in vec))
    return arr.tobytes()

def _sha256_text(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()

def _cmd_embed_titles(args: argparse.Namespace) -> None:
    ensure_full_schema()
    s = get_settings()
    model = args.model or s.embedding_model
    only_missing = bool(args.only_missing)
    limit = args.limit if args.limit and args.limit > 0 else None

    emb = EmbeddingClient()

    # Iterate agregados and upsert embeddings if missing/stale
    with sqlite_session() as conn:
        rows = conn.execute(
            "SELECT id, raw_json FROM agregados ORDER BY id"
        ).fetchall()

    count = 0
    updated = 0
    for r in rows:
        if limit is not None and count >= limit:
            break
        tid = int(r["id"])
        try:
            md = orjson.loads(r["raw_json"])
        except Exception:
            continue

        text = _canonical_table_text(md)
        if not text.strip():
            continue

        text_hash = _sha256_text(text)

        # Check existing
        with sqlite_session() as conn:
            cur = conn.execute(
                "SELECT text_hash FROM embeddings WHERE entity_type='agregado' AND entity_id=? AND model=?",
                (str(tid), model),
            ).fetchone()
            existing_hash = cur["text_hash"] if cur else None

        if existing_hash == text_hash:
            count += 1
            continue
        if only_missing and existing_hash is not None:
            count += 1
            continue

        # Compute vector
        try:
            vec = emb.embed_text(text, model=model)
        except Exception as exc:
            print(f"embed failed for {tid}: {exc}")
            count += 1
            continue

        # Upsert
        with sqlite_session() as conn:
            with conn:
                conn.execute(
                    """
                    INSERT OR REPLACE INTO embeddings(
                      entity_type, entity_id, agregado_id, text_hash, model, dimension, vector, created_at
                    ) VALUES(?,?,?,?,?,?,?,datetime('now'))
                    """,
                    (
                        "agregado",
                        str(tid),
                        tid,
                        text_hash,
                        model,
                        len(vec),
                        _vec_to_blob(vec),
                    ),
                )
        updated += 1
        count += 1

    print(f"embed-titles: scanned={count}, updated={updated}, model={model}")



def build_parser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(prog="sidra-search", description="Table-centric search & ingestion")
    p.add_argument("--manual", action="store_true", help="Print a concise CLI usage guide and exit")
    sub = p.add_subparsers(dest="cmd")

    # db
    dbmig = sub.add_parser("db", help="Database utilities")
    dbsub = dbmig.add_subparsers(dest="db_cmd")

    db_migrate = dbsub.add_parser("migrate", help="Apply base + search schema")
    db_migrate.set_defaults(func=_cmd_db_migrate)

    db_stats = dbsub.add_parser("stats", help="Show row counts for key tables")
    db_stats.set_defaults(func=_cmd_db_stats)

    # ingest single/many
    ig = sub.add_parser("ingest", help="Ingest one or more table IDs")
    ig.add_argument("table_ids", type=int, nargs="+")
    ig.set_defaults(func=_cmd_ingest)

    # ingest by coverage discovery
    ic = sub.add_parser("ingest-coverage", help="Discover by coverage and ingest")
    ic.add_argument("--coverage", required=True, help='Boolean expr, e.g. "N3 OR (N6>=5000)"')
    ic.add_argument("--subject-contains", dest="subject_contains", help="Filter catalog by subject name substring")
    ic.add_argument("--survey-contains", dest="survey_contains", help="Filter catalog by survey name substring")
    ic.add_argument("--limit", type=int, default=None)
    ic.add_argument("--concurrent", type=int, default=8, help="ingestion concurrency")
    ic.add_argument("--probe-concurrent", type=int, default=None, help="coverage probe concurrency (defaults to --concurrent)")
    ic.set_defaults(func=_cmd_ingest_coverage)

    # build links
    bl = sub.add_parser("build-links", help="(Re)build link indexes for tables")
    bl.add_argument("table_ids", type=int, nargs="*", help="Specific table IDs")
    bl.add_argument("--all", action="store_true", help="Process all ingested tables")
    bl.set_defaults(func=_cmd_build_links)

    # search
    st = sub.add_parser("search", help="Search tables with unified boolean queries")
    st.add_argument(
        "--q",
        dest="q",
        help="Unified boolean query (e.g. 'title~\"taxa\" AND (N6>=5000)')",
    )
    st.add_argument(
        "--title",
        dest="title",
        help='[deprecated] Title filter; prefer --q \'title~"..."\'',
    )
    st.add_argument(
        "--survey-contains",
        dest="survey_contains",
        help="[deprecated] Survey substring filter; use --q",
    )
    st.add_argument(
        "--subject-contains",
        dest="subject_contains",
        help="[deprecated] Subject substring filter; use --q",
    )
    st.add_argument(
        "--var",
        dest="var",
        action="append",
        help="[deprecated] Variable name (repeatable); use --q",
    )
    st.add_argument(
        "--class",
        dest="cls",
        action="append",
        help="[deprecated] Class or 'Class:Category' (repeatable); use --q",
    )
    st.add_argument(
        "--coverage",
        help="[deprecated] Coverage expression; include directly in --q",
    )
    st.add_argument("--limit", type=int, default=20)
    st.add_argument("--no-fuzzy", action="store_true")
    st.add_argument("--var-th", type=float, default=0.74)
    st.add_argument("--class-th", type=float, default=0.78)
    st.add_argument("--semantic", action="store_true", help="use semantic title ranking (requires embeddings)")
    st.add_argument("--explain", action="store_true", help="print match rationale and scores")
    st.add_argument("--json", action="store_true")
    st.add_argument("--show-classes", action="store_true", help="List up to 3 classification names for each hit")
    st.add_argument("--debug-fuzzy", action="store_true", help="Print fuzzy expansions and candidate counts")
    st.set_defaults(func=_cmd_search_tables)

    # embeddings backfill (ADD BEFORE RETURN)
    et = sub.add_parser("embed-titles", help="(Re)embed table titles (idempotent)")
    et.add_argument("--model", help="Embedding model name (defaults to settings)")
    et.add_argument("--only-missing", action="store_true", help="Skip rows that already have an embedding, even if text changed")
    et.add_argument("--limit", type=int, default=None, help="Limit number of tables processed")
    et.set_defaults(func=_cmd_embed_titles)

    return p

def main(argv: Sequence[str] | None = None) -> None:
    parser = build_parser()
    args = parser.parse_args(list(argv) if argv is not None else None)

    if getattr(args, "manual", False):
        _cmd_manual(args)
        return

    # Support nested: "db migrate"/"db stats"
    if getattr(args, "cmd", None) == "db" and not hasattr(args, "func"):
        parser.parse_args(["db", "-h"])
        return

    if not hasattr(args, "func"):
        parser.print_help()
        return
    args.func(args)


if __name__ == "__main__":
    main()



###############################################################################
### FILE: cli/__main__.py
###############################################################################
from . import main

if __name__ == "__main__":
    main()



###############################################################################
### FILE: config.py
###############################################################################
from __future__ import annotations

import os
from dataclasses import dataclass
from functools import lru_cache


@dataclass(frozen=True)
class Settings:
    # HTTP
    sidra_base_url: str = "https://servicodados.ibge.gov.br/api/v3/agregados"
    request_timeout: float = 30.0
    request_retries: int = 3
    user_agent: str = "sidra-search/0.1"

    # DB
    database_timeout: float = 60.0
    municipality_national_threshold: int = 5000

    # Embeddings (optional, for title semantics)
    embedding_api_url: str = "http://127.0.0.1:1234/v1/embeddings"
    embedding_model: str = "text-embedding-qwen3-embedding-0.6b@f16"

    # Features
    enable_titles_fts: bool = True
    enable_title_embeddings: bool = True


def _env(name: str) -> str | None:
    for key in (name, name.upper(), name.lower()):
        if key in os.environ:
            return os.environ[key]
    return None


@lru_cache(maxsize=1)
def get_settings() -> Settings:
    d = Settings().__dict__.copy()

    # strings
    for f in ("sidra_base_url", "user_agent", "embedding_api_url", "embedding_model"):
        v = _env(f"SIDRA_SEARCH_{f.upper()}")
        if v:
            d[f] = v

    # numerics/bools
    def _float(env, key):
        v = _env(env)
        if v:
            try: d[key] = float(v)
            except: pass

    def _int(env, key):
        v = _env(env)
        if v:
            try: d[key] = int(v)
            except: pass

    def _bool(env, key):
        v = _env(env)
        if v is not None:
            d[key] = v not in ("0", "false", "False", "")

    _float("SIDRA_SEARCH_REQUEST_TIMEOUT", "request_timeout")
    _float("SIDRA_SEARCH_DATABASE_TIMEOUT", "database_timeout")
    _int("SIDRA_SEARCH_REQUEST_RETRIES", "request_retries")
    _int("SIDRA_SEARCH_MUNICIPALITY_NATIONAL_THRESHOLD", "municipality_national_threshold")
    _bool("SIDRA_SEARCH_ENABLE_TITLES_FTS", "enable_titles_fts")
    _bool("SIDRA_SEARCH_ENABLE_TITLE_EMBEDDINGS", "enable_title_embeddings")

    return Settings(**d)



###############################################################################
### FILE: db/base_schema.py
###############################################################################
from __future__ import annotations

TABLE_STATEMENTS: tuple[str, ...] = (
    """
    CREATE TABLE IF NOT EXISTS agregados (
        id INTEGER PRIMARY KEY,
        nome TEXT NOT NULL,
        pesquisa TEXT,
        assunto TEXT,
        url TEXT,
        freq TEXT,
        periodo_inicio TEXT,
        periodo_fim TEXT,
        raw_json BLOB NOT NULL,
        fetched_at TEXT NOT NULL,
        municipality_locality_count INTEGER DEFAULT 0,
        covers_national_municipalities INTEGER DEFAULT 0
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS agregados_levels (
        agregado_id INTEGER NOT NULL,
        level_id TEXT NOT NULL,
        level_name TEXT,
        level_type TEXT NOT NULL,
        locality_count INTEGER DEFAULT 0,
        PRIMARY KEY (agregado_id, level_id, level_type),
        FOREIGN KEY (agregado_id) REFERENCES agregados(id)
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS variables (
        id          INTEGER NOT NULL,
        agregado_id INTEGER NOT NULL,
        nome        TEXT NOT NULL,
        unidade     TEXT,
        sumarizacao TEXT,
        text_hash   TEXT NOT NULL,
        PRIMARY KEY (agregado_id, id),
        FOREIGN KEY (agregado_id) REFERENCES agregados(id)
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS classifications (
        id INTEGER NOT NULL,
        agregado_id INTEGER NOT NULL,
        nome TEXT NOT NULL,
        sumarizacao_status INTEGER,
        sumarizacao_excecao TEXT,
        PRIMARY KEY (agregado_id, id),
        FOREIGN KEY (agregado_id) REFERENCES agregados(id)
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS categories (
        agregado_id INTEGER NOT NULL,
        classification_id INTEGER NOT NULL,
        categoria_id INTEGER NOT NULL,
        nome TEXT NOT NULL,
        unidade TEXT,
        nivel INTEGER,
        text_hash TEXT NOT NULL,
        PRIMARY KEY (agregado_id, classification_id, categoria_id),
        FOREIGN KEY (agregado_id, classification_id) REFERENCES classifications(agregado_id, id)
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS periods (
        agregado_id INTEGER NOT NULL,
        periodo_id TEXT NOT NULL,
        literals TEXT NOT NULL,
        modificacao TEXT,
        periodo_ord INTEGER,
        periodo_kind TEXT,
        PRIMARY KEY (agregado_id, periodo_id),
        FOREIGN KEY (agregado_id) REFERENCES agregados(id)
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS localities (
        agregado_id INTEGER NOT NULL,
        level_id TEXT NOT NULL,
        locality_id TEXT NOT NULL,
        nome TEXT NOT NULL,
        PRIMARY KEY (agregado_id, level_id, locality_id),
        FOREIGN KEY (agregado_id, level_id) REFERENCES agregados_levels(agregado_id, level_id)
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS ingestion_log (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        agregado_id INTEGER NOT NULL,
        stage TEXT NOT NULL,
        status TEXT NOT NULL,
        detail TEXT,
        run_at TEXT NOT NULL
    )
    """,
)

INDEX_STATEMENTS: tuple[str, ...] = (
    "CREATE INDEX IF NOT EXISTS idx_variables_agregado ON variables(agregado_id)",
    "CREATE INDEX IF NOT EXISTS idx_categories_agregado ON categories(agregado_id, classification_id)",
    "CREATE INDEX IF NOT EXISTS idx_localities_agregado ON localities(agregado_id, level_id)",
    "CREATE UNIQUE INDEX IF NOT EXISTS u_agregados_levels_pair ON agregados_levels(agregado_id, level_id)",
    "CREATE INDEX IF NOT EXISTS idx_periods_agregado_ord ON periods(agregado_id, periodo_ord)",
)

def apply_base_schema(connection) -> None:
    cur = connection.cursor()
    for stmt in TABLE_STATEMENTS:
        cur.execute(stmt)
    for stmt in INDEX_STATEMENTS:
        cur.execute(stmt)
    connection.commit()



###############################################################################
### FILE: db/migrations.py
###############################################################################
from __future__ import annotations

import sqlite3

from .search_schema import apply_search_schema as _apply_schema

SEARCH_SCHEMA_VERSION = 4
KEY = "sidra_search_schema_version"

def _ensure_meta(connection: sqlite3.Connection) -> None:
    connection.execute("""
        CREATE TABLE IF NOT EXISTS meta_kv(
          key TEXT PRIMARY KEY,
          value TEXT NOT NULL
        )
    """)

def get_search_schema_version(connection: sqlite3.Connection) -> int:
    _ensure_meta(connection)
    cur = connection.execute("SELECT value FROM meta_kv WHERE key = ?", (KEY,))
    row = cur.fetchone()
    return int(row[0]) if row else 0

def bump_search_schema_version(connection: sqlite3.Connection, to_version: int) -> None:
    _ensure_meta(connection)
    connection.execute(
        "INSERT INTO meta_kv(key,value) VALUES(?,?) "
        "ON CONFLICT(key) DO UPDATE SET value=excluded.value",
        (KEY, str(to_version)),
    )

def apply_search_schema(connection: sqlite3.Connection) -> None:
    current = get_search_schema_version(connection)
    if current >= SEARCH_SCHEMA_VERSION:
        return
    with connection:
        _apply_schema(connection)
        bump_search_schema_version(connection, SEARCH_SCHEMA_VERSION)



###############################################################################
### FILE: db/search_schema.py
###############################################################################
from __future__ import annotations

DDL: tuple[str, ...] = (
    # meta kv (shared)
    """
    CREATE TABLE IF NOT EXISTS meta_kv(
      key TEXT PRIMARY KEY,
      value TEXT NOT NULL
    )
    """,
    # link keys
    """
    CREATE TABLE IF NOT EXISTS name_keys (
      kind TEXT NOT NULL,     -- 'var' | 'class' | 'cat'
      key  TEXT NOT NULL,     -- normalized
      raw  TEXT NOT NULL,     -- original
      UNIQUE(kind, key, raw)
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS link_var (
      var_key    TEXT NOT NULL,
      table_id   INTEGER NOT NULL,
      variable_id INTEGER NOT NULL,
      UNIQUE(var_key, table_id, variable_id)
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS link_class (
      class_key TEXT NOT NULL,
      table_id  INTEGER NOT NULL,
      class_id  INTEGER NOT NULL,
      UNIQUE(class_key, table_id, class_id)
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS link_cat (
      class_key  TEXT NOT NULL,
      cat_key    TEXT NOT NULL,
      table_id   INTEGER NOT NULL,
      class_id   INTEGER NOT NULL,
      category_id INTEGER NOT NULL,
      UNIQUE(class_key, cat_key, table_id, class_id, category_id)
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS link_var_class (
      var_key   TEXT NOT NULL,
      class_key TEXT NOT NULL,
      table_id  INTEGER NOT NULL,
      variable_id INTEGER NOT NULL,
      class_id    INTEGER NOT NULL,
      UNIQUE(var_key, class_key, table_id, variable_id, class_id)
    )
    """,
    # FTS for titles (no extra index allowed on virtual table)
    """
    CREATE VIRTUAL TABLE IF NOT EXISTS table_titles_fts
    USING fts5(table_id UNINDEXED, title, survey, subject, tokenize='unicode61')
    """,
    # Embeddings table (generic, reused for table titles)
    """
    CREATE TABLE IF NOT EXISTS embeddings (
      entity_type TEXT NOT NULL,   -- 'agregado'
      entity_id   TEXT NOT NULL,   -- table_id as text
      agregado_id INTEGER,         -- optional convenience (same as entity_id)
      text_hash   TEXT NOT NULL,
      model       TEXT NOT NULL,
      dimension   INTEGER NOT NULL,
      vector      BLOB NOT NULL,
      created_at  TEXT NOT NULL,
      PRIMARY KEY (entity_type, entity_id, model)
    )
    """,
)

INDEXES: tuple[str, ...] = (
    "CREATE INDEX IF NOT EXISTS idx_link_var_key   ON link_var(var_key)",
    "CREATE INDEX IF NOT EXISTS idx_link_class_key ON link_class(class_key)",
    "CREATE INDEX IF NOT EXISTS idx_link_cat_keys  ON link_cat(class_key, cat_key)",
    "CREATE INDEX IF NOT EXISTS idx_link_var_class ON link_var_class(var_key, class_key)",
    "CREATE INDEX IF NOT EXISTS idx_embeddings_agregado ON embeddings(agregado_id, model)",
)

def apply_search_schema(connection) -> None:
    cur = connection.cursor()
    for stmt in DDL:
        cur.execute(stmt)
    for stmt in INDEXES:
        cur.execute(stmt)
    connection.commit()



###############################################################################
### FILE: db/session.py
###############################################################################
from __future__ import annotations

import os
import sqlite3
from contextlib import contextmanager
from pathlib import Path
from typing import Iterator

from ..config import get_settings
from .base_schema import apply_base_schema
from .migrations import apply_search_schema


def _has_base_tables(path: Path) -> bool:
    if not path.exists():
        return False
    try:
        conn = sqlite3.connect(path)
        cur = conn.execute(
            "SELECT name FROM sqlite_master WHERE type='table' AND name IN ('agregados','variables')"
        )
        names = {r[0] for r in cur.fetchall()}
        return "agregados" in names and "variables" in names
    except sqlite3.Error:
        return False
    finally:
        try: conn.close()
        except: pass


def _resolve_db_path() -> Path:
    env = os.getenv("SIDRA_DATABASE_PATH")
    if env:
        p = Path(env).expanduser().resolve()
        p.parent.mkdir(parents=True, exist_ok=True)
        return p
    # default
    p = Path("sidra.db").resolve()
    p.parent.mkdir(parents=True, exist_ok=True)
    return p


_DB_PATH: Path | None = None

def get_database_path() -> Path:
    global _DB_PATH
    if _DB_PATH is None:
        _DB_PATH = _resolve_db_path()
    return _DB_PATH


def create_connection() -> sqlite3.Connection:
    s = get_settings()
    conn = sqlite3.connect(
        get_database_path(),
        timeout=max(float(s.database_timeout), 30.0),
        check_same_thread=False,
    )
    conn.row_factory = sqlite3.Row

    # Safety + performance (no semantic change):
    conn.execute("PRAGMA foreign_keys=ON")
    conn.execute("PRAGMA journal_mode=WAL")
    conn.execute("PRAGMA synchronous=NORMAL")  # durable enough, already in code
    conn.execute(f"PRAGMA busy_timeout = {int(max(s.database_timeout, 60.0) * 1000)}")

    # Added: speed up big batch inserts & temp btrees
    # cache_size: negative => value is in KiB (here ~128 MiB)
    conn.execute("PRAGMA cache_size = -131072")
    conn.execute("PRAGMA temp_store = MEMORY")
    # Use mmap when supported (safe fallback if OS/FS refuses)
    try:
        conn.execute("PRAGMA mmap_size = 268435456")  # 256 MiB
    except Exception:
        pass

    return conn


@contextmanager
def sqlite_session() -> Iterator[sqlite3.Connection]:
    conn = create_connection()
    try:
        yield conn
    finally:
        conn.close()


def ensure_full_schema() -> None:
    conn = create_connection()
    try:
        apply_base_schema(conn)
        apply_search_schema(conn)
        conn.commit()
    finally:
        conn.close()



###############################################################################
### FILE: ingest/bulk.py
###############################################################################
from __future__ import annotations

import asyncio
from dataclasses import dataclass, field
from typing import Any, Iterable, Sequence

from ..net.api_client import SidraApiClient
from ..db.session import sqlite_session, ensure_full_schema
from .ingest_table import ingest_table
from ..search.coverage import parse_coverage_expr, extract_levels, eval_coverage
from ..config import get_settings
from ..search.normalize import normalize_basic

@dataclass(frozen=True)
class CatalogEntry:
    id: int
    nome: str | None
    pesquisa: str | None
    pesquisa_id: int | None
    assunto: str | None
    assunto_id: int | None
    periodicidade: Any
    nivel_territorial: dict[str, list[str]]
    level_hints: frozenset[str] = frozenset()

    @property
    def level_codes(self) -> set[str]:
        codes: set[str] = set()
        for vals in self.nivel_territorial.values():
            for v in vals:
                codes.add(str(v).upper())
        codes.update(self.level_hints)
        return codes

def _normalize_levels(payload: Any) -> dict[str, list[str]]:
    result: dict[str, list[str]] = {}
    if isinstance(payload, dict):
        for k, vals in payload.items():
            arr = vals if isinstance(vals, list) else [vals]
            out = []
            for it in arr:
                if isinstance(it, str):
                    out.append(it.upper())
                elif isinstance(it, dict):
                    code = it.get("codigo") or it.get("nivel") or it.get("id")
                    if isinstance(code, str): out.append(code.upper())
            if out: result[str(k)] = out
    return result

def _extract_subject_fields(survey: Any, ag: Any) -> tuple[str | None, int | None]:
    """
    Try hard to extract a human-readable subject name and id
    from either the table (agregado) or the survey object.
    Handles: plain str, nested dict {'nome','id'}, and a few common aliases.
    """
    def _name_and_id(obj: Any) -> tuple[str | None, int | None]:
        if not isinstance(obj, dict):
            return None, None
        v = obj.get("assunto")
        # variante 1: nested object
        if isinstance(v, dict):
            name = v.get("nome") or v.get("name")
            sid  = v.get("id") or obj.get("idAssunto")
            return (name if isinstance(name, str) else None,
                    int(sid) if isinstance(sid, (int, str)) and str(sid).isdigit() else None)
        # variante 2: plain string
        if isinstance(v, str):
            sid = obj.get("idAssunto")
            return v, int(sid) if isinstance(sid, (int, str)) and str(sid).isdigit() else None
        # variante 3: loose aliases some payloads use
        for k in ("assuntoNome", "assunto_nome", "assuntoDescricao", "assunto_descricao"):
            w = obj.get(k)
            if isinstance(w, str):
                sid = obj.get("idAssunto")
                return w, int(sid) if isinstance(sid, (int, str)) and str(sid).isdigit() else None
        return None, None

    # Prefer table-level, fallback to survey-level
    a_name, a_id = _name_and_id(ag)
    if a_name:
        return a_name, a_id
    s_name, s_id = _name_and_id(survey)
    return s_name, s_id

async def fetch_catalog_entries(
    *, client: SidraApiClient | None = None, subject_id: int | None = None,
    periodicity: str | None = None, levels: Sequence[str] | None = None
) -> list[CatalogEntry]:
    own = False
    if client is None:
        client = SidraApiClient()
        own = True
    normalized_levels = [c.upper() for c in levels or [] if c]
    try:
        catalog = await client.fetch_catalog(subject_id=subject_id, periodicity=periodicity, levels=normalized_levels or None)
    finally:
        if own:
            await client.close()

    out: list[CatalogEntry] = []
    if not isinstance(catalog, list): return out
    for survey in catalog:
        ags = survey.get("agregados") if isinstance(survey, dict) else None
        if not isinstance(ags, list): continue
        for ag in ags:
            if not isinstance(ag, dict): continue
            assunto_name, assunto_id = _extract_subject_fields(survey, ag)

            entry = CatalogEntry(
                id = int(ag.get("id")),
                nome = ag.get("nome") or ag.get("tabela"),
                pesquisa = survey.get("pesquisa") or survey.get("nome"),
                pesquisa_id = survey.get("idPesquisa") or survey.get("id"),
                assunto = assunto_name,
                assunto_id = assunto_id,
                periodicidade = survey.get("periodicidade"),
                nivel_territorial = _normalize_levels(ag.get("nivelTerritorial")),
                level_hints = frozenset(c.upper() for c in normalized_levels),
            )
            out.append(entry)
    return out

def filter_catalog_entries(
    entries: Sequence[CatalogEntry], *,
    require_any_levels: Iterable[str] | None = None,
    require_all_levels: Iterable[str] | None = None,
    exclude_levels: Iterable[str] | None = None,
    subject_contains: str | None = None,
    survey_contains: str | None = None,
) -> list[CatalogEntry]:
    any_levels = {c.upper() for c in require_any_levels or ()}
    all_levels = {c.upper() for c in require_all_levels or ()}
    excluded  = {c.upper() for c in exclude_levels or ()}
    subj_q = normalize_basic(subject_contains) if subject_contains else None
    surv_q = normalize_basic(survey_contains) if survey_contains else None

    out: list[CatalogEntry] = []
    for e in entries:
        codes = e.level_codes
        if any_levels and not (codes & any_levels): continue
        if all_levels and not all_levels.issubset(codes): continue
        if excluded and (codes & excluded): continue
        subj_val = normalize_basic(e.assunto or "")
        surv_val = normalize_basic(e.pesquisa or "")
        if subj_q and subj_q not in subj_val: continue
        if surv_q and surv_q not in surv_val: continue
        out.append(e)
    return out


@dataclass
class BulkReport:
    discovered_ids: list[int] = field(default_factory=list)
    scheduled_ids: list[int] = field(default_factory=list)
    skipped_existing: list[int] = field(default_factory=list)
    ingested_ids: list[int] = field(default_factory=list)
    failed: list[tuple[int, str]] = field(default_factory=list)

async def _probe_counts_for_levels(
    client: SidraApiClient,
    table_id: int,
    levels: Iterable[str],
) -> tuple[dict[str, int], dict[str, list]]:
    """
    Fetch locality lists per level for a table and return:
      - counts: {LEVEL: count}
      - payloads: {LEVEL: list_of_localities}  (for reuse during ingest)
    Levels are normalized to UPPERCASE strings. Non-list responses are treated as empty lists.
    """
    lvls = sorted({str(l).upper() for l in levels if l})
    if not lvls:
        return {}, {}

    async def one(lvl: str) -> tuple[str, list]:
        try:
            payload = await client.fetch_localities(table_id, lvl)
            if isinstance(payload, list):
                return (lvl, payload)
            try:
                return (lvl, list(payload))
            except Exception:
                return (lvl, [])
        except Exception:
            return (lvl, [])

    results = await asyncio.gather(*(one(l) for l in lvls))
    payloads: dict[str, list] = {k: v for (k, v) in results}
    counts: dict[str, int] = {k: (len(v) if isinstance(v, list) else 0) for k, v in payloads.items()}
    return counts, payloads


# Use local DB counts (agregados_levels) for specific levels, if present
def _db_counts_for_levels(table_id: int, levels: Iterable[str]) -> dict[str, int]:
    lvls = [str(l).upper() for l in levels if l]
    if not lvls:
        return {}
    with sqlite_session() as conn:
        placeholders = ",".join("?" for _ in lvls)
        sql = (
            f"SELECT level_id, COALESCE(locality_count,0) AS c "
            f"FROM agregados_levels "
            f"WHERE agregado_id=? AND UPPER(level_id) IN ({placeholders})"
        )
        rows = conn.execute(sql, (table_id, *lvls)).fetchall()
    return {str(r["level_id"]).upper(): int(r["c"]) for r in rows}


async def _subject_gate_with_metadata(
    *,
    entries: list["CatalogEntry"],
    subject_contains: str,
    client: SidraApiClient,
    need: int | None,
    parallel: int,
) -> list["CatalogEntry"]:
    """
    Keep only entries whose SUBJECT matches `subject_contains`.
    Prefer catalog subject; otherwise check local DB; otherwise fetch /{id}/metadados.
    Bounded concurrency, early-stop, and progress ticks.
    """
    q = normalize_basic(subject_contains)
    if not q:
        return entries

    total = len(entries)
    kept: list[CatalogEntry] = []
    done = 0

    print(f"[subject] filtering {total} entries by assunto~{subject_contains!r} (parallel={parallel})")

    async def check_one(e: CatalogEntry) -> tuple[CatalogEntry, bool]:
        # 0) DB cache (already ingested)
        with sqlite_session() as conn:
            row = conn.execute("SELECT assunto FROM agregados WHERE id=?", (e.id,)).fetchone()
        if row and row[0]:
            subj_db = normalize_basic(str(row[0] or ""))
            if subj_db and (q in subj_db):
                return e, True
            # DB had a subject and it didn't match → skip network
            return e, False

        # 1) catalog-provided subject
        subj_cat = normalize_basic(e.assunto or "")
        if subj_cat and (q in subj_cat):
            return e, True

        # 2) fallback: metadata
        per_table_timeout = max(10.0, 2.0 * float(get_settings().request_timeout))
        try:
            md = await asyncio.wait_for(client.fetch_metadata(e.id), timeout=per_table_timeout)
        except Exception:
            return e, False

        subj = md.get("assunto")
        if isinstance(subj, dict):
            subj = subj.get("nome")
        subj = normalize_basic(str(subj or ""))
        return e, (q in subj)

    sem = asyncio.Semaphore(max(1, int(parallel)))
    it = iter(entries)
    pending: set[asyncio.Task] = set()

    async def schedule_next() -> bool:
        try:
            e = next(it)
        except StopIteration:
            return False
        async def _task():
            async with sem:
                return await check_one(e)
        t = asyncio.create_task(_task())
        pending.add(t)
        return True

    # prime
    for _ in range(parallel):
        if not await schedule_next():
            break

    # drain with progress + early-stop
    while pending:
        done_set, pending = await asyncio.wait(pending, return_when=asyncio.FIRST_COMPLETED)
        for t in done_set:
            e, ok = await t
            done += 1
            if ok:
                kept.append(e)
            if (done % 10 == 0) or (need and len(kept) >= need):
                pct = (done * 100) // max(1, total)
                print(f"\r[subject] checked={done}/{total} kept={len(kept)} ({pct}%)", end="", flush=True)
            if need and len(kept) >= need:
                pending.clear()
                break
        while len(pending) < parallel:
            more = await schedule_next()
            if not more:
                break
    print()  # newline

    return kept


async def ingest_by_coverage(
    *,
    coverage: str,
    subject_contains: str | None = None,
    survey_contains: str | None = None,
    limit: int | None = None,
    concurrency: int = 8,
    probe_concurrent: int | None = None,
) -> BulkReport:
    """
    Discover tables that satisfy the coverage expression and ingest them.

    - Catalog fetch pruned by mentioned levels (cheap).
    - Optional text narrowing on survey (cheap).
    - Optional subject narrowing that falls back to metadata (bounded + early stop).
    - Coverage probe (bounded + early stop).
    """
    ensure_full_schema()
    report = BulkReport()
    
    # Cache per-table locality payloads for levels we probed (e.g., N3/N6)
    prefetched_localities: dict[int, dict[str, list]] = {}

    # Parse coverage and extract hinted levels
    try:
        cov_ast = parse_coverage_expr(coverage)
    except Exception as exc:
        raise RuntimeError(f"Invalid --coverage expression: {coverage!r}: {exc}") from exc
    level_hints = extract_levels(cov_ast)

    need = max(0, int(limit)) if (limit is not None and limit >= 0) else None
    par_probe = max(1, int(probe_concurrent or concurrency))

    async with SidraApiClient() as client:
        # 1) Catalog fetch with hinted levels (server-side pruning)
        entries = await fetch_catalog_entries(client=client, levels=sorted(level_hints) or None)

        # 2) Survey narrowing (accent-insensitive, already implemented in filter_catalog_entries)
        entries = filter_catalog_entries(
            entries,
            require_any_levels=None,
            require_all_levels=None,
            exclude_levels=None,
            subject_contains=None,           # handled below via metadata fallback
            survey_contains=survey_contains,
        )
        if not entries:
            return report

        # 3) Subject narrowing with metadata fallback (bounded, early-stop)
        if subject_contains:
            # probe a bit more than 'need' so the coverage step still has room to filter
            subj_need = need if need is not None else None
            entries = await _subject_gate_with_metadata(
                entries=entries,
                subject_contains=subject_contains,
                client=client,
                need=subj_need,
                parallel=par_probe,
            )
            if not entries:
                return report

        total = len(entries)
        print(f"[probe] candidates={total}, levels={sorted(level_hints) or []}, parallel={par_probe}")

        # 4) Coverage probe (bounded, early-stop)
        kept: list[CatalogEntry] = []
        pending: set[asyncio.Task] = set()
        it = iter(entries)

        async def schedule_next() -> bool:
            try:
                e = next(it)
            except StopIteration:
                return False

            check_levels = (level_hints & e.level_codes) if level_hints else set()
            if not check_levels:
                # Nothing to check for this table per coverage expression → accept (no network)
                async def _one_accept():
                    return (e, True, {})
                pending.add(asyncio.create_task(_one_accept()))
                return True

            # 1) Try local DB counts first (only if table already ingested)
            db_counts = await asyncio.to_thread(_db_counts_for_levels, e.id, check_levels)
            have_all = bool(db_counts) and all(l.upper() in db_counts for l in check_levels)
            if have_all:
                ok_local = eval_coverage(cov_ast, db_counts)
                async def _one_db():
                    return (e, ok_local, {})
                pending.add(asyncio.create_task(_one_db()))
                return True

            # 2) Fallback: live probe from API (also returns payloads we can reuse)
            async def _one_net(e_: CatalogEntry) -> tuple[CatalogEntry, bool, dict[str, list]]:
                per_table_timeout = max(10.0, 2.0 * float(get_settings().request_timeout))
                try:
                    counts, payloads = await asyncio.wait_for(
                        _probe_counts_for_levels(client, e_.id, check_levels),
                        timeout=per_table_timeout,
                    )
                    ok = eval_coverage(cov_ast, counts)
                    return e_, ok, (payloads if ok else {})
                except Exception:
                    return e_, False, {}

            pending.add(asyncio.create_task(_one_net(e)))
            return True

        # prime
        for _ in range(par_probe):
            if not await schedule_next():
                break

        probed = 0
        stop = False
        while pending:
            done, pending = await asyncio.wait(pending, return_when=asyncio.FIRST_COMPLETED)
            for t in done:
                probed += 1
                e, ok, payloads = await t
                if ok:
                    kept.append(e)
                    if payloads:
                        prefetched_localities[e.id] = payloads
                if (probed % 10 == 0) or (need and len(kept) >= need):
                    pct = (probed * 100) // max(1, total)
                    inflight = len(pending)
                    print(
                        f"\r[probe] done={probed}/{total} (accepted={len(kept)} • {pct}%) | inflight={inflight}",
                        end="",
                        flush=True,
                    )
                if need and len(kept) >= need:
                    stop = True
                    pending.clear()
                    break
            if stop:
                break

            while len(pending) < par_probe:
                more = await schedule_next()
                if not more:
                    break

        print()  # newline after progress

        if need is not None:
            kept = kept[:need]
        report.discovered_ids = [e.id for e in kept]

        # 5) Skip existing + ingest (+ include historical failures)
        with sqlite_session() as conn:
            existing = {int(r[0]) for r in conn.execute("SELECT id FROM agregados")}
            past_failed = {
                int(r[0])
                for r in conn.execute(
                    "SELECT DISTINCT agregado_id FROM ingestion_log WHERE status='error'"
                )
            }

        # schedule = new-kept (not existing)  ∪  (historical failures not existing)
        new_kept = [e.id for e in kept if e.id not in existing]
        hist_retry = sorted(past_failed - existing)

        to_do = list(dict.fromkeys([*new_kept, *hist_retry]))  # preserve order, dedupe

        report.discovered_ids = [e.id for e in kept]  # unchanged semantics
        report.scheduled_ids = list(to_do)
        report.skipped_existing = [e.id for e in kept if e.id in existing]

        if not to_do:
            return report

        print(f"[ingest] {len(to_do)} tables (parallel={concurrency})")
        sem = asyncio.Semaphore(max(1, concurrency))

        async def worker(tid: int) -> None:
            async with sem:
                # Per-table retry with short backoff (other tasks continue)
                delays = (1.0, 3.0, 7.0)  # seconds
                attempt = 0
                while True:
                    try:
                        await ingest_table(tid, prefetched_localities=prefetched_localities.get(tid))
                        report.ingested_ids.append(tid)
                        print(f"  ingested {tid}")
                        return
                    except Exception as exc:
                        msg = str(exc)
                        attempt += 1

                        # Persistent server-side 500? Fail fast after 2 tries.
                        if "statusCode\":500" in msg or " 500:" in msg:
                            if attempt >= 2:
                                report.failed.append((tid, msg[:200]))
                                print(f"  failed {tid}: {exc} (permanent 500?)")
                                return

                        if attempt > len(delays):
                            report.failed.append((tid, msg[:200]))
                            print(f"  failed {tid}: {exc}")
                            return

                        backoff = delays[attempt - 1]
                        print(f"  retry {tid} in {backoff:.0f}s (attempt {attempt+1})")
                        await asyncio.sleep(backoff)


        await asyncio.gather(*(worker(t) for t in to_do))

    return report



###############################################################################
### FILE: ingest/ingest_table.py
###############################################################################
from __future__ import annotations

import asyncio
import hashlib
from array import array
from datetime import datetime, timezone
from typing import Any, Sequence

import orjson

from ..config import get_settings
from ..db.session import sqlite_session
from ..db.migrations import apply_search_schema
from ..db.base_schema import apply_base_schema
from ..ingest.links import build_links_for_table
from ..net.api_client import SidraApiClient
from ..net.embedding_client import EmbeddingClient
from ..search.fuzzy3gram import reset_cache

ISO = "%Y-%m-%dT%H:%M:%SZ"


def _hash_fields(*vals: object) -> str:
    # Robust: stringify, replace None with "", and hash a stable joined string
    return _sha256_text("||".join("" if v is None else str(v) for v in vals))

def _now() -> str:
    return datetime.now(timezone.utc).strftime(ISO)


def _json(obj: Any) -> bytes:
    return orjson.dumps(obj)


def _json_text(obj: Any) -> str:
    return orjson.dumps(obj).decode("utf-8")


def _sha256_text(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()


def _period_to_ord_kind(periodo_id: Any) -> tuple[int | None, str]:
    """
    YYYY -> (YYYY00, 'Y')
    YYYYMM -> (YYYYMM, 'YM')
    YYYYMMDD -> (YYYYMMDD, 'YMD')
    otherwise -> (None, 'UNK')
    """
    s = str(periodo_id or "").strip()
    digits = "".join(ch for ch in s if ch.isdigit())
    if len(digits) == 4:
        return int(digits + "00"), "Y"
    if len(digits) == 6:
        return int(digits), "YM"
    if len(digits) == 8:
        return int(digits), "YMD"
    return None, "UNK"


def _canonical_table_text(md: dict[str, Any]) -> str:
    periodicidade = md.get("periodicidade") or {}
    freq = periodicidade.get("frequencia")
    inicio = periodicidade.get("inicio")
    fim = periodicidade.get("fim")

    if inicio and fim:
        period_line = f"Period: {inicio} - {fim}" if inicio != fim else f"Period: {inicio}"
    elif inicio or fim:
        period_line = f"Period: {inicio or fim}"
    else:
        period_line = None

    nivel = md.get("nivelTerritorial") or {}
    level_parts: list[str] = []
    if isinstance(nivel, dict):
        for level_type in sorted(nivel):
            codes = nivel.get(level_type) or []
            if codes:
                level_parts.append(f"{level_type}: {', '.join(str(c) for c in codes)}")

    lines = [
        f"Table {str(md.get('id'))}: {str(md.get('nome') or '').strip()}".strip(),
        f"Survey: {str(md.get('pesquisa') or '')}" if md.get("pesquisa") else None,
        f"Subject: {str(md.get('assunto') or '')}" if md.get("assunto") else None,
        f"Frequency: {str(freq)}" if freq else None,
        period_line,
        f"Territorial levels: {'; '.join(level_parts)}" if level_parts else None,
        f"URL: {str(md.get('URL') or '')}" if md.get("URL") else None,
    ]
    # every yielded piece is a str; ignore falsy items
    return "\n".join([s for s in lines if isinstance(s, str) and s])



def _vec_to_blob(vec: Sequence[float]) -> bytes:
    arr = array("f", (float(x) for x in vec))
    return arr.tobytes()


async def _ensure_embeddings(table_id: int, text: str, *, embedding_client: EmbeddingClient) -> None:
    if not text.strip():
        return
    text_hash = _sha256_text(text)
    model = embedding_client.model

    def _read_existing() -> str | None:
        with sqlite_session() as conn:
            cur = conn.execute(
                "SELECT text_hash FROM embeddings "
                "WHERE entity_type='agregado' AND entity_id=? AND model=?",
                (str(table_id), model),
            ).fetchone()
            return cur["text_hash"] if cur else None

    existing_hash = await asyncio.to_thread(_read_existing)
    if existing_hash == text_hash:
        return

    # compute vector
    vector = await asyncio.to_thread(embedding_client.embed_text, text, model=model)
    dim = len(vector)

    def _write():
        with sqlite_session() as conn:
            with conn:
                conn.execute(
                    """
                    INSERT OR REPLACE INTO embeddings(
                      entity_type, entity_id, agregado_id, text_hash, model, dimension, vector, created_at
                    ) VALUES(?,?,?,?,?,?,?,?)
                    """,
                    (
                        "agregado",
                        str(table_id),
                        table_id,
                        text_hash,
                        model,
                        dim,
                        _vec_to_blob(vector),
                        _now(),
                    ),
                )

    await asyncio.to_thread(_write)


async def ingest_table(
    table_id: int,
    *,
    client: SidraApiClient | None = None,
    embedding_client: EmbeddingClient | None = None,
    build_links: bool = True,
    prefetched_localities: dict[str, list] | None = None,  # optional reuse from probe
) -> None:
    """
    Fetch metadata from SIDRA and persist into base + search schemas.
    Also refresh the table title FTS row and (optionally) a title embedding.
    """
    # ensure schema
    with sqlite_session() as conn:
        apply_base_schema(conn)
        apply_search_schema(conn)
        conn.commit()

    own_client = False
    if client is None:
        client = SidraApiClient()
        own_client = True

    try:
        # --- fetch payloads
        try:
            md = await client.fetch_metadata(table_id)
        except Exception:
            _log_ingestion(table_id, "error", "api:metadata")
            raise
        try:
            periods = await client.fetch_periods(table_id)
        except Exception:
            _log_ingestion(table_id, "error", "api:periods")
            raise

        # --- derive levels + localities (count + names)
        nivel_groups = md.get("nivelTerritorial") or {}
        if not isinstance(nivel_groups, dict):
            nivel_groups = {}

        settings = get_settings()
        municipality_count = 0
        level_rows: list[tuple[int, str, str | None, str, int]] = []
        locality_rows: list[tuple[int, str, str | None, str | None]] = []

        # NOTE:
        # - `prefetched_localities` is a dict like {"N3": [...], "N6": [...]} if passed by the caller.
        # - Keys we store/reuse are UPPERCASE to avoid case mismatches.
        for level_type, codes in (nivel_groups or {}).items():
            if not codes:
                continue
            # Be defensive: ensure we can iterate even if API returned a scalar.
            try:
                code_list = list(codes)
            except TypeError:
                code_list = [codes]

            for code in code_list:
                code_s = str(code)
                code_key = code_s.upper()

                # Prefer the probe's cached payload for this level (e.g., N3/N6).
                payload: list | None = None
                if prefetched_localities and isinstance(prefetched_localities.get(code_key), list):
                    payload = prefetched_localities[code_key]
                else:
                    try:
                        payload = await client.fetch_localities(table_id, code_s)
                    except Exception:
                        payload = []
                    if not isinstance(payload, list):
                        try:
                            payload = list(payload)
                        except Exception:
                            payload = []

                # Count first (coverage uses counts only)
                count = len(payload)
                if code_key == "N6":
                    municipality_count = max(municipality_count, count)

                # Try to extract a representative level_name (same as before)
                level_name = None
                if payload:
                    try:
                        node = payload[0].get("nivel") if isinstance(payload[0], dict) else None
                        if isinstance(node, dict):
                            level_name = node.get("nome")
                    except Exception:
                        level_name = None

                # Record per-level counts
                level_rows.append((table_id, code_s, level_name, str(level_type), count))

                # Persist exact membership (unchanged behavior)
                if payload:
                    for loc in payload:
                        if isinstance(loc, dict):
                            lid = loc.get("id")
                            lname = loc.get("nome")
                        else:
                            lid = None
                            lname = None
                        locality_rows.append((table_id, code_s, lid, lname))


        covers_nat = (
            1
            if (municipality_count >= max(0, int(settings.municipality_national_threshold)))
            else 0
        )

        # variables
        variables = md.get("variaveis") or []
        var_rows: list[tuple[Any, int, Any, Any, str, str]] = []
        for v in variables:
            var_rows.append(
                (
                    v.get("id"),
                    table_id,
                    v.get("nome"),
                    v.get("unidade"),
                    _json_text(v.get("sumarizacao", [])),
                    _hash_fields(v.get("id"), v.get("nome"), v.get("unidade")),   # <— here
                )
            )

        # classifications + categories
        class_rows: list[tuple[int, int, str | None, int, str]] = []
        cat_rows: list[tuple[int, int, int, str | None, str | None, Any, str]] = []
        for cl in md.get("classificacoes", []) or []:
            cid = cl.get("id")
            class_rows.append(
                (
                    cid,
                    table_id,
                    cl.get("nome"),
                    1 if (cl.get("sumarizacao", {}).get("status")) else 0,
                    _json_text(cl.get("sumarizacao", {}).get("excecao", [])),
                )
            )
            for cat in cl.get("categorias", []) or []:
                cat_rows.append(
                    (
                        table_id,
                        cid,
                        cat.get("id"),
                        cat.get("nome"),
                        cat.get("unidade"),
                        cat.get("nivel"),
                        _hash_fields(cid, cat.get("id"), cat.get("nome"), cat.get("unidade")),  # <— here
                    )
                )

        # periods
        period_rows: list[tuple[int, str, str, Any, int | None, str]] = []
        for p in periods or []:
            pid = p.get("id") if isinstance(p, dict) else p
            literals = p.get("literals", [pid]) if isinstance(p, dict) else [pid]
            modificacao = p.get("modificacao") if isinstance(p, dict) else None
            ord_val, kind = _period_to_ord_kind(pid)
            period_rows.append((table_id, str(pid), _json_text(literals), modificacao, ord_val, kind))

        # --- write to DB (replace child rows)
        fetched_at = _now()

        with sqlite_session() as conn:
            apply_base_schema(conn)
            apply_search_schema(conn)

            conn.execute("BEGIN")
            try:
                # parent: agregados header
                conn.execute(
                    """
                    INSERT OR REPLACE INTO agregados(
                      id, nome, pesquisa, assunto, url, freq, periodo_inicio, periodo_fim,
                      raw_json, fetched_at, municipality_locality_count, covers_national_municipalities
                    ) VALUES(?,?,?,?,?,?,?,?,?,?,?,?)
                    """,
                    (
                        md.get("id"),
                        md.get("nome"),
                        md.get("pesquisa"),
                        md.get("assunto"),
                        md.get("URL"),
                        (md.get("periodicidade") or {}).get("frequencia"),
                        (md.get("periodicidade") or {}).get("inicio"),
                        (md.get("periodicidade") or {}).get("fim"),
                        _json(md),
                        fetched_at,
                        municipality_count,
                        covers_nat,
                    ),
                )

                # purge children → insert fresh
                conn.execute("DELETE FROM localities WHERE agregado_id=?", (table_id,))
                conn.execute("DELETE FROM agregados_levels WHERE agregado_id=?", (table_id,))
                conn.execute("DELETE FROM categories WHERE agregado_id=?", (table_id,))
                conn.execute("DELETE FROM classifications WHERE agregado_id=?", (table_id,))
                conn.execute("DELETE FROM variables WHERE agregado_id=?", (table_id,))
                conn.execute("DELETE FROM periods WHERE agregado_id=?", (table_id,))

                if level_rows:
                    conn.executemany(
                        """
                        INSERT OR REPLACE INTO agregados_levels(
                          agregado_id, level_id, level_name, level_type, locality_count
                        ) VALUES(?,?,?,?,?)
                        """,
                        level_rows,
                    )
                if var_rows:
                    conn.executemany(
                        """
                        INSERT OR REPLACE INTO variables(
                          id, agregado_id, nome, unidade, sumarizacao, text_hash
                        ) VALUES(?,?,?,?,?,?)
                        """,
                        var_rows,
                    )
                if class_rows:
                    conn.executemany(
                        """
                        INSERT OR REPLACE INTO classifications(
                          id, agregado_id, nome, sumarizacao_status, sumarizacao_excecao
                        ) VALUES(?,?,?,?,?)
                        """,
                        class_rows,
                    )
                if cat_rows:
                    conn.executemany(
                        """
                        INSERT OR REPLACE INTO categories(
                          agregado_id, classification_id, categoria_id, nome, unidade, nivel, text_hash
                        ) VALUES(?,?,?,?,?,?,?)
                        """,
                        cat_rows,
                    )
                if period_rows:
                    conn.executemany(
                        """
                        INSERT OR REPLACE INTO periods(
                          agregado_id, periodo_id, literals, modificacao, periodo_ord, periodo_kind
                        ) VALUES(?,?,?,?,?,?)
                        """,
                        period_rows,
                    )
                if locality_rows:
                    conn.executemany(
                        """
                        INSERT OR REPLACE INTO localities(
                          agregado_id, level_id, locality_id, nome
                        ) VALUES(?,?,?,?)
                        """,
                        locality_rows,
                    )

                # refresh FTS for title/survey/subject
                conn.execute("DELETE FROM table_titles_fts WHERE table_id=?", (table_id,))
                conn.execute(
                    "INSERT INTO table_titles_fts(table_id, title, survey, subject) VALUES(?,?,?,?)",
                    (
                        table_id,
                        md.get("nome") or "",
                        md.get("pesquisa") or "",
                        md.get("assunto") or "",
                    ),
                )

                # log
                conn.execute(
                    "INSERT INTO ingestion_log(agregado_id, stage, status, detail, run_at) VALUES(?,?,?,?,?)",
                    (table_id, "metadata", "success", None, fetched_at),
                )

                conn.commit()
            except Exception:
                conn.rollback()
                _log_ingestion(table_id, "error", "db:write")
                raise

        # build name→table link indexes
        if build_links:
            await asyncio.to_thread(build_links_for_table, table_id)
            # make new names visible to in-process searches
            reset_cache()  # ADD

        # optional embeddings for title text
        if get_settings().enable_title_embeddings:
            try:
                embedding_client = embedding_client or EmbeddingClient()
                text = _canonical_table_text(md)
                await _ensure_embeddings(table_id, text, embedding_client=embedding_client)
            except Exception:
                # don't break ingestion if embeddings fail
                pass

    finally:
        if own_client:
            await client.close()


def _log_ingestion(table_id: int, status: str, detail: str) -> None:
    with sqlite_session() as conn:
        with conn:
            conn.execute(
                """
                INSERT INTO ingestion_log(agregado_id, stage, status, detail, run_at)
                VALUES(?,?,?,?,?)
                """,
                (table_id, "metadata", status, detail, _now()),
            )



###############################################################################
### FILE: ingest/links.py
###############################################################################
from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, List, Tuple

from ..db.session import create_connection
from ..db.migrations import apply_search_schema
from ..search.normalize import normalize_basic


@dataclass
class LinkCounts:
    vars: int = 0
    classes: int = 0
    cats: int = 0
    var_class: int = 0


def _delete_links_for_table(conn, table_id: int) -> None:
    with conn:
        conn.execute("DELETE FROM link_var WHERE table_id = ?", (table_id,))
        conn.execute("DELETE FROM link_class WHERE table_id = ?", (table_id,))
        conn.execute("DELETE FROM link_cat WHERE table_id = ?", (table_id,))
        conn.execute("DELETE FROM link_var_class WHERE table_id = ?", (table_id,))


def _infer_var_class_pairs(metadata_json: dict) -> List[Tuple[int, int]]:
    """
    Optional hook: try to infer actual applicable {variable_id, class_id} pairs from metadata.
    For now, return empty (fallback to cross-product).
    """
    return []


def build_links_for_table(table_id: int) -> LinkCounts:
    conn = create_connection()
    try:
        apply_search_schema(conn)

        # load rows
        var_rows = conn.execute(
            "SELECT id, nome FROM variables WHERE agregado_id = ? ORDER BY id", (table_id,)
        ).fetchall()
        class_rows = conn.execute(
            "SELECT id, nome FROM classifications WHERE agregado_id = ? ORDER BY id", (table_id,)
        ).fetchall()
        cat_rows = conn.execute(
            """
            SELECT classification_id, categoria_id, nome
            FROM categories
            WHERE agregado_id = ?
            ORDER BY classification_id, categoria_id
            """,
            (table_id,),
        ).fetchall()
        if not var_rows and not class_rows:
            return LinkCounts()

        cats_by_class: Dict[int, List[Tuple[int, str]]] = {}
        for cr in cat_rows:
            cats_by_class.setdefault(int(cr["classification_id"]), []).append((int(cr["categoria_id"]), cr["nome"]))

        _delete_links_for_table(conn, table_id)

        with conn:
            # variables
            for vr in var_rows:
                v_id = int(vr["id"]); raw = str(vr["nome"] or ""); key = normalize_basic(raw)
                if not key: continue
                conn.execute("INSERT OR IGNORE INTO name_keys(kind,key,raw) VALUES(?,?,?)", ("var", key, raw))
                conn.execute(
                    "INSERT OR IGNORE INTO link_var(var_key, table_id, variable_id) VALUES(?,?,?)",
                    (key, table_id, v_id),
                )

            # classes & cats
            for cl in class_rows:
                c_id = int(cl["id"]); raw = str(cl["nome"] or ""); key = normalize_basic(raw)
                if not key: continue
                conn.execute("INSERT OR IGNORE INTO name_keys(kind,key,raw) VALUES(?,?,?)", ("class", key, raw))
                conn.execute(
                    "INSERT OR IGNORE INTO link_class(class_key, table_id, class_id) VALUES(?,?,?)",
                    (key, table_id, c_id),
                )
                for cat_id, cat_raw in cats_by_class.get(c_id, []):
                    cat_key = normalize_basic(str(cat_raw or ""))
                    if not cat_key: continue
                    conn.execute("INSERT OR IGNORE INTO name_keys(kind,key,raw) VALUES(?,?,?)", ("cat", cat_key, cat_raw))
                    conn.execute(
                        "INSERT OR IGNORE INTO link_cat(class_key, cat_key, table_id, class_id, category_id) VALUES(?,?,?,?,?)",
                        (key, cat_key, table_id, c_id, int(cat_id)),
                    )

            # var×class pairs — cross-product fallback
            inferred = set(_infer_var_class_pairs({}))  # reserved for future use
            if inferred:
                to_pairs = inferred
            else:
                to_pairs = {(int(vr["id"]), int(cl["id"])) for vr in var_rows for cl in class_rows}

            for v_id, c_id in to_pairs:
                v_key = normalize_basic(str(next((vr["nome"] for vr in var_rows if int(vr["id"])==v_id), "")))
                c_key = normalize_basic(str(next((cr["nome"] for cr in class_rows if int(cr["id"])==c_id), "")))
                if not v_key or not c_key: continue
                conn.execute(
                    "INSERT OR IGNORE INTO link_var_class(var_key, class_key, table_id, variable_id, class_id) VALUES(?,?,?,?,?)",
                    (v_key, c_key, table_id, v_id, c_id),
                )

        c_var = conn.execute("SELECT COUNT(*) FROM link_var WHERE table_id=?", (table_id,)).fetchone()[0]
        c_cls = conn.execute("SELECT COUNT(*) FROM link_class WHERE table_id=?", (table_id,)).fetchone()[0]
        c_cat = conn.execute("SELECT COUNT(*) FROM link_cat WHERE table_id=?", (table_id,)).fetchone()[0]
        c_vc  = conn.execute("SELECT COUNT(*) FROM link_var_class WHERE table_id=?", (table_id,)).fetchone()[0]
        return LinkCounts(c_var, c_cls, c_cat, c_vc)
    finally:
        conn.close()



###############################################################################
### FILE: net/api_client.py
###############################################################################
from __future__ import annotations

import asyncio
from typing import Any, Mapping, Self

import httpx
import orjson
from tenacity import retry, retry_if_exception_type, stop_after_attempt, wait_exponential

from ..config import get_settings


class SidraApiError(RuntimeError):
    pass


class SidraApiClient:
    def __init__(self, *, base_url: str | None = None, timeout: float | None = None) -> None:
        s = get_settings()
        # Explicit timeouts so slow servers can't stall forever on read/close
        t = float(timeout or s.request_timeout)
        http_timeout = httpx.Timeout(connect=t, read=t, write=t, pool=t)
        # Keep connections bounded (Windows sockets behave better this way)
        limits = httpx.Limits(max_connections=50, max_keepalive_connections=20)

        self._client = httpx.AsyncClient(
            base_url=base_url or s.sidra_base_url,
            timeout=http_timeout,
            headers={"User-Agent": s.user_agent},
            limits=limits,
        )

    async def __aenter__(self) -> Self: return self
    async def __aexit__(self, *_): await self.close()
    async def close(self) -> None: await self._client.aclose()

    @retry(
        stop=stop_after_attempt(get_settings().request_retries),
        wait=wait_exponential(multiplier=1, min=1, max=10),
        retry=retry_if_exception_type((httpx.TransportError, SidraApiError)),
        reraise=True,
    )
    async def _get_json(self, path: str, params: Mapping[str, Any] | None = None) -> Any:
        r = await self._client.get(path, params=params)
        if r.status_code in (429,) or r.status_code >= 500:
            raise SidraApiError(f"SIDRA API failed {r.status_code}: {r.text[:200]}")
        if r.status_code >= 400:
            raise RuntimeError(f"SIDRA API failed {r.status_code}: {r.text[:200]}")
        return orjson.loads(r.content)

    async def fetch_metadata(self, agregado_id: int) -> Any:
        return await self._get_json(f"/{agregado_id}/metadados")

    async def fetch_periods(self, agregado_id: int) -> Any:
        return await self._get_json(f"/{agregado_id}/periodos")

    async def fetch_localities(self, agregado_id: int, level: str) -> Any:
        return await self._get_json(f"/{agregado_id}/localidades/{level}")

    async def fetch_catalog(self, *, subject_id: int | None = None, periodicity: str | None = None,
                            levels: list[str] | None = None) -> Any:
        params: dict[str, Any] = {}
        if subject_id is not None: params["assunto"] = subject_id
        if periodicity: params["periodicidade"] = periodicity
        if levels:
            lv = [c.upper() for c in levels if c]
            if lv: params["nivel"] = "|".join(lv)
        return await self._get_json("", params=params or None)


def fetch_metadata_sync(agregado_id: int) -> Any:
    async def _r() -> Any:
        async with SidraApiClient() as c:
            return await c.fetch_metadata(agregado_id)
    return asyncio.run(_r())


__all__ = ["SidraApiClient", "SidraApiError", "fetch_metadata_sync"]



###############################################################################
### FILE: net/embedding_client.py
###############################################################################
from __future__ import annotations

from typing import Iterable, Sequence

import httpx
import orjson

from ..config import get_settings


class EmbeddingClient:
    def __init__(self, *, base_url: str | None = None, model: str | None = None, timeout: float | None = None) -> None:
        s = get_settings()
        self._base_url = base_url or s.embedding_api_url
        self._model = model or s.embedding_model
        self._timeout = timeout or s.request_timeout
        self._headers = {"Content-Type": "application/json", "User-Agent": s.user_agent}

    @property
    def model(self) -> str: return self._model

    def embed_text(self, text: str, *, model: str | None = None) -> Sequence[float]:
        payload = {"model": model or self._model, "input": text}
        r = httpx.post(self._base_url, content=orjson.dumps(payload), headers=self._headers, timeout=self._timeout)
        r.raise_for_status()
        data = r.json()
        return data["data"][0]["embedding"]

    def embed_batch(self, texts: Iterable[str], *, model: str | None = None) -> list[Sequence[float]]:
        payload = {"model": model or self._model, "input": list(texts)}
        r = httpx.post(self._base_url, content=orjson.dumps(payload), headers=self._headers, timeout=self._timeout)
        r.raise_for_status()
        data = r.json()
        return [item["embedding"] for item in data["data"]]



###############################################################################
### FILE: search/coverage.py
###############################################################################
from __future__ import annotations
from dataclasses import dataclass
from typing import Any, Iterator

@dataclass(frozen=True)
class _Tok:
    kind: str
    value: str
    pos: int

def _tokens(s: str) -> Iterator[_Tok]:
    i, n = 0, len(s)
    while i < n:
        ch = s[i]
        if ch.isspace(): i += 1; continue
        if ch in '()':
            yield _Tok('LP' if ch=='(' else 'RP', ch, i); i += 1; continue
        if i+1<n:
            two = s[i:i+2]
            if two in ('>=','<=','==','!=','&&'): yield _Tok('OP' if two!='&&' else 'AND', two, i); i += 2; continue
        if ch in ('<','>','='): yield _Tok('OP', ch, i); i += 1; continue
        if ch.isalpha() or ch=='_':
            start=i; i+=1
            while i<n and (s[i].isalnum() or s[i]=='_'): i+=1
            word = s[start:i].upper()
            if word in ('AND','OR','NOT'): yield _Tok(word, word, start)
            else: yield _Tok('ID', word, start)
            continue
        if ch.isdigit():
            start=i; i+=1
            while i<n and s[i].isdigit(): i+=1
            yield _Tok('NUM', s[start:i], start); continue
        if ch=='|':
            if i+1<n and s[i+1]=='|': yield _Tok('OR','||',i); i+=2; continue
        if ch=='!': yield _Tok('NOT','!',i); i+=1; continue
        raise SyntaxError(f"Unexpected {ch!r} at {i}")
    yield _Tok('EOF','',n)

@dataclass(frozen=True)
class _Cmp: op: str; ident: str; number: int
@dataclass(frozen=True)
class _Not: node: Any
@dataclass(frozen=True)
class _And: left: Any; right: Any
@dataclass(frozen=True)
class _Or: left: Any; right: Any

class _Parser:
    def __init__(self, text: str) -> None:
        self._it = iter(_tokens(text)); self.cur = next(self._it)
    def _eat(self, kind: str) -> _Tok:
        if self.cur.kind != kind: raise SyntaxError(f"Expected {kind}, got {self.cur.kind} at {self.cur.pos}")
        t = self.cur; self.cur = next(self._it); return t
    def parse(self) -> Any:
        n = self._expr()
        if self.cur.kind != 'EOF': raise SyntaxError(f"Unexpected {self.cur.kind} at {self.cur.pos}")
        return n
    def _expr(self) -> Any:
        n = self._and()
        while self.cur.kind == 'OR':
            self._eat('OR'); n = _Or(n, self._and())
        return n
    def _and(self) -> Any:
        n = self._unary()
        while self.cur.kind == 'AND':
            self._eat('AND'); n = _And(n, self._unary())
        return n
    def _unary(self) -> Any:
        if self.cur.kind=='NOT': self._eat('NOT'); return _Not(self._unary())
        return self._primary()
    def _primary(self) -> Any:
        if self.cur.kind=='LP':
            self._eat('LP'); n=self._expr(); self._eat('RP'); return n
        return self._cmp()
    def _cmp(self) -> Any:
        ident = self._eat('ID').value
        # Allow bare identifiers like "N3" (shorthand for "N3 >= 1")
        if self.cur.kind != 'OP':
            return _Cmp('>=', ident, 1)
        op = self._eat('OP').value
        if op == '=':
            op = '=='
        number = int(self._eat('NUM').value)
        return _Cmp(op, ident, number)

def parse_coverage_expr(text: str) -> Any: return _Parser(text).parse()

def extract_levels(node: Any) -> set[str]:
    out: set[str] = set()
    def _w(n: Any) -> None:
        if isinstance(n, _Cmp): out.add(n.ident)
        elif isinstance(n, _Not): _w(n.node)
        elif isinstance(n, (_And,_Or)): _w(n.left); _w(n.right)
    _w(node)
    return out      

def eval_coverage(node: Any, counts: dict[str,int]) -> bool:
    def _cmp(op: str, a: int, b: int) -> bool:
        return {'>=':a>=b,'>':a>b,'<=':a<=b,'<':a<b,'==':a==b,'!=':a!=b}[op]
    def _ev(n: Any) -> bool:
        if isinstance(n,_Cmp): return _cmp(n.op, int(counts.get(n.ident,0)), n.number)
        if isinstance(n,_Not): return not _ev(n.node)
        if isinstance(n,_And): return _ev(n.left) and _ev(n.right)
        if isinstance(n,_Or): return _ev(n.left) or _ev(n.right)
        raise TypeError(f"Bad node {n}")
    return _ev(node)



###############################################################################
### FILE: search/fuzzy3gram.py
###############################################################################
# src/sidra_search/search/fuzzy3gram.py
from __future__ import annotations

from typing import Dict, List, Tuple, Literal

from rapidfuzz import fuzz, process

from ..db.session import create_connection
from ..db.migrations import apply_search_schema
from .normalize import normalize_basic

# In-RAM corpus of normalized names by kind
# kind: "var" or "class"
_CORPUS: Dict[str, List[str]] = {"var": [], "class": []}
_BUILT = False


def reset_cache() -> None:
    """Clear the in-RAM corpus so new names become visible in the same process."""
    global _BUILT
    _BUILT = False
    _CORPUS["var"].clear()
    _CORPUS["class"].clear()


def _build() -> None:
    """Build the list of normalized names for variables and classifications."""
    global _BUILT
    if _BUILT:
        return
    conn = create_connection()
    try:
        apply_search_schema(conn)
        # Pull *normalized* names from DB and unique them
        var_keys = [normalize_basic(r[0]) for r in conn.execute("SELECT DISTINCT nome FROM variables")]
        class_keys = [normalize_basic(r[0]) for r in conn.execute("SELECT DISTINCT nome FROM classifications")]

        _CORPUS["var"] = sorted({k for k in var_keys if k})
        _CORPUS["class"] = sorted({k for k in class_keys if k})
        _BUILT = True
    finally:
        conn.close()


def _rf_score(a: str, b: str, **kwargs) -> float:
    """
    Blend a few RapidFuzz scorers (all 0..100) to cover:
      - substrings (partial_ratio),
      - bag-of-words overlap (token_set_ratio),
      - general fuzz (WRatio).
    """
    if not a or not b:
        return 0.0
    if a == b:
        return 100.0
    s2 = fuzz.token_set_ratio(a, b)   # multi-token overlap, order-insensitive
    s1 = fuzz.partial_ratio(a, b)     # substring-ish
    s3 = fuzz.WRatio(a, b)            # robust overall
    return 0.5 * s2 + 0.3 * s1 + 0.2 * s3


def similar_keys(
    kind: Literal["var", "class"],
    query_raw: str,
    *,
    threshold: float,
    top_k: int = 10,
) -> List[Tuple[str, float]]:
    assert kind in ("var", "class")
    _build()

    q = normalize_basic(query_raw)
    if not q:
        return []

    choices = _CORPUS[kind]
    if not choices:
        return []

    # Adaptive threshold: for short one-token queries (e.g., "pessoal"),
    # allow a bit more fuzz so it can hit "pessoas", etc.
    tok_count = len(q.split())
    qlen = len(q.replace(" ", ""))
    eff_th = float(threshold)
    if tok_count == 1 and qlen <= 8:
        eff_th = min(eff_th, 0.72)  # relax to 0.72 if default is higher
    cutoff = max(0, min(100, int(round(eff_th * 100.0))))


    # Pass 1 — strict (user cutoff)
    results = process.extract(
        q, choices, scorer=_rf_score, limit=max(10, top_k * 5), score_cutoff=cutoff
    )

    # Pass 2 — if empty, slightly relax (−12 pts ~ 0.12)
    if not results and len(q) <= 10:
        results = process.extract(
            q, choices, scorer=_rf_score, limit=max(10, top_k * 5), score_cutoff=max(0, cutoff - 12)
        )
        
    # Pass 2.5 — single-token: compare the query against individual tokens in each key.
    # This pulls in long names containing a near-match token (e.g., "pessoas" vs "pessoal").
    if not results and tok_count == 1:
        hits = []
        for key in choices:
            tokens = key.replace("-", " ").split()
            best = 0
            for t in tokens:
                s = fuzz.ratio(q, t)
                if s > best:
                    best = s
                if best >= 86:  # short-circuit when very close
                    break
            # tolerate near misses like "pessoal"↔"pessoas"
            if best >= max(68, cutoff - 15):
                hits.append((key, best))
        hits.sort(key=lambda x: x[1], reverse=True)
        results = [(k, s, None) for (k, s) in hits[: max(10, top_k * 5)]]


    # Pass 3 — if still empty, try a robust single scorer (WRatio), no cutoff
    if not results:
        results = process.extract(
            q, choices, scorer=fuzz.WRatio, limit=max(10, top_k * 5)
        )
        

    out: List[Tuple[str, float]] = [(key, float(score) / 100.0) for (key, score, _idx) in results]

    # Last resort — exact substring if still nothing
    if not out:
        rel = [(key, len(q) / max(1, len(key))) for key in choices if q in key]
        rel.sort(key=lambda x: x[1], reverse=True)
        out = rel[:top_k]

    return out[:top_k]



###############################################################################
### FILE: search/normalize.py
###############################################################################
# src/sidra_search/search/normalize.py
from __future__ import annotations

import re
import unicodedata

_WS = re.compile(r"\s+")
# Keep hyphen as a token-forming char per plan; drop other punctuation.
_PUNCT = re.compile(r"[^\w\s-]", re.UNICODE)

def normalize_basic(s: str | None) -> str:
    if not s:
        return ""
    s = unicodedata.normalize("NFKD", s)
    s = "".join(ch for ch in s if not unicodedata.combining(ch))
    s = s.lower().replace("\u00a0", " ")
    # normalize whitespace around hyphens
    s = _WS.sub(" ", s).strip()
    s = _PUNCT.sub(" ", s)
    s = _WS.sub(" ", s).strip()
    # collapse repeated hyphens/spaces like " -  - "
    s = re.sub(r"\s*-\s*", "-", s)
    s = _WS.sub(" ", s).strip()
    return s



###############################################################################
### FILE: search/tables.py
###############################################################################
from __future__ import annotations

import asyncio
import math
import re
from dataclasses import dataclass
from typing import Any, Dict, Iterable, List, Optional, Sequence, Set, Tuple

from array import array

from ..config import get_settings
from ..db.migrations import apply_search_schema
from ..db.session import create_connection
from ..net.embedding_client import EmbeddingClient
from ..search.normalize import normalize_basic
from ..search.title_rank import rrf
from .where_eval import eval_where
from .where_expr import WhereNode, iter_contains_literals
from ..search.fuzzy3gram import similar_keys


@dataclass(frozen=True)
class TableHit:
    table_id: int
    title: str
    period_start: str | None
    period_end: str | None
    n3: int
    n6: int
    why: List[str]
    score: float
    rrf_score: float
    struct_score: float


@dataclass(frozen=True)
class SearchArgs:
    q: str | None
    where: WhereNode | None
    limit: int
    allow_fuzzy: bool
    var_th: float
    class_th: float
    semantic: bool
    debug_fuzzy: bool


@dataclass
class _LiteralQuery:
    raw: str
    normalized: str
    scores: Dict[str, float]


@dataclass
class _TableContext:
    title: str
    title_norm: str
    survey: str
    survey_norm: str
    subject: str
    subject_norm: str
    vars: List[str]
    classes: List[str]
    cats: List[str]
    cat_any: Set[str]
    class_cat_map: Dict[str, Set[str]]
    var_class_map: Dict[str, Set[str]]
    coverage_counts: Dict[str, int]
    period_years: Set[int]
    period_start: str | None
    period_end: str | None
    n3: int
    n6: int

    @property
    def vars_set(self) -> Set[str]:
        return set(self.vars)

    @property
    def classes_set(self) -> Set[str]:
        return set(self.classes)

    @property
    def cats_set(self) -> Set[str]:
        return set(self.cats)


@dataclass(frozen=True)
class _CatRequirements:
    loose: Set[str]
    strict: Dict[str, Set[str]]
    strict_total: int
    has_any: bool


_YEAR_RE = re.compile(r"(\d{4})")


def _fts_query(text: str) -> str:
    toks = [t for t in normalize_basic(text).split() if t]
    return " ".join(toks)


def _positive_literals(where: WhereNode | None) -> Dict[str, List[str]]:
    out: Dict[str, List[str]] = {}
    if not where:
        return out
    for field, polarity, text in iter_contains_literals(where):
        if not polarity:
            continue
        if not text:
            continue
        out.setdefault(field.upper(), []).append(text)
    return out


def _build_literal_queries(
    field: str,
    literals: Iterable[str],
    *,
    allow_fuzzy: bool,
    threshold: float,
) -> List[_LiteralQuery]:
    seen: Set[str] = set()
    queries: List[_LiteralQuery] = []
    for lit in literals:
        norm = normalize_basic(lit)
        if not norm or norm in seen:
            continue
        seen.add(norm)
        scores: Dict[str, float] = {norm: 1.0}
        if allow_fuzzy:
            kind = "var" if field == "VAR" else "class"
            for key, score in similar_keys(kind, lit, threshold=threshold, top_k=12):
                scores[key] = max(scores.get(key, 0.0), float(score))
        queries.append(_LiteralQuery(raw=lit, normalized=norm, scores=scores))
    return queries


def _prefilter_link_exact(
    conn,
    table: str,
    column: str,
    needles: Iterable[str],
) -> Set[int]:
    values = [normalize_basic(n) for n in needles if normalize_basic(n)]
    if not values:
        return set()
    unique = sorted(set(values))
    ids: Set[int] = set()
    chunk = 500
    for idx in range(0, len(unique), chunk):
        batch = unique[idx : idx + chunk]
        placeholders = ",".join("?" for _ in batch)
        rows = conn.execute(
            f"SELECT DISTINCT table_id FROM {table} WHERE {column} IN ({placeholders})",
            batch,
        ).fetchall()
        ids.update(int(r[0]) for r in rows)
    return ids


def _prefilter_agregado_text(
    conn,
    column: str,
    needles: Iterable[str],
) -> Set[int]:
    # NOTE: still a scan, but cheap relative to per-table context loads.
    normalized = [normalize_basic(n) for n in needles if normalize_basic(n)]
    if not normalized:
        return set()
    rows = conn.execute(f"SELECT id, {column} FROM agregados").fetchall()
    ids: Set[int] = set()
    for row in rows:
        value = normalize_basic(str(row[column] or ""))
        if any(needle in value for needle in normalized):
            ids.add(int(row["id"]))
    return ids


def _prefilter_title_fts(conn, literals: Iterable[str]) -> Set[int]:
    settings = get_settings()
    if not settings.enable_titles_fts:
        return set()
    terms: List[str] = []
    for lit in literals:
        norm = normalize_basic(lit)
        if not norm:
            continue
        tokens = [t for t in norm.split() if t]
        if not tokens:
            continue
        if len(tokens) == 1:
            terms.append(tokens[0])
        else:
            terms.append(" ".join(tokens))
    if not terms:
        return set()
    query = " OR ".join(f'"{t}"' if " " in t else t for t in terms)
    rows = conn.execute(
        "SELECT DISTINCT table_id FROM table_titles_fts WHERE table_titles_fts MATCH ?",
        (query,),
    ).fetchall()
    return {int(r[0]) for r in rows}


def _extract_years(pid: Any, pord: Any) -> Set[int]:
    years: Set[int] = set()
    for value in (pid, pord):
        if value is None:
            continue
        text = str(value)
        match = _YEAR_RE.search(text)
        if match:
            year = int(match.group(1))
            if 1500 <= year <= 2100:
                years.add(year)
    return years


# ------------- NEW: bulk context loader (batched) -------------
def _bulk_load_contexts(conn, table_ids: Sequence[int], *, batch_size: int = 400) -> Dict[int, _TableContext]:
    """
    Load _TableContext for many tables in batches with a handful of queries per batch,
    instead of 6–7 queries per table. This is the core perf fix.
    """
    if not table_ids:
        return {}

    out: Dict[int, _TableContext] = {}
    norm = normalize_basic

    for start in range(0, len(table_ids), batch_size):
        batch = list(table_ids[start : start + batch_size])
        ph = ",".join("?" for _ in batch)

        # 1) headers
        hdr_rows = conn.execute(
            f"""
            SELECT id, nome, pesquisa, assunto, periodo_inicio, periodo_fim
            FROM agregados
            WHERE id IN ({ph})
            """,
            batch,
        ).fetchall()
        # seed contexts
        for r in hdr_rows:
            tid = int(r["id"])
            title = str(r["nome"] or "")
            survey = str(r["pesquisa"] or "")
            subject = str(r["assunto"] or "")
            out[tid] = _TableContext(
                title=title,
                title_norm=norm(title),
                survey=survey,
                survey_norm=norm(survey),
                subject=subject,
                subject_norm=norm(subject),
                vars=[],
                classes=[],
                cats=[],
                cat_any=set(),
                class_cat_map={},
                var_class_map={},
                coverage_counts={},
                period_years=set(),
                period_start=r["periodo_inicio"],
                period_end=r["periodo_fim"],
                n3=0,
                n6=0,
            )

        if not out:
            continue  # none of these batch ids exist

        # 2) link_var
        for r in conn.execute(
            f"SELECT table_id, var_key FROM link_var WHERE table_id IN ({ph})",
            batch,
        ):
            tid = int(r["table_id"])
            if tid not in out:
                continue
            v = norm(str(r["var_key"] or ""))
            if v:
                out[tid].vars.append(v)

        # 3) link_class
        for r in conn.execute(
            f"SELECT table_id, class_key FROM link_class WHERE table_id IN ({ph})",
            batch,
        ):
            tid = int(r["table_id"])
            if tid not in out:
                continue
            c = norm(str(r["class_key"] or ""))
            if c:
                out[tid].classes.append(c)

        # 4) link_var_class
        for r in conn.execute(
            f"SELECT table_id, var_key, class_key FROM link_var_class WHERE table_id IN ({ph})",
            batch,
        ):
            tid = int(r["table_id"])
            if tid not in out:
                continue
            v = norm(str(r["var_key"] or ""))
            c = norm(str(r["class_key"] or ""))
            if v and c:
                out[tid].var_class_map.setdefault(v, set()).add(c)

        # 5) link_cat
        for r in conn.execute(
            f"SELECT table_id, class_key, cat_key FROM link_cat WHERE table_id IN ({ph})",
            batch,
        ):
            tid = int(r["table_id"])
            if tid not in out:
                continue
            ck_raw = str(r["class_key"] or "")
            cat_raw = str(r["cat_key"] or "")
            ck = norm(ck_raw)
            cat = norm(cat_raw)
            if not cat:
                continue
            ctx = out[tid]
            ctx.cat_any.add(cat)
            ctx.cats.append(cat)
            if ck:
                ctx.cats.append(f"{ck}::{cat}")
                ctx.class_cat_map.setdefault(ck, set()).add(cat)

        # 6) coverage / levels
        for r in conn.execute(
            f"SELECT agregado_id, level_id, locality_count FROM agregados_levels WHERE agregado_id IN ({ph})",
            batch,
        ):
            tid = int(r["agregado_id"])
            if tid not in out:
                continue
            lvl = str(r["level_id"]).upper() if r["level_id"] else ""
            cnt = int(r["locality_count"] or 0)
            if lvl:
                out[tid].coverage_counts[lvl] = cnt
                if lvl == "N3":
                    out[tid].n3 = cnt
                elif lvl == "N6":
                    out[tid].n6 = cnt

        # 7) periods → years
        for r in conn.execute(
            f"SELECT agregado_id, periodo_id, periodo_ord FROM periods WHERE agregado_id IN ({ph})",
            batch,
        ):
            tid = int(r["agregado_id"])
            if tid not in out:
                continue
            out[tid].period_years |= _extract_years(r["periodo_id"], r["periodo_ord"])

        # de-dup preserve order for vars/classes/cats (like original)
        for tid, ctx in out.items():
            ctx.vars = list(dict.fromkeys(v for v in ctx.vars if v))
            ctx.classes = list(dict.fromkeys(c for c in ctx.classes if c))
            ctx.cats = list(dict.fromkeys(ctx.cats))

    return out


def _parse_cat_requirements(cat_literals: Iterable[str]) -> _CatRequirements:
    loose: Set[str] = set()
    strict: Dict[str, Set[str]] = {}
    has_any = False
    for lit in cat_literals:
        raw = lit.strip()
        if not raw:
            continue
        has_any = True
        if "::" in raw:
            class_part, cat_part = raw.split("::", 1)
            class_norm = normalize_basic(class_part)
            cat_norm = normalize_basic(cat_part)
            if class_norm and cat_norm:
                strict.setdefault(class_norm, set()).add(cat_norm)
        else:
            cat_norm = normalize_basic(raw)
            if cat_norm:
                loose.add(cat_norm)
    strict_total = sum(len(v) for v in strict.values())
    return _CatRequirements(loose=loose, strict=strict, strict_total=strict_total, has_any=has_any)


def _first_strict_pair(ctx: _TableContext, cat_req: _CatRequirements) -> Optional[str]:
    for class_key, cats in cat_req.strict.items():
        have = ctx.class_cat_map.get(class_key, set())
        for cat in cats:
            if cat in have:
                return f"{class_key}::{cat}"
    return None


def _aggregate_var_scores(queries: List[_LiteralQuery]) -> Tuple[Dict[str, float], Set[str]]:
    scores: Dict[str, float] = {}
    exact: Set[str] = set()
    for q in queries:
        if q.normalized:
            exact.add(q.normalized)
        for key, value in q.scores.items():
            scores[key] = max(scores.get(key, 0.0), float(value))
    return scores, exact


def _build_class_groups(
    queries: List[_LiteralQuery],
) -> Tuple[List[Dict[str, float]], List[str]]:
    groups: List[Dict[str, float]] = []
    exact: List[str] = []
    for q in queries:
        if not q.scores:
            continue
        groups.append({k: float(v) for k, v in q.scores.items()})
        exact.append(q.normalized)
    return groups, exact


def _structural_for_table(
    ctx: _TableContext,
    *,
    var_scores: Dict[str, float],
    var_exact: Set[str],
    class_groups: List[Dict[str, float]],
    class_exact: List[str],
    cat_req: _CatRequirements,
) -> Optional[Tuple[float, List[str]]]:
    # Enforce loose category presence
    for cat in cat_req.loose:
        if cat not in ctx.cat_any:
            return None

    # Enforce strict category presence per class
    for class_key, cats in cat_req.strict.items():
        have = ctx.class_cat_map.get(class_key, set())
        if not cats.issubset(have):
            return None

    has_vars = bool(var_scores)
    has_classes = bool(class_groups)

    strict_exact = cat_req.strict_total

    if has_vars:
        present_vars = [vk for vk in ctx.vars if vk in var_scores]
        if not present_vars:
            return None

        best_struct = -1.0
        best_why: List[str] = []

        for vk in present_vars:
            cls_picks: List[Tuple[str, float, bool]] = []
            required_classes = set(cat_req.strict.keys())
            var_allowed_total = ctx.var_class_map.get(vk, set())
            if required_classes and not required_classes.issubset(var_allowed_total):
                continue
            if has_classes:
                allowed_classes = var_allowed_total
                if not allowed_classes:
                    continue
                ok = True
                for idx, group in enumerate(class_groups):
                    if not group:
                        ok = False
                        break
                    candidates = set(group.keys()) & allowed_classes & ctx.classes_set
                    if not candidates:
                        ok = False
                        break
                    filtered = [
                        ck
                        for ck in candidates
                        if cat_req.strict.get(ck, set()).issubset(ctx.class_cat_map.get(ck, set()))
                    ]
                    if not filtered:
                        ok = False
                        break
                    best_ck = max(filtered, key=lambda ck: group.get(ck, 0.0))
                    cls_picks.append(
                        (
                            best_ck,
                            group.get(best_ck, 0.0),
                            bool(class_exact[idx]) and best_ck == class_exact[idx],
                        )
                    )
                if not ok:
                    continue

            var_score = var_scores.get(vk, 0.0)
            if has_classes and len(class_groups) > 0:
                avg_class = sum(score for (_ck, score, _ex) in cls_picks) / len(class_groups)
            else:
                avg_class = 0.0
            exact_boost = 0.0
            if vk in var_exact:
                exact_boost += 0.10
            exact_boost += 0.05 * sum(1 for (_ck, _s, ex) in cls_picks if ex)
            exact_boost += 0.03 * strict_exact
            struct = 0.55 * var_score + 0.35 * avg_class + exact_boost
            struct = min(1.0, struct)

            why: List[str] = []
            label = "=" if vk in var_exact else "≈"
            why.append(f'var{label}"{vk}"')
            for ck, _score, ex in cls_picks:
                clabel = "=" if ex else "≈"
                why.append(f'class{clabel}"{ck}"')
            if cls_picks:
                why.append("var×class")
            if cat_req.has_any:
                if cat_req.strict:
                    strict_hit = _first_strict_pair(ctx, cat_req)
                    if strict_hit:
                        why.append(f'cat="{strict_hit}"')
                why.append("cat")

            if struct > best_struct:
                best_struct = struct
                best_why = why

        if best_struct < 0:
            return None
        return best_struct, best_why

    if has_classes:
        picks: List[Tuple[str, float, bool]] = []
        for idx, group in enumerate(class_groups):
            if not group:
                return None
            candidates = set(group.keys()) & ctx.classes_set
            if not candidates:
                return None
            filtered = [
                ck
                for ck in candidates
                if cat_req.strict.get(ck, set()).issubset(ctx.class_cat_map.get(ck, set()))
            ]
            if not filtered:
                return None
            best_ck = max(filtered, key=lambda ck: group.get(ck, 0.0))
            picks.append(
                (
                    best_ck,
                    group.get(best_ck, 0.0),
                    bool(class_exact[idx]) and best_ck == class_exact[idx],
                )
            )
        avg_class = sum(score for (_ck, score, _ex) in picks) / len(class_groups)
        exact_boost = 0.05 * sum(1 for (_ck, _s, ex) in picks if ex) + 0.03 * strict_exact
        struct = 0.35 * avg_class + exact_boost
        struct = min(1.0, struct)
        why = [f'class{"=" if ex else "≈"}"{ck}"' for (ck, _s, ex) in picks]
        if cat_req.has_any:
            if cat_req.strict:
                strict_hit = _first_strict_pair(ctx, cat_req)
                if strict_hit:
                    why.append(f'cat="{strict_hit}"')
            why.append("cat")
        return struct, why

    # No var/class filters; only categories may contribute exact boost
    exact_boost = 0.03 * strict_exact
    struct = min(1.0, exact_boost)
    why: List[str] = []
    if cat_req.has_any:
        if cat_req.strict:
            strict_hit = _first_strict_pair(ctx, cat_req)
            if strict_hit:
                why.append(f'cat="{strict_hit}"')
        why.append("cat")
    return struct, why


async def search_tables(
    args: SearchArgs,
    *,
    embedding_client: EmbeddingClient | None = None,
) -> List[TableHit]:
    conn = create_connection()
    try:
        apply_search_schema(conn)

        where_ast = args.where
        positives = _positive_literals(where_ast)

        var_queries = _build_literal_queries(
            "VAR",
            positives.get("VAR", []),
            allow_fuzzy=args.allow_fuzzy,
            threshold=args.var_th,
        )
        class_queries = _build_literal_queries(
            "CLASS",
            positives.get("CLASS", []),
            allow_fuzzy=args.allow_fuzzy,
            threshold=args.class_th,
        )
        cat_literals = positives.get("CAT", [])

        var_scores, var_exact = _aggregate_var_scores(var_queries)
        class_groups, class_exact = _build_class_groups(class_queries)
        cat_req = _parse_cat_requirements(cat_literals)

        if args.debug_fuzzy:
            top_vars = [
                f"{k}:{s:.2f}"
                for q in var_queries
                for k, s in sorted(q.scores.items(), key=lambda x: x[1], reverse=True)[:5]
            ]
            print("[fuzzy] var:", ", ".join(top_vars) or "(none)")
            for idx, q in enumerate(class_queries):
                top_cls = sorted(q.scores.items(), key=lambda x: x[1], reverse=True)[:5]
                print(
                    f"[fuzzy] class[{idx}]:",
                    ", ".join(f"{k}:{s:.2f}" for k, s in top_cls) or "(none)",
                )

        candidates: Optional[Set[int]] = None
        pre_counts: Dict[str, int] = {}

        # Prefilters (positive-only; safe and already in your design)
        var_hints = {normalize_basic(x) for x in positives.get("VAR", []) if normalize_basic(x)}
        if var_hints:
            ids = _prefilter_link_exact(conn, "link_var", "var_key", var_hints)
            if ids:
                candidates = set(ids) if candidates is None else candidates & ids
            pre_counts["var"] = len(ids)

        class_hints = {normalize_basic(x) for x in positives.get("CLASS", []) if normalize_basic(x)}
        if class_hints:
            ids = _prefilter_link_exact(conn, "link_class", "class_key", class_hints)
            if ids:
                candidates = set(ids) if candidates is None else candidates & ids
            pre_counts["class"] = len(ids)

        # categories: loose and strict
        cat_hints = set(cat_req.loose)
        for cats in cat_req.strict.values():
            cat_hints.update(cats)
        if cat_hints:
            ids = _prefilter_link_exact(conn, "link_cat", "cat_key", cat_hints)
            if ids:
                candidates = set(ids) if candidates is None else candidates & ids
            pre_counts["cat"] = len(ids)

        title_hints = positives.get("TITLE", [])
        if title_hints:
            ids = _prefilter_title_fts(conn, title_hints)
            if ids:
                candidates = set(ids) if candidates is None else candidates & ids
            pre_counts["titlefts"] = len(ids)

        survey_hints = positives.get("SURVEY", [])
        if survey_hints:
            ids = _prefilter_agregado_text(conn, "pesquisa", survey_hints)
            if ids:
                candidates = set(ids) if candidates is None else candidates & ids
            pre_counts["survey"] = len(ids)

        subject_hints = positives.get("SUBJECT", [])
        if subject_hints:
            ids = _prefilter_agregado_text(conn, "assunto", subject_hints)
            if ids:
                candidates = set(ids) if candidates is None else candidates & ids
            pre_counts["subject"] = len(ids)

        # Fallback universe if hints produced nothing
        if candidates is None:
            rows = conn.execute("SELECT id FROM agregados").fetchall()
            candidates = {int(r[0]) for r in rows}

        if args.debug_fuzzy and pre_counts:
            print(
                "[pre] candidates≈{} (hints: var={}, class={}, cat={}, titlefts={}, survey={}, subject={})".format(
                    len(candidates),
                    pre_counts.get("var", 0),
                    pre_counts.get("class", 0),
                    pre_counts.get("cat", 0),
                    pre_counts.get("titlefts", 0),
                    pre_counts.get("survey", 0),
                    pre_counts.get("subject", 0),
                )
            )

        # --------- NEW: bulk-load contexts for all candidates (batched) ---------
        cand_list_all = sorted(candidates)
        ctx_by_id = _bulk_load_contexts(conn, cand_list_all, batch_size=400)

        # where-eval filter (re-uses same contexts; no per-table DB work)
        if where_ast:
            filtered: Set[int] = set()
            for tid in cand_list_all:
                ctx = ctx_by_id.get(tid)
                if not ctx:
                    continue
                if eval_where(where_ast, table_ctx=ctx.__dict__):
                    filtered.add(tid)
            candidates = filtered

        if not candidates:
            if args.semantic:
                print("semantic: no tables matched filters; skipping embeddings")
            return []

        # ---------- Title ranking (lexical + semantic) ----------
        lexical_ranks: Dict[int, int] = {}
        semantic_ranks: Dict[int, int] = {}

        title_literals = positives.get("TITLE", [])
        title_text = " ".join(dict.fromkeys(title_literals)) if title_literals else ""

        if title_text and get_settings().enable_titles_fts:
            q = _fts_query(title_text)
            if q:
                rows = conn.execute(
                    "SELECT table_id FROM table_titles_fts WHERE table_titles_fts MATCH ?",
                    (q,),
                ).fetchall()
                order: List[int] = []
                seen: Set[int] = set()
                for r in rows:
                    tid = int(r[0])
                    if tid in candidates and tid not in seen:
                        seen.add(tid)
                        order.append(tid)
                    if len(order) >= args.limit * 5:
                        break
                lexical_ranks = {tid: idx + 1 for idx, tid in enumerate(order)}

        semantic_enabled = False
        semantic_client = embedding_client if embedding_client else None
        if args.semantic:
            settings = get_settings()
            if not title_text.strip():
                print("[semantic] no title literals in query; skipping embeddings")
            elif not settings.enable_title_embeddings:
                print("semantic disabled: ENABLE_TITLE_EMBEDDINGS=0")
            else:
                model = semantic_client.model if semantic_client else settings.embedding_model
                row = conn.execute(
                    "SELECT COUNT(*) FROM embeddings WHERE entity_type='agregado' AND model=?",
                    (model,),
                ).fetchone()
                count = int(row[0]) if row else 0
                if count == 0:
                    print("semantic: no table embeddings found; run `embed-titles`")
                else:
                    semantic_enabled = True
                    if semantic_client is None:
                        semantic_client = EmbeddingClient(model=model)
        if title_text and semantic_enabled and semantic_client:
            try:
                qvec = await asyncio.to_thread(
                    lambda: semantic_client.embed_text(title_text, model=semantic_client.model)
                )
            except Exception as exc:
                print(f"semantic: embeddings request failed: {exc} (continuing without)")
                qvec = None
            if qvec:
                ordered = sorted(
                    candidates,
                    key=lambda tid: lexical_ranks.get(tid, float("inf")),
                )
                cap = max(200, args.limit * 10)
                ordered = ordered[:cap]
                if ordered:
                    placeholders = ",".join("?" for _ in ordered)
                    sql = (
                        "SELECT entity_id, dimension, vector "
                        "FROM embeddings WHERE entity_type='agregado' AND model=? AND entity_id IN ("
                        + placeholders
                        + ")"
                    )
                    cur = conn.execute(sql, (semantic_client.model, *ordered))
                    sims: List[Tuple[int, float]] = []
                    for row in cur.fetchall():
                        tid = int(row["entity_id"])
                        blob = row["vector"]
                        dim = int(row["dimension"])
                        arr = array("f")
                        arr.frombytes(blob)
                        vec = list(arr)
                        if dim and len(vec) > dim:
                            vec = vec[:dim]
                        if not vec:
                            continue
                        dot = sum(a * b for a, b in zip(qvec, vec))
                        norm_q = math.sqrt(sum(a * a for a in qvec))
                        norm_t = math.sqrt(sum(a * a for a in vec))
                        sim = 0.0 if norm_q == 0 or norm_t == 0 else dot / (norm_q * norm_t)
                        if sim > 0:
                            sims.append((tid, sim))
                    sims.sort(key=lambda x: x[1], reverse=True)
                    semantic_ranks = {tid: idx + 1 for idx, (tid, _s) in enumerate(sims[: args.limit * 5])}

        combined_ranks = dict(lexical_ranks)
        combined_ranks.update(semantic_ranks)
        rrf_scores = rrf(combined_ranks, k=60.0)

        # ---------- Assemble hits (no extra DB — reuse contexts) ----------
        hits: List[TableHit] = []
        for tid in sorted(candidates):
            ctx = ctx_by_id.get(tid)
            if not ctx:
                continue
            struct_info = _structural_for_table(
                ctx,
                var_scores=var_scores,
                var_exact=var_exact,
                class_groups=class_groups,
                class_exact=class_exact,
                cat_req=cat_req,
            )
            if struct_info is None:
                continue
            struct, why = struct_info

            rrf_score = float(rrf_scores.get(tid, 0.0))
            final = 0.75 * struct + 0.25 * rrf_score

            hits.append(
                TableHit(
                    table_id=tid,
                    title=ctx.title,
                    period_start=ctx.period_start,
                    period_end=ctx.period_end,
                    n3=ctx.n3,
                    n6=ctx.n6,
                    why=why,
                    score=final,
                    rrf_score=rrf_score,
                    struct_score=struct,
                )
            )

        hits.sort(key=lambda h: h.score, reverse=True)
        return hits[: max(1, int(args.limit))]
    finally:
        conn.close()



###############################################################################
### FILE: search/title_rank.py
###############################################################################
from __future__ import annotations
from typing import Mapping, Dict

def rrf(ranks: Mapping[int, int], k: float = 60.0) -> Dict[int, float]:
    return {k_: 1.0 / (k + r) for k_, r in ranks.items()}



###############################################################################
### FILE: search/where_eval.py
###############################################################################
from __future__ import annotations

from typing import Any, Sequence

from .normalize import normalize_basic
from .where_expr import (
    WhereNode,
    _And,
    _Cmp,
    _Contains,
    _Not,
    _Or,
    _PeriodCmp,
    _PeriodRange,
    _StrLit,
)


def _cmp(op: str, left: int, right: int) -> bool:
    return {
        ">=": left >= right,
        ">": left > right,
        "<=": left <= right,
        "<": left < right,
        "==": left == right,
        "!=": left != right,
    }[op]


def _years_overlap(years: Sequence[int], start: int, end: int) -> bool:
    if not years:
        return False
    for y in years:
        if start <= y <= end:
            return True
    return False


def _field_values(table_ctx: dict[str, Any], field: str) -> list[str]:
    field = field.upper()
    if field == "TITLE":
        return [table_ctx.get("title_norm", "")]
    if field == "SURVEY":
        return [table_ctx.get("survey_norm", "")]
    if field == "SUBJECT":
        return [table_ctx.get("subject_norm", "")]
    if field == "VAR":
        return table_ctx.get("vars", [])
    if field == "CLASS":
        return table_ctx.get("classes", [])
    if field == "CAT":
        return table_ctx.get("cats", [])
    return []


def _eval_contains_expr(node: WhereNode, haystack: str) -> bool:
    if isinstance(node, _StrLit):
        needle = normalize_basic(node.text)
        if not needle:
            return True
        return needle in haystack
    if isinstance(node, _Not):
        return not _eval_contains_expr(node.node, haystack)
    if isinstance(node, _And):
        return _eval_contains_expr(node.left, haystack) and _eval_contains_expr(node.right, haystack)
    if isinstance(node, _Or):
        return _eval_contains_expr(node.left, haystack) or _eval_contains_expr(node.right, haystack)
    raise TypeError(f"Unexpected node inside contains: {node!r}")


def _eval_contains(node: _Contains, table_ctx: dict[str, Any]) -> bool:
    values = _field_values(table_ctx, node.field)
    if not values:
        return False
    if isinstance(node.needle, _StrLit):
        needle = normalize_basic(node.needle.text)
        if not needle:
            return True
        return any(needle in v for v in values)
    for value in values:
        if _eval_contains_expr(node.needle, value):
            return True
    return False


def eval_where(node: WhereNode, *, table_ctx: dict[str, Any]) -> bool:
    def _eval(n: WhereNode) -> bool:
        if isinstance(n, _Not):
            return not _eval(n.node)
        if isinstance(n, _And):
            return _eval(n.left) and _eval(n.right)
        if isinstance(n, _Or):
            return _eval(n.left) or _eval(n.right)
        if isinstance(n, _Cmp):
            counts = table_ctx.get("coverage_counts", {})
            value = int(counts.get(n.ident, 0))
            return _cmp(n.op, value, n.number)
        if isinstance(n, _Contains):
            return _eval_contains(n, table_ctx)
        if isinstance(n, _PeriodCmp):
            years = sorted(table_ctx.get("period_years", set()))
            if not years:
                return False
            if n.op == "==":
                return n.year in years
            if n.op == ">=":
                return years[-1] >= n.year
            if n.op == ">":
                return years[-1] > n.year
            if n.op == "<=":
                return years[0] <= n.year
            if n.op == "<":
                return years[0] < n.year
            raise ValueError(f"Unknown period comparator {n.op}")
        if isinstance(n, _PeriodRange):
            years = sorted(table_ctx.get("period_years", set()))
            return _years_overlap(years, n.start, n.end)
        if isinstance(n, _StrLit):
            # Should not reach top-level string literals
            return bool(normalize_basic(n.text))
        raise TypeError(f"Unsupported node: {n!r}")

    return _eval(node)


__all__ = ["eval_where"]



###############################################################################
### FILE: search/where_expr.py
###############################################################################
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Iterator, Tuple


@dataclass(frozen=True)
class _Tok:
    kind: str
    value: str
    pos: int


def _tokens(text: str) -> Iterator[_Tok]:
    i, n = 0, len(text)
    while i < n:
        ch = text[i]
        if ch.isspace():
            i += 1
            continue
        if ch in "()":
            yield _Tok("LP" if ch == "(" else "RP", ch, i)
            i += 1
            continue
        if ch == "[":
            yield _Tok("LBRACK", ch, i)
            i += 1
            continue
        if ch == "]":
            yield _Tok("RBRACK", ch, i)
            i += 1
            continue
        if ch == "~":
            yield _Tok("TILDE", ch, i)
            i += 1
            continue
        if ch == "." and i + 1 < n and text[i + 1] == ".":
            yield _Tok("RANGE", "..", i)
            i += 2
            continue
        if ch == '"':
            start = i
            i += 1
            buf: list[str] = []
            while i < n:
                cur = text[i]
                if cur == "\\":
                    i += 1
                    if i >= n:
                        raise SyntaxError(f"Unterminated string starting at {start}")
                    esc = text[i]
                    if esc not in {'"', "\\"}:
                        buf.append(esc)
                    else:
                        buf.append(esc)
                    i += 1
                    continue
                if cur == '"':
                    i += 1
                    break
                buf.append(cur)
                i += 1
            else:
                raise SyntaxError(f"Unterminated string starting at {start}")
            yield _Tok("STR", "".join(buf), start)
            continue
        if i + 1 < n:
            two = text[i : i + 2]
            if two in (">=", "<=", "==", "!=", "&&"):
                yield _Tok("OP" if two != "&&" else "AND", two, i)
                i += 2
                continue
            if two == "||":
                yield _Tok("OR", two, i)
                i += 2
                continue
        if ch in ("<", ">", "="):
            yield _Tok("OP", ch, i)
            i += 1
            continue
        if ch.isalpha() or ch == "_":
            start = i
            i += 1
            while i < n and (text[i].isalnum() or text[i] == "_"):
                i += 1
            word = text[start:i].upper()
            if word in {"AND", "OR", "NOT", "IN"}:
                yield _Tok(word, word, start)
            else:
                yield _Tok("ID", word, start)
            continue
        if ch.isdigit():
            start = i
            i += 1
            while i < n and text[i].isdigit():
                i += 1
            yield _Tok("NUM", text[start:i], start)
            continue
        if ch == "!":
            yield _Tok("NOT", "!", i)
            i += 1
            continue
        raise SyntaxError(f"Unexpected {ch!r} at {i}")
    yield _Tok("EOF", "", n)


@dataclass(frozen=True)
class _Cmp:
    op: str
    ident: str
    number: int


@dataclass(frozen=True)
class _Not:
    node: Any


@dataclass(frozen=True)
class _And:
    left: Any
    right: Any


@dataclass(frozen=True)
class _Or:
    left: Any
    right: Any


@dataclass(frozen=True)
class _StrLit:
    text: str


@dataclass(frozen=True)
class _Contains:
    field: str
    needle: Any


@dataclass(frozen=True)
class _PeriodCmp:
    op: str
    year: int


@dataclass(frozen=True)
class _PeriodRange:
    start: int
    end: int


WhereNode = Any


class _Parser:
    def __init__(self, text: str) -> None:
        self._it = iter(_tokens(text))
        self.cur = next(self._it)

    def _eat(self, kind: str) -> _Tok:
        if self.cur.kind != kind:
            raise SyntaxError(f"Expected {kind}, got {self.cur.kind} at {self.cur.pos}")
        tok = self.cur
        self.cur = next(self._it)
        return tok

    def parse(self) -> WhereNode:
        node = self._expr()
        if self.cur.kind != "EOF":
            raise SyntaxError(f"Unexpected {self.cur.kind} at {self.cur.pos}")
        return node

    def _expr(self) -> WhereNode:
        node = self._and()
        while self.cur.kind == "OR":
            self._eat("OR")
            node = _Or(node, self._and())
        return node

    def _and(self) -> WhereNode:
        node = self._unary()
        while self.cur.kind == "AND":
            self._eat("AND")
            node = _And(node, self._unary())
        return node

    def _unary(self) -> WhereNode:
        if self.cur.kind == "NOT":
            self._eat("NOT")
            return _Not(self._unary())
        return self._primary()

    def _primary(self) -> WhereNode:
        if self.cur.kind == "LP":
            self._eat("LP")
            node = self._expr()
            self._eat("RP")
            return node
        if self.cur.kind == "ID":
            return self._field_expr()
        raise SyntaxError(f"Unexpected {self.cur.kind} at {self.cur.pos}")

    def _field_expr(self) -> WhereNode:
        ident = self._eat("ID").value
        if ident == "PERIOD":
            return self._period_expr()
        if self.cur.kind == "TILDE":
            self._eat("TILDE")
            return _Contains(ident, self._contains_rhs())
        if self.cur.kind != "OP":
            return _Cmp(">=", ident, 1)
        op = self._eat("OP").value
        if op == "=":
            op = "=="
        number = int(self._eat("NUM").value)
        return _Cmp(op, ident, number)

    def _period_expr(self) -> WhereNode:
        if self.cur.kind == "ID" and self.cur.value == "IN":
            self._eat("ID")
            self._eat("LBRACK")
            start = int(self._eat("NUM").value)
            self._eat("RANGE")
            end = int(self._eat("NUM").value)
            self._eat("RBRACK")
            if end < start:
                raise SyntaxError("period range end < start")
            return _PeriodRange(start, end)
        if self.cur.kind != "OP":
            raise SyntaxError("Expected comparator or IN after PERIOD")
        op = self._eat("OP").value
        if op == "=":
            op = "=="
        year = int(self._eat("NUM").value)
        return _PeriodCmp(op, year)

    def _contains_rhs(self) -> WhereNode:
        if self.cur.kind == "STR":
            return _StrLit(self._eat("STR").value)
        if self.cur.kind == "LP":
            self._eat("LP")
            node = self._contains_expr()
            self._eat("RP")
            return node
        raise SyntaxError("Expected string or group after ~")

    def _contains_expr(self) -> WhereNode:
        node = self._contains_and()
        while self.cur.kind == "OR":
            self._eat("OR")
            node = _Or(node, self._contains_and())
        return node

    def _contains_and(self) -> WhereNode:
        node = self._contains_unary()
        while self.cur.kind == "AND":
            self._eat("AND")
            node = _And(node, self._contains_unary())
        return node

    def _contains_unary(self) -> WhereNode:
        if self.cur.kind == "NOT":
            self._eat("NOT")
            return _Not(self._contains_unary())
        return self._contains_primary()

    def _contains_primary(self) -> WhereNode:
        if self.cur.kind == "LP":
            self._eat("LP")
            node = self._contains_expr()
            self._eat("RP")
            return node
        if self.cur.kind == "STR":
            return _StrLit(self._eat("STR").value)
        raise SyntaxError(f"Expected string literal in contains expression at {self.cur.pos}")


def parse_where_expr(text: str) -> WhereNode:
    text = text.strip()
    if not text:
        raise SyntaxError("empty query")
    return _Parser(text).parse()


def extract_fields(node: WhereNode) -> set[str]:
    fields: set[str] = set()

    def _walk(n: WhereNode) -> None:
        if isinstance(n, _Contains):
            fields.add(n.field)
            _walk(n.needle)
        elif isinstance(n, _Cmp):
            fields.add(n.ident)
        elif isinstance(n, (_PeriodCmp, _PeriodRange)):
            fields.add("PERIOD")
        elif isinstance(n, _StrLit):
            return
        elif isinstance(n, _Not):
            _walk(n.node)
        elif isinstance(n, (_And, _Or)):
            _walk(n.left)
            _walk(n.right)

    _walk(node)
    return fields


def iter_contains_literals(node: WhereNode, *, positive: bool = True) -> Iterator[Tuple[str, bool, str]]:
    def _walk(n: WhereNode, polarity: bool) -> Iterator[Tuple[str, bool, str]]:
        if isinstance(n, _Not):
            yield from _walk(n.node, not polarity)
        elif isinstance(n, _And) or isinstance(n, _Or):
            yield from _walk(n.left, polarity)
            yield from _walk(n.right, polarity)
        elif isinstance(n, _Contains):
            yield from _walk_contains(n.field, n.needle, polarity)
        elif isinstance(n, (_Cmp, _PeriodCmp, _PeriodRange, _StrLit)):
            return
        else:
            return

    def _walk_contains(field: str, needle: WhereNode, polarity: bool) -> Iterator[Tuple[str, bool, str]]:
        if isinstance(needle, _StrLit):
            yield field, polarity, needle.text
            return
        if isinstance(needle, _Not):
            yield from _walk_contains(field, needle.node, not polarity)
            return
        if isinstance(needle, (_And, _Or)):
            yield from _walk_contains(field, needle.left, polarity)
            yield from _walk_contains(field, needle.right, polarity)
            return

    yield from _walk(node, positive)


__all__ = [
    "WhereNode",
    "_Cmp",
    "_Not",
    "_And",
    "_Or",
    "_StrLit",
    "_Contains",
    "_PeriodCmp",
    "_PeriodRange",
    "parse_where_expr",
    "extract_fields",
    "iter_contains_literals",
]



###############################################################################
### FILE: util/__init__.py
###############################################################################
from __future__ import annotations

import hashlib
from datetime import datetime, timezone

ISO = "%Y-%m-%dT%H:%M:%SZ"

def utcnow_iso() -> str:
    return datetime.now(timezone.utc).strftime(ISO)

def sha256_text(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()



