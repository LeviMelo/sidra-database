Project structure for '/c/Users/Galaxy/LEVI/projects/sidra-database/src/sidra_search':
===============================================================================
  __init__.py
  cli/__init__.py
  config.py
  db/__init__.py
  db/base_schema.py
  db/migrations.py
  db/search_schema.py
  db/session.py
  ingest/__init__.py
  ingest/bulk.py
  ingest/ingest_table.py
  ingest/links.py
  net/__init__.py
  net/api_client.py
  net/embedding_client.py
  search/__init__.py
  search/coverage.py
  search/fuzzy3gram.py
  search/normalize.py
  search/tables.py
  search/title_rank.py
  util/__init__.py



###############################################################################
### FILE: search/tables.py
###############################################################################
from __future__ import annotations

import asyncio
from dataclasses import dataclass
from typing import Dict, Iterable, List, Optional, Sequence, Set, Tuple

from ..db.session import create_connection
from ..db.migrations import apply_search_schema
from ..search.normalize import normalize_basic
from ..search.coverage import parse_coverage_expr, extract_levels, eval_coverage
from ..search.fuzzy3gram import similar_keys
from ..search.title_rank import rrf
from ..net.embedding_client import EmbeddingClient
from ..config import get_settings


@dataclass(frozen=True)
class TableHit:
    table_id: int
    title: str
    period_start: str | None
    period_end: str | None
    n3: int
    n6: int
    why: List[str]
    score: float
    rrf_score: float
    struct_score: float


@dataclass(frozen=True)
class SearchArgs:
    title: str | None
    vars: Tuple[str, ...]
    classes: Tuple[str, ...]   # "Class" or "Class:Category"
    coverage: str | None
    limit: int
    allow_fuzzy: bool
    var_th: float
    class_th: float
    semantic: bool    # use embeddings for titles


def _split_class(spec: str) -> Tuple[str, Optional[str]]:
    if ":" in spec:
        a, b = spec.split(":", 1)
        return a.strip(), b.strip()
    return spec.strip(), None


def _fts_query(text: str) -> str:
    toks = [t for t in normalize_basic(text).split() if t]
    return " ".join(toks)


def _tables_for_key(conn, table: str, col: str, keys: Iterable[str]) -> Set[int]:
    if not keys:
        return set()
    tables: Set[int] = set()
    for key in keys:
        rows = conn.execute(
            f"SELECT DISTINCT table_id FROM {table} WHERE {col} = ?", (key,)
        ).fetchall()
        ids = {int(r[0]) for r in rows}
        tables = ids if not tables else tables & ids
    return tables


def _tables_for_class_cat(conn, pairs: Iterable[Tuple[str, str]]) -> Set[int]:
    if not pairs:
        return set()
    tables: Set[int] = set()
    for ck, catk in pairs:
        rows = conn.execute(
            "SELECT DISTINCT table_id FROM link_cat WHERE class_key=? AND cat_key=?",
            (ck, catk),
        ).fetchall()
        ids = {int(r[0]) for r in rows}
        tables = ids if not tables else tables & ids
    return tables


def _enforce_var_class(conn, table_ids: Set[int], var_keys: Set[str], class_keys: Set[str]) -> Set[int]:
    if not table_ids or not var_keys or not class_keys:
        return table_ids
    keep: Set[int] = set()
    for tid in table_ids:
        ok = True
        for vk in var_keys:
            for ck in class_keys:
                row = conn.execute(
                    """
                    SELECT 1 FROM link_var_class
                    WHERE table_id=? AND var_key=? AND class_key=?
                    LIMIT 1
                    """,
                    (tid, vk, ck),
                ).fetchone()
                if not row:
                    ok = False
                    break
            if not ok:
                break
        if ok:
            keep.add(tid)
    return keep


async def search_tables(
    args: SearchArgs,
    *,
    embedding_client: EmbeddingClient | None = None,
) -> List[TableHit]:
    conn = create_connection()
    try:
        apply_search_schema(conn)

        # normalize inputs
        var_keys_strict = {normalize_basic(v) for v in args.vars if normalize_basic(v)}
        class_specs = [s for s in args.classes if s and s.strip()]
        class_pairs = [_split_class(s) for s in class_specs]
        class_keys_strict = {normalize_basic(a) for (a, b) in class_pairs if normalize_basic(a)}
        class_cat_pairs = [
            (normalize_basic(a), normalize_basic(b))
            for (a, b) in class_pairs
            if b and normalize_basic(a) and normalize_basic(b)
        ]

        # initial candidates (intersection)
        candidates: Optional[Set[int]] = None

        if var_keys_strict:
            ids = _tables_for_key(conn, "link_var", "var_key", sorted(var_keys_strict))
            candidates = ids if candidates is None else candidates & ids

        if class_keys_strict:
            ids = _tables_for_key(conn, "link_class", "class_key", sorted(class_keys_strict))
            candidates = ids if candidates is None else candidates & ids

        if class_cat_pairs:
            ids = _tables_for_class_cat(conn, class_cat_pairs)
            candidates = ids if candidates is None else candidates & ids

        # co-occurrence enforcement (strict only)
        if candidates and var_keys_strict and class_keys_strict:
            candidates = _enforce_var_class(conn, candidates, var_keys_strict, class_keys_strict)

        # fuzzy expansions
        fuzzy_used_vars: Set[str] = set()
        fuzzy_used_classes: Set[str] = set()
        if args.allow_fuzzy:
            if var_keys_strict:
                expanded = set(var_keys_strict)
                for v in args.vars:
                    for key, _ in similar_keys("var", v, threshold=args.var_th, top_k=8):
                        if key not in var_keys_strict:
                            expanded.add(key)
                            fuzzy_used_vars.add(key)
                if expanded != var_keys_strict:
                    strict_ids = _tables_for_key(conn, "link_var", "var_key", sorted(var_keys_strict))
                    fuzzy_only = sorted(expanded - var_keys_strict)
                    fuzzy_ids: Set[int] = set()
                    for k in fuzzy_only:
                        rows = conn.execute("SELECT DISTINCT table_id FROM link_var WHERE var_key=?", (k,)).fetchall()
                        fuzzy_ids |= {int(r[0]) for r in rows}
                    ids = (strict_ids | fuzzy_ids) if strict_ids else fuzzy_ids
                    candidates = ids if candidates is None else candidates & ids

            if class_keys_strict:
                expanded = set(class_keys_strict)
                for c_raw, _ in class_pairs:
                    for key, _ in similar_keys("class", c_raw, threshold=args.class_th, top_k=8):
                        if key not in class_keys_strict:
                            expanded.add(key)
                            fuzzy_used_classes.add(key)
                if expanded != class_keys_strict:
                    strict_ids = _tables_for_key(conn, "link_class", "class_key", sorted(class_keys_strict))
                    fuzzy_only = sorted(expanded - class_keys_strict)
                    fuzzy_ids: Set[int] = set()
                    for k in fuzzy_only:
                        rows = conn.execute("SELECT DISTINCT table_id FROM link_class WHERE class_key=?", (k,)).fetchall()
                        fuzzy_ids |= {int(r[0]) for r in rows}
                    ids = (strict_ids | fuzzy_ids) if strict_ids else fuzzy_ids
                    candidates = ids if candidates is None else candidates & ids

                # keep strict var×class co-occurrence
                if candidates and var_keys_strict and class_keys_strict:
                    candidates = _enforce_var_class(conn, candidates, var_keys_strict, class_keys_strict)

        # no structural facets? start from all tables
        if candidates is None:
            rows = conn.execute("SELECT id FROM agregados").fetchall()
            candidates = {int(r[0]) for r in rows}

        if not candidates:
            return []

        # coverage filter
        if args.coverage:
            try:
                ast = parse_coverage_expr(args.coverage)
            except Exception:
                ast = None
            if ast:
                needed = extract_levels(ast)
                keep: Set[int] = set()
                for tid in candidates:
                    rows = conn.execute(
                        "SELECT level_id, locality_count FROM agregados_levels WHERE agregado_id=?",
                        (tid,),
                    ).fetchall()
                    counts = {str(r["level_id"]).upper(): int(r["locality_count"] or 0) for r in rows}
                    if eval_coverage(ast, counts):
                        keep.add(tid)
                candidates = keep
                if not candidates:
                    return []

        # title ranking
        lexical_ranks: Dict[int, int] = {}
        semantic_ranks: Dict[int, int] = {}

        if args.title and get_settings().enable_titles_fts:
            q = _fts_query(args.title)
            if q:
                rows = conn.execute(
                    """
                    SELECT table_id FROM table_titles_fts
                    WHERE table_titles_fts MATCH ?
                    """,
                    (q,),
                ).fetchall()
                order: List[int] = []
                seen: Set[int] = set()
                for r in rows:
                    t = int(r[0])
                    if t in candidates and t not in seen:
                        seen.add(t); order.append(t)
                    if len(order) >= args.limit * 5:
                        break
                lexical_ranks = {tid: idx + 1 for idx, tid in enumerate(order)}

        if args.title and args.semantic and get_settings().enable_title_embeddings:
            emb = embedding_client or EmbeddingClient()
            qvec = await asyncio.to_thread(emb.embed_text, args.title, emb.model)
            ordered = sorted(candidates)
            placeholders = ",".join("?" for _ in ordered)
            sql = (
                f"SELECT entity_id, dimension, vector "
                f"FROM embeddings WHERE entity_type='agregado' AND model=? AND entity_id IN ({placeholders})"
            )
            cur = conn.execute(sql, (emb.model, *ordered))

            from array import array
            import math

            def to_vec(blob, dim):
                arr = array("f"); arr.frombytes(blob); v = list(arr)
                return v[:dim] if dim and len(v) > dim else v

            def cosine(a: Sequence[float], b: Sequence[float]) -> float:
                if not a or not b or len(a) != len(b): return 0.0
                dot = sum(x*y for x,y in zip(a,b))
                na = math.sqrt(sum(x*x for x in a))
                nb = math.sqrt(sum(x*x for x in b))
                return 0.0 if na==0 or nb==0 else dot/(na*nb)

            sims: List[Tuple[int, float]] = []
            for row in cur.fetchall():
                tid = int(row["entity_id"])
                vec = to_vec(row["vector"], int(row["dimension"]))
                sim = cosine(qvec, vec)
                if sim > 0:
                    sims.append((tid, sim))
            sims.sort(key=lambda x: x[1], reverse=True)
            top = [tid for (tid, _s) in sims[: args.limit * 5]]
            semantic_ranks = {tid: idx + 1 for idx, tid in enumerate(top)}

        rrf_scores = rrf({**lexical_ranks, **semantic_ranks}, k=60.0)

        def struct_score_for(tid: int) -> Tuple[float, List[str]]:
            why: List[str] = []
            score = 0.0

            for vk in sorted(var_keys_strict):
                row = conn.execute(
                    "SELECT 1 FROM link_var WHERE table_id=? AND var_key=? LIMIT 1",
                    (tid, vk),
                ).fetchone()
                if row:
                    if vk in fuzzy_used_vars:
                        score += 0.5; why.append(f'var≈"{vk}"')
                    else:
                        score += 1.0; why.append(f'var="{vk}"')

            for ck in sorted(class_keys_strict):
                row = conn.execute(
                    "SELECT 1 FROM link_class WHERE table_id=? AND class_key=? LIMIT 1",
                    (tid, ck),
                ).fetchone()
                if row:
                    if ck in fuzzy_used_classes:
                        score += 0.5; why.append(f'class≈"{ck}"')
                    else:
                        score += 1.0; why.append(f'class="{ck}"')

            for (ck, catk) in class_cat_pairs:
                row = conn.execute(
                    "SELECT 1 FROM link_cat WHERE table_id=? AND class_key=? AND cat_key=? LIMIT 1",
                    (tid, ck, catk),
                ).fetchone()
                if row:
                    score += 0.5; why.append(f'{ck}:"{catk}"')

            if var_keys_strict and class_keys_strict:
                ok_all = True
                for vk in var_keys_strict:
                    for ck in class_keys_strict:
                        row = conn.execute(
                            "SELECT 1 FROM link_var_class WHERE table_id=? AND var_key=? AND class_key=? LIMIT 1",
                            (tid, vk, ck),
                        ).fetchone()
                        if not row:
                            ok_all = False; break
                    if not ok_all: break
                if ok_all:
                    score += 0.5; why.append("var×class")

            return score, why

        hits: List[TableHit] = []
        for tid in candidates:
            row = conn.execute(
                "SELECT id, nome, periodo_inicio, periodo_fim FROM agregados WHERE id=?",
                (tid,),
            ).fetchone()
            if not row:
                continue

            n3r = conn.execute(
                "SELECT COALESCE(locality_count,0) FROM agregados_levels WHERE agregado_id=? AND level_id='N3'",
                (tid,),
            ).fetchone()
            n6r = conn.execute(
                "SELECT COALESCE(locality_count,0) FROM agregados_levels WHERE agregado_id=? AND level_id='N6'",
                (tid,),
            ).fetchone()
            n3 = int(n3r[0]) if n3r else 0
            n6 = int(n6r[0]) if n6r else 0

            struct, why = struct_score_for(tid)
            rrf_score = float(rrf_scores.get(tid, 0.0))
            final = 0.7 * struct + 0.3 * rrf_score

            hits.append(
                TableHit(
                    table_id=int(row["id"]),
                    title=str(row["nome"] or ""),
                    period_start=row["periodo_inicio"],
                    period_end=row["periodo_fim"],
                    n3=n3,
                    n6=n6,
                    why=why,
                    score=final,
                    rrf_score=rrf_score,
                    struct_score=struct,
                )
            )

        hits.sort(key=lambda h: h.score, reverse=True)
        return hits[: max(1, int(args.limit))]
    finally:
        conn.close()



###############################################################################
### FILE: __init__.py
###############################################################################
"""sidra_search — table-centric search (titles + var/class links + fuzzy)."""

from .db.migrations import apply_search_schema, get_search_schema_version  # noqa: F401
from .db.base_schema import apply_base_schema  # noqa: F401

__all__ = [
    "apply_base_schema",
    "apply_search_schema",
    "get_search_schema_version",
]



###############################################################################
### FILE: cli/__init__.py
###############################################################################
from __future__ import annotations

import argparse
import asyncio
import json
from dataclasses import asdict

from ..db.session import ensure_full_schema, sqlite_session
from ..ingest.ingest_table import ingest_table
from ..ingest.bulk import ingest_by_coverage
from ..ingest.links import build_links_for_table
from ..net.embedding_client import EmbeddingClient
from ..search.tables import SearchArgs, search_tables


def _print_json(obj) -> None:
    print(json.dumps(obj, ensure_ascii=False, indent=2))


def _cmd_ingest(args: argparse.Namespace) -> None:
    ensure_full_schema()
    async def run():
        for tid in args.table_ids:
            try:
                await ingest_table(int(tid))
                print(f"ingested {tid}")
            except Exception as exc:
                print(f"failed {tid}: {exc}")
    asyncio.run(run())


def _cmd_ingest_coverage(args: argparse.Namespace) -> None:
    ensure_full_schema()
    report = asyncio.run(
        ingest_by_coverage(
            require_any_levels=args.any_level,
            require_all_levels=args.all_level,
            exclude_levels=args.exclude_level,
            subject_contains=args.subject_contains,
            survey_contains=args.survey_contains,
            limit=args.limit,
            concurrency=args.concurrent,
        )
    )
    _print_json(asdict(report))


def _cmd_build_links(args: argparse.Namespace) -> None:
    ensure_full_schema()
    for tid in args.table_ids:
        c = build_links_for_table(int(tid))
        print(f"{tid}: vars={c.vars} classes={c.classes} cats={c.cats} var×class={c.var_class}")


def _cmd_search_tables(args: argparse.Namespace) -> None:
    ensure_full_schema()
    sargs = SearchArgs(
        title=args.title,
        vars=tuple(args.var or ()),
        classes=tuple(args.cls or ()),
        coverage=args.coverage,
        limit=max(1, args.limit),
        allow_fuzzy=not args.no_fuzzy,
        var_th=float(args.var_th),
        class_th=float(args.class_th),
        semantic=bool(args.semantic),
    )
    emb = EmbeddingClient() if args.semantic else None
    hits = asyncio.run(search_tables(sargs, embedding_client=emb))
    if args.json:
        _print_json([
            {
                "id": h.table_id,
                "title": h.title,
                "period_start": h.period_start,
                "period_end": h.period_end,
                "n3": h.n3,
                "n6": h.n6,
                "why": h.why,
                "score": h.score,
                "rrf_score": h.rrf_score,
                "struct_score": h.struct_score,
            }
            for h in hits
        ])
        return
    if not hits:
        print("No results.")
        return
    for h in hits:
        period = ""
        if h.period_start or h.period_end:
            if h.period_start and h.period_end and h.period_start != h.period_end:
                period = f" | {h.period_start}–{h.period_end}"
            else:
                period = f" | {h.period_start or h.period_end}"
        cov = f" | N3={h.n3} N6={h.n6}" if (h.n3 or h.n6) else ""
        print(f"{h.table_id}: {h.title}{period}{cov}")
        if h.why:
            print("  matches:", " ".join(f"[{w}]" for w in h.why))
        print(f"  score={h.score:.3f} (struct={h.struct_score:.3f}, rrf={h.rrf_score:.3f})")


def build_parser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(prog="sidra-search", description="Table-centric search & ingestion")
    sub = p.add_subparsers(dest="cmd")

    # ingest single/many
    ig = sub.add_parser("ingest", help="Ingest one or more table IDs")
    ig.add_argument("table_ids", type=int, nargs="+")
    ig.set_defaults(func=_cmd_ingest)

    # ingest by simple coverage discovery
    ic = sub.add_parser("ingest-coverage", help="Discover by coverage and ingest")
    ic.add_argument("--any-level", dest="any_level", nargs="+")
    ic.add_argument("--all-level", dest="all_level", nargs="+")
    ic.add_argument("--exclude-level", dest="exclude_level", nargs="+")
    ic.add_argument("--subject-contains")
    ic.add_argument("--survey-contains")
    ic.add_argument("--limit", type=int, default=None)
    ic.add_argument("--concurrent", type=int, default=8)
    ic.set_defaults(func=_cmd_ingest_coverage)

    # build links explicitly (usually done automatically by ingest)
    bl = sub.add_parser("build-links", help="(Re)build link indexes for tables")
    bl.add_argument("table_ids", type=int, nargs="+")
    bl.set_defaults(func=_cmd_build_links)

    # search
    st = sub.add_parser("search", help="Search tables by facets and title")
    st.add_argument("--title", dest="title", help="free-text title query")
    st.add_argument("--var", dest="var", action="append", help="variable name (repeatable)")
    st.add_argument("--class", dest="cls", action="append", help='class name or "Class:Category" (repeatable)')
    st.add_argument("--coverage", help="boolean coverage expr, e.g. '(N6>=5000) AND (N3>=27)'")
    st.add_argument("--limit", type=int, default=20)
    st.add_argument("--no-fuzzy", action="store_true")
    st.add_argument("--var-th", type=float, default=0.90)
    st.add_argument("--class-th", type=float, default=0.85)
    st.add_argument("--semantic", action="store_true", help="use semantic title ranking (requires embeddings)")
    st.add_argument("--json", action="store_true")
    st.set_defaults(func=_cmd_search_tables)

    return p


def main(argv: Sequence[str] | None = None) -> None:
    parser = build_parser()
    args = parser.parse_args(list(argv) if argv is not None else None)
    if not hasattr(args, "func"):
        parser.print_help()
        return
    args.func(args)


if __name__ == "__main__":
    main()



###############################################################################
### FILE: config.py
###############################################################################
from __future__ import annotations

import os
from dataclasses import dataclass
from functools import lru_cache


@dataclass(frozen=True)
class Settings:
    # HTTP
    sidra_base_url: str = "https://servicodados.ibge.gov.br/api/v3/agregados"
    request_timeout: float = 30.0
    request_retries: int = 3
    user_agent: str = "sidra-search/0.1"

    # DB
    database_timeout: float = 60.0
    municipality_national_threshold: int = 5000

    # Embeddings (optional, for title semantics)
    embedding_api_url: str = "http://127.0.0.1:1234/v1/embeddings"
    embedding_model: str = "text-embedding-qwen3-embedding-0.6b@f16"

    # Features
    enable_titles_fts: bool = True
    enable_title_embeddings: bool = True


def _env(name: str) -> str | None:
    for key in (name, name.upper(), name.lower()):
        if key in os.environ:
            return os.environ[key]
    return None


@lru_cache(maxsize=1)
def get_settings() -> Settings:
    d = Settings().__dict__.copy()

    # strings
    for f in ("sidra_base_url", "user_agent", "embedding_api_url", "embedding_model"):
        v = _env(f"SIDRA_SEARCH_{f.upper()}")
        if v:
            d[f] = v

    # numerics/bools
    def _float(env, key):
        v = _env(env)
        if v:
            try: d[key] = float(v)
            except: pass

    def _int(env, key):
        v = _env(env)
        if v:
            try: d[key] = int(v)
            except: pass

    def _bool(env, key):
        v = _env(env)
        if v is not None:
            d[key] = v not in ("0", "false", "False", "")

    _float("SIDRA_SEARCH_REQUEST_TIMEOUT", "request_timeout")
    _float("SIDRA_SEARCH_DATABASE_TIMEOUT", "database_timeout")
    _int("SIDRA_SEARCH_REQUEST_RETRIES", "request_retries")
    _int("SIDRA_SEARCH_MUNICIPALITY_NATIONAL_THRESHOLD", "municipality_national_threshold")
    _bool("SIDRA_SEARCH_ENABLE_TITLES_FTS", "enable_titles_fts")
    _bool("SIDRA_SEARCH_ENABLE_TITLE_EMBEDDINGS", "enable_title_embeddings")

    return Settings(**d)



###############################################################################
### FILE: db/base_schema.py
###############################################################################
from __future__ import annotations

TABLE_STATEMENTS: tuple[str, ...] = (
    """
    CREATE TABLE IF NOT EXISTS agregados (
        id INTEGER PRIMARY KEY,
        nome TEXT NOT NULL,
        pesquisa TEXT,
        assunto TEXT,
        url TEXT,
        freq TEXT,
        periodo_inicio TEXT,
        periodo_fim TEXT,
        raw_json BLOB NOT NULL,
        fetched_at TEXT NOT NULL,
        municipality_locality_count INTEGER DEFAULT 0,
        covers_national_municipalities INTEGER DEFAULT 0
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS agregados_levels (
        agregado_id INTEGER NOT NULL,
        level_id TEXT NOT NULL,
        level_name TEXT,
        level_type TEXT NOT NULL,
        locality_count INTEGER DEFAULT 0,
        PRIMARY KEY (agregado_id, level_id, level_type),
        FOREIGN KEY (agregado_id) REFERENCES agregados(id)
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS variables (
        id          INTEGER NOT NULL,
        agregado_id INTEGER NOT NULL,
        nome        TEXT NOT NULL,
        unidade     TEXT,
        sumarizacao TEXT,
        text_hash   TEXT NOT NULL,
        PRIMARY KEY (agregado_id, id),
        FOREIGN KEY (agregado_id) REFERENCES agregados(id)
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS classifications (
        id INTEGER NOT NULL,
        agregado_id INTEGER NOT NULL,
        nome TEXT NOT NULL,
        sumarizacao_status INTEGER,
        sumarizacao_excecao TEXT,
        PRIMARY KEY (agregado_id, id),
        FOREIGN KEY (agregado_id) REFERENCES agregados(id)
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS categories (
        agregado_id INTEGER NOT NULL,
        classification_id INTEGER NOT NULL,
        categoria_id INTEGER NOT NULL,
        nome TEXT NOT NULL,
        unidade TEXT,
        nivel INTEGER,
        text_hash TEXT NOT NULL,
        PRIMARY KEY (agregado_id, classification_id, categoria_id),
        FOREIGN KEY (agregado_id, classification_id) REFERENCES classifications(agregado_id, id)
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS periods (
        agregado_id INTEGER NOT NULL,
        periodo_id TEXT NOT NULL,
        literals TEXT NOT NULL,
        modificacao TEXT,
        periodo_ord INTEGER,
        periodo_kind TEXT,
        PRIMARY KEY (agregado_id, periodo_id),
        FOREIGN KEY (agregado_id) REFERENCES agregados(id)
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS localities (
        agregado_id INTEGER NOT NULL,
        level_id TEXT NOT NULL,
        locality_id TEXT NOT NULL,
        nome TEXT NOT NULL,
        PRIMARY KEY (agregado_id, level_id, locality_id),
        FOREIGN KEY (agregado_id, level_id) REFERENCES agregados_levels(agregado_id, level_id)
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS ingestion_log (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        agregado_id INTEGER NOT NULL,
        stage TEXT NOT NULL,
        status TEXT NOT NULL,
        detail TEXT,
        run_at TEXT NOT NULL
    )
    """,
)

INDEX_STATEMENTS: tuple[str, ...] = (
    "CREATE INDEX IF NOT EXISTS idx_variables_agregado ON variables(agregado_id)",
    "CREATE INDEX IF NOT EXISTS idx_categories_agregado ON categories(agregado_id, classification_id)",
    "CREATE INDEX IF NOT EXISTS idx_localities_agregado ON localities(agregado_id, level_id)",
    "CREATE UNIQUE INDEX IF NOT EXISTS u_agregados_levels_pair ON agregados_levels(agregado_id, level_id)",
    "CREATE INDEX IF NOT EXISTS idx_periods_agregado_ord ON periods(agregado_id, periodo_ord)",
)

def apply_base_schema(connection) -> None:
    cur = connection.cursor()
    for stmt in TABLE_STATEMENTS:
        cur.execute(stmt)
    for stmt in INDEX_STATEMENTS:
        cur.execute(stmt)
    connection.commit()



###############################################################################
### FILE: db/migrations.py
###############################################################################
from __future__ import annotations

import sqlite3

from .search_schema import apply_search_schema as _apply_schema

SEARCH_SCHEMA_VERSION = 4
KEY = "sidra_search_schema_version"

def _ensure_meta(connection: sqlite3.Connection) -> None:
    connection.execute("""
        CREATE TABLE IF NOT EXISTS meta_kv(
          key TEXT PRIMARY KEY,
          value TEXT NOT NULL
        )
    """)

def get_search_schema_version(connection: sqlite3.Connection) -> int:
    _ensure_meta(connection)
    cur = connection.execute("SELECT value FROM meta_kv WHERE key = ?", (KEY,))
    row = cur.fetchone()
    return int(row[0]) if row else 0

def bump_search_schema_version(connection: sqlite3.Connection, to_version: int) -> None:
    _ensure_meta(connection)
    connection.execute(
        "INSERT INTO meta_kv(key,value) VALUES(?,?) "
        "ON CONFLICT(key) DO UPDATE SET value=excluded.value",
        (KEY, str(to_version)),
    )

def apply_search_schema(connection: sqlite3.Connection) -> None:
    current = get_search_schema_version(connection)
    if current >= SEARCH_SCHEMA_VERSION:
        return
    with connection:
        _apply_schema(connection)
        bump_search_schema_version(connection, SEARCH_SCHEMA_VERSION)



###############################################################################
### FILE: db/search_schema.py
###############################################################################
from __future__ import annotations

DDL: tuple[str, ...] = (
    # meta kv (shared)
    """
    CREATE TABLE IF NOT EXISTS meta_kv(
      key TEXT PRIMARY KEY,
      value TEXT NOT NULL
    )
    """,
    # link keys
    """
    CREATE TABLE IF NOT EXISTS name_keys (
      kind TEXT NOT NULL,     -- 'var' | 'class' | 'cat'
      key  TEXT NOT NULL,     -- normalized
      raw  TEXT NOT NULL,     -- original
      UNIQUE(kind, key, raw)
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS link_var (
      var_key    TEXT NOT NULL,
      table_id   INTEGER NOT NULL,
      variable_id INTEGER NOT NULL,
      UNIQUE(var_key, table_id, variable_id)
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS link_class (
      class_key TEXT NOT NULL,
      table_id  INTEGER NOT NULL,
      class_id  INTEGER NOT NULL,
      UNIQUE(class_key, table_id, class_id)
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS link_cat (
      class_key  TEXT NOT NULL,
      cat_key    TEXT NOT NULL,
      table_id   INTEGER NOT NULL,
      class_id   INTEGER NOT NULL,
      category_id INTEGER NOT NULL,
      UNIQUE(class_key, cat_key, table_id, class_id, category_id)
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS link_var_class (
      var_key   TEXT NOT NULL,
      class_key TEXT NOT NULL,
      table_id  INTEGER NOT NULL,
      variable_id INTEGER NOT NULL,
      class_id    INTEGER NOT NULL,
      UNIQUE(var_key, class_key, table_id, variable_id, class_id)
    )
    """,
    # FTS for titles
    """
    CREATE VIRTUAL TABLE IF NOT EXISTS table_titles_fts
    USING fts5(table_id UNINDEXED, title, survey, subject, tokenize='unicode61')
    """,
    "CREATE INDEX IF NOT EXISTS idx_table_titles_fts_id ON table_titles_fts(table_id)",
    # Embeddings table (generic, reused for table titles)
    """
    CREATE TABLE IF NOT EXISTS embeddings (
      entity_type TEXT NOT NULL,   -- 'agregado'
      entity_id   TEXT NOT NULL,   -- table_id as text
      agregado_id INTEGER,         -- optional convenience (same as entity_id)
      text_hash   TEXT NOT NULL,
      model       TEXT NOT NULL,
      dimension   INTEGER NOT NULL,
      vector      BLOB NOT NULL,
      created_at  TEXT NOT NULL,
      PRIMARY KEY (entity_type, entity_id, model)
    )
    """,
    "CREATE INDEX IF NOT EXISTS idx_embeddings_agregado ON embeddings(agregado_id, model)",
)

INDEXES: tuple[str, ...] = (
    "CREATE INDEX IF NOT EXISTS idx_link_var_key   ON link_var(var_key)",
    "CREATE INDEX IF NOT EXISTS idx_link_class_key ON link_class(class_key)",
    "CREATE INDEX IF NOT EXISTS idx_link_cat_keys  ON link_cat(class_key, cat_key)",
    "CREATE INDEX IF NOT EXISTS idx_link_var_class ON link_var_class(var_key, class_key)",
)

def apply_search_schema(connection) -> None:
    cur = connection.cursor()
    for stmt in DDL:
        cur.execute(stmt)
    for stmt in INDEXES:
        cur.execute(stmt)
    connection.commit()



###############################################################################
### FILE: db/session.py
###############################################################################
from __future__ import annotations

import os
import sqlite3
from contextlib import contextmanager
from pathlib import Path
from typing import Iterator

from ..config import get_settings
from .base_schema import apply_base_schema
from .migrations import apply_search_schema


def _has_base_tables(path: Path) -> bool:
    if not path.exists():
        return False
    try:
        conn = sqlite3.connect(path)
        cur = conn.execute(
            "SELECT name FROM sqlite_master WHERE type='table' AND name IN ('agregados','variables')"
        )
        names = {r[0] for r in cur.fetchall()}
        return "agregados" in names and "variables" in names
    except sqlite3.Error:
        return False
    finally:
        try: conn.close()
        except: pass


def _resolve_db_path() -> Path:
    env = os.getenv("SIDRA_DATABASE_PATH")
    if env:
        p = Path(env).expanduser().resolve()
        p.parent.mkdir(parents=True, exist_ok=True)
        return p
    # default
    p = Path("sidra.db").resolve()
    p.parent.mkdir(parents=True, exist_ok=True)
    return p


_DB_PATH: Path | None = None

def get_database_path() -> Path:
    global _DB_PATH
    if _DB_PATH is None:
        _DB_PATH = _resolve_db_path()
    return _DB_PATH


def create_connection() -> sqlite3.Connection:
    s = get_settings()
    conn = sqlite3.connect(
        get_database_path(),
        timeout=max(float(s.database_timeout), 30.0),
        check_same_thread=False,
    )
    conn.row_factory = sqlite3.Row
    conn.execute("PRAGMA foreign_keys=ON")
    conn.execute("PRAGMA journal_mode=WAL")
    conn.execute("PRAGMA synchronous=NORMAL")
    conn.execute(f"PRAGMA busy_timeout = {int(max(s.database_timeout, 60.0) * 1000)}")
    return conn


@contextmanager
def sqlite_session() -> Iterator[sqlite3.Connection]:
    conn = create_connection()
    try:
        yield conn
    finally:
        conn.close()


def ensure_full_schema() -> None:
    conn = create_connection()
    try:
        apply_base_schema(conn)
        apply_search_schema(conn)
        conn.commit()
    finally:
        conn.close()



###############################################################################
### FILE: ingest/bulk.py
###############################################################################
from __future__ import annotations

import asyncio
from dataclasses import dataclass, field
from typing import Any, Callable, Iterable, Sequence

from ..net.api_client import SidraApiClient
from ..db.session import sqlite_session, ensure_full_schema
from .ingest_table import ingest_table
from ..config import get_settings


@dataclass(frozen=True)
class CatalogEntry:
    id: int
    nome: str | None
    pesquisa: str | None
    pesquisa_id: int | None
    assunto: str | None
    assunto_id: int | None
    periodicidade: Any
    nivel_territorial: dict[str, list[str]]
    level_hints: frozenset[str] = frozenset()

    @property
    def level_codes(self) -> set[str]:
        codes: set[str] = set()
        for vals in self.nivel_territorial.values():
            for v in vals:
                codes.add(str(v).upper())
        codes.update(self.level_hints)
        return codes

def _normalize_levels(payload: Any) -> dict[str, list[str]]:
    result: dict[str, list[str]] = {}
    if isinstance(payload, dict):
        for k, vals in payload.items():
            arr = vals if isinstance(vals, list) else [vals]
            out = []
            for it in arr:
                if isinstance(it, str):
                    out.append(it.upper())
                elif isinstance(it, dict):
                    code = it.get("codigo") or it.get("nivel") or it.get("id")
                    if isinstance(code, str): out.append(code.upper())
            if out: result[str(k)] = out
    return result

async def fetch_catalog_entries(
    *, client: SidraApiClient | None = None, subject_id: int | None = None,
    periodicity: str | None = None, levels: Sequence[str] | None = None
) -> list[CatalogEntry]:
    own = False
    if client is None:
        client = SidraApiClient()
        own = True
    normalized_levels = [c.upper() for c in levels or [] if c]
    try:
        catalog = await client.fetch_catalog(subject_id=subject_id, periodicity=periodicity, levels=normalized_levels or None)
    finally:
        if own:
            await client.close()

    out: list[CatalogEntry] = []
    if not isinstance(catalog, list): return out
    for survey in catalog:
        ags = survey.get("agregados") if isinstance(survey, dict) else None
        if not isinstance(ags, list): continue
        for ag in ags:
            if not isinstance(ag, dict): continue
            entry = CatalogEntry(
                id = int(ag.get("id")),
                nome = ag.get("nome") or ag.get("tabela"),
                pesquisa = survey.get("pesquisa") or survey.get("nome"),
                pesquisa_id = survey.get("idPesquisa") or survey.get("id"),
                assunto = (survey.get("assunto") or {}).get("nome") if isinstance(survey.get("assunto"), dict) else survey.get("assunto"),
                assunto_id = (survey.get("assunto") or {}).get("id") if isinstance(survey.get("assunto"), dict) else survey.get("idAssunto"),
                periodicidade = survey.get("periodicidade"),
                nivel_territorial = _normalize_levels(ag.get("nivelTerritorial")),
                level_hints = frozenset(c.upper() for c in normalized_levels),
            )
            out.append(entry)
    return out

def filter_catalog_entries(
    entries: Sequence[CatalogEntry], *,
    require_any_levels: Iterable[str] | None = None,
    require_all_levels: Iterable[str] | None = None,
    exclude_levels: Iterable[str] | None = None,
    subject_contains: str | None = None,
    survey_contains: str | None = None,
) -> list[CatalogEntry]:
    any_levels = {c.upper() for c in require_any_levels or ()}
    all_levels = {c.upper() for c in require_all_levels or ()}
    excluded  = {c.upper() for c in exclude_levels or ()}
    subj_q = subject_contains.lower() if subject_contains else None
    surv_q = survey_contains.lower() if survey_contains else None

    out: list[CatalogEntry] = []
    for e in entries:
        codes = e.level_codes
        if any_levels and not (codes & any_levels): continue
        if all_levels and not all_levels.issubset(codes): continue
        if excluded and (codes & excluded): continue
        if subj_q and (e.assunto or "").lower().find(subj_q) == -1: continue
        if surv_q and (e.pesquisa or "").lower().find(surv_q) == -1: continue
        out.append(e)
    return out


@dataclass
class BulkReport:
    discovered_ids: list[int] = field(default_factory=list)
    scheduled_ids: list[int] = field(default_factory=list)
    skipped_existing: list[int] = field(default_factory=list)
    ingested_ids: list[int] = field(default_factory=list)
    failed: list[tuple[int, str]] = field(default_factory=list)


async def ingest_by_coverage(
    *, require_any_levels: Iterable[str] | None = None,
    require_all_levels: Iterable[str] | None = None,
    exclude_levels: Iterable[str] | None = None,
    subject_contains: str | None = None,
    survey_contains: str | None = None,
    limit: int | None = None,
    concurrency: int = 8,
) -> BulkReport:
    ensure_full_schema()
    report = BulkReport()
    async with SidraApiClient() as client:
        entries = await fetch_catalog_entries(
            client=client, levels=[*(require_any_levels or ()), *(require_all_levels or ())]
        )
        entries = filter_catalog_entries(
            entries,
            require_any_levels=require_any_levels,
            require_all_levels=require_all_levels,
            exclude_levels=exclude_levels,
            subject_contains=subject_contains,
            survey_contains=survey_contains,
        )
        if limit is not None and limit >= 0:
            entries = entries[:limit]
        report.discovered_ids = [e.id for e in entries]

        # skip already ingested (optional)
        with sqlite_session() as conn:
            existing = {int(r[0]) for r in conn.execute("SELECT id FROM agregados")}
        to_do = [e.id for e in entries if e.id not in existing]
        report.scheduled_ids = list(to_do)

        sem = asyncio.Semaphore(max(1, concurrency))
        async def worker(tid: int) -> None:
            async with sem:
                try:
                    await ingest_table(tid)
                    report.ingested_ids.append(tid)
                except Exception as exc:
                    report.failed.append((tid, str(exc)[:200]))

        await asyncio.gather(*(worker(t) for t in to_do))
    return report



###############################################################################
### FILE: ingest/ingest_table.py
###############################################################################
from __future__ import annotations

import asyncio
import hashlib
from array import array
from datetime import datetime, timezone
from typing import Any, Sequence

import orjson

from ..config import get_settings
from ..db.session import sqlite_session
from ..db.migrations import apply_search_schema
from ..db.base_schema import apply_base_schema
from ..ingest.links import build_links_for_table
from ..net.api_client import SidraApiClient
from ..net.embedding_client import EmbeddingClient
from ..search.normalize import normalize_basic

ISO = "%Y-%m-%dT%H:%M:%SZ"


def _now() -> str:
    return datetime.now(timezone.utc).strftime(ISO)


def _json(obj: Any) -> bytes:
    return orjson.dumps(obj)


def _json_text(obj: Any) -> str:
    return orjson.dumps(obj).decode("utf-8")


def _sha256_text(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()


def _period_to_ord_kind(periodo_id: Any) -> tuple[int | None, str]:
    """
    YYYY -> (YYYY00, 'Y')
    YYYYMM -> (YYYYMM, 'YM')
    YYYYMMDD -> (YYYYMMDD, 'YMD')
    otherwise -> (None, 'UNK')
    """
    s = str(periodo_id or "").strip()
    digits = "".join(ch for ch in s if ch.isdigit())
    if len(digits) == 4:
        return int(digits + "00"), "Y"
    if len(digits) == 6:
        return int(digits), "YM"
    if len(digits) == 8:
        return int(digits), "YMD"
    return None, "UNK"


def _canonical_table_text(md: dict[str, Any]) -> str:
    periodicidade = md.get("periodicidade") or {}
    freq = periodicidade.get("frequencia")
    inicio = periodicidade.get("inicio")
    fim = periodicidade.get("fim")

    if inicio and fim:
        period_line = f"Period: {inicio} - {fim}" if inicio != fim else f"Period: {inicio}"
    elif inicio or fim:
        period_line = f"Period: {inicio or fim}"
    else:
        period_line = None

    nivel = md.get("nivelTerritorial") or {}
    level_parts: list[str] = []
    if isinstance(nivel, dict):
        for level_type in sorted(nivel):
            codes = nivel.get(level_type) or []
            if codes:
                level_parts.append(f"{level_type}: {', '.join(str(c) for c in codes)}")

    lines = [
        f"Table {md.get('id')}: {str(md.get('nome') or '').strip()}".strip(),
        f"Survey: {md.get('pesquisa')}" if md.get("pesquisa") else None,
        f"Subject: {md.get('assunto')}" if md.get("assunto") else None,
        f"Frequency: {freq}" if freq else None,
        period_line,
        f"Territorial levels: {'; '.join(level_parts)}" if level_parts else None,
        f"URL: {md.get('URL')}" if md.get("URL") else None,
    ]
    return "\n".join(l for l in lines if l)


def _vec_to_blob(vec: Sequence[float]) -> bytes:
    arr = array("f", (float(x) for x in vec))
    return arr.tobytes()


async def _ensure_embeddings(table_id: int, text: str, *, embedding_client: EmbeddingClient) -> None:
    if not text.strip():
        return
    text_hash = _sha256_text(text)
    model = embedding_client.model

    def _read_existing() -> str | None:
        with sqlite_session() as conn:
            cur = conn.execute(
                "SELECT text_hash FROM embeddings "
                "WHERE entity_type='agregado' AND entity_id=? AND model=?",
                (str(table_id), model),
            ).fetchone()
            return cur["text_hash"] if cur else None

    existing_hash = await asyncio.to_thread(_read_existing)
    if existing_hash == text_hash:
        return

    # compute vector
    vector = await asyncio.to_thread(embedding_client.embed_text, text, model=model)
    dim = len(vector)

    def _write():
        with sqlite_session() as conn:
            with conn:
                conn.execute(
                    """
                    INSERT OR REPLACE INTO embeddings(
                      entity_type, entity_id, agregado_id, text_hash, model, dimension, vector, created_at
                    ) VALUES(?,?,?,?,?,?,?,?)
                    """,
                    (
                        "agregado",
                        str(table_id),
                        table_id,
                        text_hash,
                        model,
                        dim,
                        _vec_to_blob(vector),
                        _now(),
                    ),
                )

    await asyncio.to_thread(_write)


async def ingest_table(
    table_id: int,
    *,
    client: SidraApiClient | None = None,
    embedding_client: EmbeddingClient | None = None,
    build_links: bool = True,
) -> None:
    """
    Fetch metadata from SIDRA and persist into base + search schemas.
    Also refresh the table title FTS row and (optionally) a title embedding.
    """
    # ensure schema
    with sqlite_session() as conn:
        apply_base_schema(conn)
        apply_search_schema(conn)
        conn.commit()

    own_client = False
    if client is None:
        client = SidraApiClient()
        own_client = True

    try:
        # --- fetch payloads
        try:
            md = await client.fetch_metadata(table_id)
        except Exception:
            _log_ingestion(table_id, "error", "api:metadata")
            raise
        try:
            periods = await client.fetch_periods(table_id)
        except Exception:
            _log_ingestion(table_id, "error", "api:periods")
            raise

        # --- derive levels + localities (count + names)
        nivel_groups = md.get("nivelTerritorial") or {}
        if not isinstance(nivel_groups, dict):
            nivel_groups = {}

        settings = get_settings()
        municipality_count = 0
        level_rows: list[tuple[int, str, str | None, str, int]] = []
        locality_rows: list[tuple[int, str, str | None, str | None]] = []

        for level_type, codes in nivel_groups.items():
            if not codes:
                continue
            for code in codes:
                # fetch localities per code
                try:
                    payload = await client.fetch_localities(table_id, str(code))
                except Exception:
                    payload = []
                if not isinstance(payload, list):
                    try:
                        payload = list(payload)
                    except Exception:
                        payload = []

                count = len(payload)
                if str(code).upper() == "N6":
                    municipality_count = max(municipality_count, count)

                level_name = None
                if payload:
                    level_name = (payload[0].get("nivel") or {}).get("nome")

                level_rows.append(
                    (table_id, str(code), level_name, str(level_type), count)
                )
                for loc in payload:
                    locality_rows.append(
                        (table_id, str(code), loc.get("id"), loc.get("nome"))
                    )

        covers_nat = (
            1
            if (municipality_count >= max(0, int(settings.municipality_national_threshold)))
            else 0
        )

        # variables
        variables = md.get("variaveis") or []
        var_rows: list[tuple[Any, int, Any, Any, str, str]] = []
        for v in variables:
            var_rows.append(
                (
                    v.get("id"),
                    table_id,
                    v.get("nome"),
                    v.get("unidade"),
                    _json_text(v.get("sumarizacao", [])),
                    _sha256_text("||".join([str(v.get("id")), v.get("nome", ""), v.get("unidade", "")])),
                )
            )

        # classifications + categories
        class_rows: list[tuple[int, int, str | None, int, str]] = []
        cat_rows: list[tuple[int, int, int, str | None, str | None, Any, str]] = []
        for cl in md.get("classificacoes", []) or []:
            cid = cl.get("id")
            class_rows.append(
                (
                    cid,
                    table_id,
                    cl.get("nome"),
                    1 if (cl.get("sumarizacao", {}).get("status")) else 0,
                    _json_text(cl.get("sumarizacao", {}).get("excecao", [])),
                )
            )
            for cat in cl.get("categorias", []) or []:
                cat_rows.append(
                    (
                        table_id,
                        cid,
                        cat.get("id"),
                        cat.get("nome"),
                        cat.get("unidade"),
                        cat.get("nivel"),
                        _sha256_text("||".join([str(cid), str(cat.get("id")), cat.get("nome",""), cat.get("unidade","")])),
                    )
                )

        # periods
        period_rows: list[tuple[int, str, str, Any, int | None, str]] = []
        for p in periods or []:
            pid = p.get("id") if isinstance(p, dict) else p
            literals = p.get("literals", [pid]) if isinstance(p, dict) else [pid]
            modificacao = p.get("modificacao") if isinstance(p, dict) else None
            ord_val, kind = _period_to_ord_kind(pid)
            period_rows.append((table_id, str(pid), _json_text(literals), modificacao, ord_val, kind))

        # --- write to DB (replace child rows)
        fetched_at = _now()

        with sqlite_session() as conn:
            apply_base_schema(conn)
            apply_search_schema(conn)

            conn.execute("BEGIN")
            try:
                # parent: agregados header
                conn.execute(
                    """
                    INSERT OR REPLACE INTO agregados(
                      id, nome, pesquisa, assunto, url, freq, periodo_inicio, periodo_fim,
                      raw_json, fetched_at, municipality_locality_count, covers_national_municipalities
                    ) VALUES(?,?,?,?,?,?,?,?,?,?,?,?)
                    """,
                    (
                        md.get("id"),
                        md.get("nome"),
                        md.get("pesquisa"),
                        md.get("assunto"),
                        md.get("URL"),
                        (md.get("periodicidade") or {}).get("frequencia"),
                        (md.get("periodicidade") or {}).get("inicio"),
                        (md.get("periodicidade") or {}).get("fim"),
                        _json(md),
                        fetched_at,
                        municipality_count,
                        covers_nat,
                    ),
                )

                # purge children → insert fresh
                conn.execute("DELETE FROM localities WHERE agregado_id=?", (table_id,))
                conn.execute("DELETE FROM agregados_levels WHERE agregado_id=?", (table_id,))
                conn.execute("DELETE FROM categories WHERE agregado_id=?", (table_id,))
                conn.execute("DELETE FROM classifications WHERE agregado_id=?", (table_id,))
                conn.execute("DELETE FROM variables WHERE agregado_id=?", (table_id,))
                conn.execute("DELETE FROM periods WHERE agregado_id=?", (table_id,))

                if level_rows:
                    conn.executemany(
                        """
                        INSERT OR REPLACE INTO agregados_levels(
                          agregado_id, level_id, level_name, level_type, locality_count
                        ) VALUES(?,?,?,?,?)
                        """,
                        level_rows,
                    )
                if var_rows:
                    conn.executemany(
                        """
                        INSERT OR REPLACE INTO variables(
                          id, agregado_id, nome, unidade, sumarizacao, text_hash
                        ) VALUES(?,?,?,?,?,?)
                        """,
                        var_rows,
                    )
                if class_rows:
                    conn.executemany(
                        """
                        INSERT OR REPLACE INTO classifications(
                          id, agregado_id, nome, sumarizacao_status, sumarizacao_excecao
                        ) VALUES(?,?,?,?,?)
                        """,
                        class_rows,
                    )
                if cat_rows:
                    conn.executemany(
                        """
                        INSERT OR REPLACE INTO categories(
                          agregado_id, classification_id, categoria_id, nome, unidade, nivel, text_hash
                        ) VALUES(?,?,?,?,?,?,?)
                        """,
                        cat_rows,
                    )
                if period_rows:
                    conn.executemany(
                        """
                        INSERT OR REPLACE INTO periods(
                          agregado_id, periodo_id, literals, modificacao, periodo_ord, periodo_kind
                        ) VALUES(?,?,?,?,?,?)
                        """,
                        period_rows,
                    )
                if locality_rows:
                    conn.executemany(
                        """
                        INSERT OR REPLACE INTO localities(
                          agregado_id, level_id, locality_id, nome
                        ) VALUES(?,?,?,?)
                        """,
                        locality_rows,
                    )

                # refresh FTS for title/survey/subject
                conn.execute("DELETE FROM table_titles_fts WHERE table_id=?", (table_id,))
                conn.execute(
                    "INSERT INTO table_titles_fts(table_id, title, survey, subject) VALUES(?,?,?,?)",
                    (
                        table_id,
                        md.get("nome") or "",
                        md.get("pesquisa") or "",
                        md.get("assunto") or "",
                    ),
                )

                # log
                conn.execute(
                    "INSERT INTO ingestion_log(agregado_id, stage, status, detail, run_at) VALUES(?,?,?,?,?)",
                    (table_id, "metadata", "success", None, fetched_at),
                )

                conn.commit()
            except Exception:
                conn.rollback()
                _log_ingestion(table_id, "error", "db:write")
                raise

        # build name→table link indexes
        if build_links:
            await asyncio.to_thread(build_links_for_table, table_id)

        # optional embeddings for title text
        if get_settings().enable_title_embeddings:
            embedding_client = embedding_client or EmbeddingClient()
            text = _canonical_table_text(md)
            await _ensure_embeddings(table_id, text, embedding_client=embedding_client)

    finally:
        if own_client:
            await client.close()


def _log_ingestion(table_id: int, status: str, detail: str) -> None:
    with sqlite_session() as conn:
        with conn:
            conn.execute(
                """
                INSERT INTO ingestion_log(agregado_id, stage, status, detail, run_at)
                VALUES(?,?,?,?,?)
                """,
                (table_id, "metadata", status, detail, _now()),
            )



###############################################################################
### FILE: ingest/links.py
###############################################################################
from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, List, Tuple

from ..db.session import create_connection
from ..db.migrations import apply_search_schema
from ..search.normalize import normalize_basic


@dataclass
class LinkCounts:
    vars: int = 0
    classes: int = 0
    cats: int = 0
    var_class: int = 0


def _delete_links_for_table(conn, table_id: int) -> None:
    with conn:
        conn.execute("DELETE FROM link_var WHERE table_id = ?", (table_id,))
        conn.execute("DELETE FROM link_class WHERE table_id = ?", (table_id,))
        conn.execute("DELETE FROM link_cat WHERE table_id = ?", (table_id,))
        conn.execute("DELETE FROM link_var_class WHERE table_id = ?", (table_id,))


def _infer_var_class_pairs(metadata_json: dict) -> List[Tuple[int, int]]:
    """
    Optional hook: try to infer actual applicable {variable_id, class_id} pairs from metadata.
    For now, return empty (fallback to cross-product).
    """
    return []


def build_links_for_table(table_id: int) -> LinkCounts:
    conn = create_connection()
    try:
        apply_search_schema(conn)

        # load rows
        var_rows = conn.execute(
            "SELECT id, nome FROM variables WHERE agregado_id = ? ORDER BY id", (table_id,)
        ).fetchall()
        class_rows = conn.execute(
            "SELECT id, nome FROM classifications WHERE agregado_id = ? ORDER BY id", (table_id,)
        ).fetchall()
        cat_rows = conn.execute(
            """
            SELECT classification_id, categoria_id, nome
            FROM categories
            WHERE agregado_id = ?
            ORDER BY classification_id, categoria_id
            """,
            (table_id,),
        ).fetchall()
        if not var_rows and not class_rows:
            return LinkCounts()

        cats_by_class: Dict[int, List[Tuple[int, str]]] = {}
        for cr in cat_rows:
            cats_by_class.setdefault(int(cr["classification_id"]), []).append((int(cr["categoria_id"]), cr["nome"]))

        _delete_links_for_table(conn, table_id)

        with conn:
            # variables
            for vr in var_rows:
                v_id = int(vr["id"]); raw = str(vr["nome"] or ""); key = normalize_basic(raw)
                if not key: continue
                conn.execute("INSERT OR IGNORE INTO name_keys(kind,key,raw) VALUES(?,?,?)", ("var", key, raw))
                conn.execute(
                    "INSERT OR IGNORE INTO link_var(var_key, table_id, variable_id) VALUES(?,?,?)",
                    (key, table_id, v_id),
                )

            # classes & cats
            for cl in class_rows:
                c_id = int(cl["id"]); raw = str(cl["nome"] or ""); key = normalize_basic(raw)
                if not key: continue
                conn.execute("INSERT OR IGNORE INTO name_keys(kind,key,raw) VALUES(?,?,?)", ("class", key, raw))
                conn.execute(
                    "INSERT OR IGNORE INTO link_class(class_key, table_id, class_id) VALUES(?,?,?)",
                    (key, table_id, c_id),
                )
                for cat_id, cat_raw in cats_by_class.get(c_id, []):
                    cat_key = normalize_basic(str(cat_raw or ""))
                    if not cat_key: continue
                    conn.execute("INSERT OR IGNORE INTO name_keys(kind,key,raw) VALUES(?,?,?)", ("cat", cat_key, cat_raw))
                    conn.execute(
                        "INSERT OR IGNORE INTO link_cat(class_key, cat_key, table_id, class_id, category_id) VALUES(?,?,?,?,?)",
                        (key, cat_key, table_id, c_id, int(cat_id)),
                    )

            # var×class pairs — cross-product fallback
            inferred = set(_infer_var_class_pairs({}))  # reserved for future use
            if inferred:
                to_pairs = inferred
            else:
                to_pairs = {(int(vr["id"]), int(cl["id"])) for vr in var_rows for cl in class_rows}

            for v_id, c_id in to_pairs:
                v_key = normalize_basic(str(next((vr["nome"] for vr in var_rows if int(vr["id"])==v_id), "")))
                c_key = normalize_basic(str(next((cr["nome"] for cr in class_rows if int(cr["id"])==c_id), "")))
                if not v_key or not c_key: continue
                conn.execute(
                    "INSERT OR IGNORE INTO link_var_class(var_key, class_key, table_id, variable_id, class_id) VALUES(?,?,?,?,?)",
                    (v_key, c_key, table_id, v_id, c_id),
                )

        c_var = conn.execute("SELECT COUNT(*) FROM link_var WHERE table_id=?", (table_id,)).fetchone()[0]
        c_cls = conn.execute("SELECT COUNT(*) FROM link_class WHERE table_id=?", (table_id,)).fetchone()[0]
        c_cat = conn.execute("SELECT COUNT(*) FROM link_cat WHERE table_id=?", (table_id,)).fetchone()[0]
        c_vc  = conn.execute("SELECT COUNT(*) FROM link_var_class WHERE table_id=?", (table_id,)).fetchone()[0]
        return LinkCounts(c_var, c_cls, c_cat, c_vc)
    finally:
        conn.close()



###############################################################################
### FILE: net/api_client.py
###############################################################################
from __future__ import annotations

import asyncio
from typing import Any, Mapping, Self

import httpx
import orjson
from tenacity import retry, retry_if_exception_type, stop_after_attempt, wait_exponential

from ..config import get_settings


class SidraApiError(RuntimeError):
    pass


class SidraApiClient:
    def __init__(self, *, base_url: str | None = None, timeout: float | None = None) -> None:
        s = get_settings()
        self._client = httpx.AsyncClient(
            base_url=base_url or s.sidra_base_url,
            timeout=timeout or s.request_timeout,
            headers={"User-Agent": s.user_agent},
        )

    async def __aenter__(self) -> Self: return self
    async def __aexit__(self, *_): await self.close()
    async def close(self) -> None: await self._client.aclose()

    @retry(
        stop=stop_after_attempt(get_settings().request_retries),
        wait=wait_exponential(multiplier=1, min=1, max=10),
        retry=retry_if_exception_type((httpx.TransportError, SidraApiError)),
        reraise=True,
    )
    async def _get_json(self, path: str, params: Mapping[str, Any] | None = None) -> Any:
        r = await self._client.get(path, params=params)
        if r.status_code in (429,) or r.status_code >= 500:
            raise SidraApiError(f"SIDRA API failed {r.status_code}: {r.text[:200]}")
        if r.status_code >= 400:
            raise RuntimeError(f"SIDRA API failed {r.status_code}: {r.text[:200]}")
        return orjson.loads(r.content)

    async def fetch_metadata(self, agregado_id: int) -> Any:
        return await self._get_json(f"/{agregado_id}/metadados")

    async def fetch_periods(self, agregado_id: int) -> Any:
        return await self._get_json(f"/{agregado_id}/periodos")

    async def fetch_localities(self, agregado_id: int, level: str) -> Any:
        return await self._get_json(f"/{agregado_id}/localidades/{level}")

    async def fetch_catalog(self, *, subject_id: int | None = None, periodicity: str | None = None,
                            levels: list[str] | None = None) -> Any:
        params: dict[str, Any] = {}
        if subject_id is not None: params["assunto"] = subject_id
        if periodicity: params["periodicidade"] = periodicity
        if levels:
            lv = [c.upper() for c in levels if c]
            if lv: params["nivel"] = "|".join(lv)
        return await self._get_json("", params=params or None)


def fetch_metadata_sync(agregado_id: int) -> Any:
    async def _r() -> Any:
        async with SidraApiClient() as c:
            return await c.fetch_metadata(agregado_id)
    return asyncio.run(_r())


__all__ = ["SidraApiClient", "SidraApiError", "fetch_metadata_sync"]



###############################################################################
### FILE: net/embedding_client.py
###############################################################################
from __future__ import annotations

from typing import Iterable, Sequence

import httpx
import orjson

from ..config import get_settings


class EmbeddingClient:
    def __init__(self, *, base_url: str | None = None, model: str | None = None, timeout: float | None = None) -> None:
        s = get_settings()
        self._base_url = base_url or s.embedding_api_url
        self._model = model or s.embedding_model
        self._timeout = timeout or s.request_timeout
        self._headers = {"Content-Type": "application/json", "User-Agent": s.user_agent}

    @property
    def model(self) -> str: return self._model

    def embed_text(self, text: str, *, model: str | None = None) -> Sequence[float]:
        payload = {"model": model or self._model, "input": text}
        r = httpx.post(self._base_url, content=orjson.dumps(payload), headers=self._headers, timeout=self._timeout)
        r.raise_for_status()
        data = r.json()
        return data["data"][0]["embedding"]

    def embed_batch(self, texts: Iterable[str], *, model: str | None = None) -> list[Sequence[float]]:
        payload = {"model": model or self._model, "input": list(texts)}
        r = httpx.post(self._base_url, content=orjson.dumps(payload), headers=self._headers, timeout=self._timeout)
        r.raise_for_status()
        data = r.json()
        return [item["embedding"] for item in data["data"]]



###############################################################################
### FILE: search/coverage.py
###############################################################################
from __future__ import annotations
from dataclasses import dataclass
from typing import Any, Iterator

@dataclass(frozen=True)
class _Tok:
    kind: str
    value: str
    pos: int

def _tokens(s: str) -> Iterator[_Tok]:
    i, n = 0, len(s)
    while i < n:
        ch = s[i]
        if ch.isspace(): i += 1; continue
        if ch in '()':
            yield _Tok('LP' if ch=='(' else 'RP', ch, i); i += 1; continue
        if i+1<n:
            two = s[i:i+2]
            if two in ('>=','<=','==','!=','&&'): yield _Tok('OP' if two!='&&' else 'AND', two, i); i += 2; continue
        if ch in ('<','>','='): yield _Tok('OP', ch, i); i += 1; continue
        if ch.isalpha() or ch=='_':
            start=i; i+=1
            while i<n and (s[i].isalnum() or s[i]=='_'): i+=1
            word = s[start:i].upper()
            if word in ('AND','OR','NOT'): yield _Tok(word, word, start)
            else: yield _Tok('ID', word, start)
            continue
        if ch.isdigit():
            start=i; i+=1
            while i<n and s[i].isdigit(): i+=1
            yield _Tok('NUM', s[start:i], start); continue
        if ch=='|':
            if i+1<n and s[i+1]=='|': yield _Tok('OR','||',i); i+=2; continue
        if ch=='!': yield _Tok('NOT','!',i); i+=1; continue
        raise SyntaxError(f"Unexpected {ch!r} at {i}")
    yield _Tok('EOF','',n)

@dataclass(frozen=True)
class _Cmp: op: str; ident: str; number: int
@dataclass(frozen=True)
class _Not: node: Any
@dataclass(frozen=True)
class _And: left: Any; right: Any
@dataclass(frozen=True)
class _Or: left: Any; right: Any

class _Parser:
    def __init__(self, text: str) -> None:
        self._it = iter(_tokens(text)); self.cur = next(self._it)
    def _eat(self, kind: str) -> _Tok:
        if self.cur.kind != kind: raise SyntaxError(f"Expected {kind}, got {self.cur.kind} at {self.cur.pos}")
        t = self.cur; self.cur = next(self._it); return t
    def parse(self) -> Any:
        n = self._expr()
        if self.cur.kind != 'EOF': raise SyntaxError(f"Unexpected {self.cur.kind} at {self.cur.pos}")
        return n
    def _expr(self) -> Any:
        n = self._and()
        while self.cur.kind == 'OR':
            self._eat('OR'); n = _Or(n, self._and())
        return n
    def _and(self) -> Any:
        n = self._unary()
        while self.cur.kind == 'AND':
            self._eat('AND'); n = _And(n, self._unary())
        return n
    def _unary(self) -> Any:
        if self.cur.kind=='NOT': self._eat('NOT'); return _Not(self._unary())
        return self._primary()
    def _primary(self) -> Any:
        if self.cur.kind=='LP':
            self._eat('LP'); n=self._expr(); self._eat('RP'); return n
        return self._cmp()
    def _cmp(self) -> Any:
        ident = self._eat('ID').value
        op = self._eat('OP').value
        if op == '=': op = '=='
        number = int(self._eat('NUM').value)
        return _Cmp(op, ident, number)

def parse_coverage_expr(text: str) -> Any: return _Parser(text).parse()

def extract_levels(node: Any) -> set[str]:
    out: set[str] = set()
    def _w(n: Any) -> None:
        if isinstance(n, _Cmp): out.add(n.ident)
        elif isinstance(n, _Not): _w(n.node)
        elif isinstance(n, (_And,_Or)): _w(n.left); _w(n.right)
    _w(node)
    return out

def eval_coverage(node: Any, counts: dict[str,int]) -> bool:
    def _cmp(op: str, a: int, b: int) -> bool:
        return {'>=':a>=b,'>':a>b,'<=':a<=b,'<':a<b,'==':a==b,'!=':a!=b}[op]
    def _ev(n: Any) -> bool:
        if isinstance(n,_Cmp): return _cmp(n.op, int(counts.get(n.ident,0)), n.number)
        if isinstance(n,_Not): return not _ev(n.node)
        if isinstance(n,_And): return _ev(n.left) and _ev(n.right)
        if isinstance(n,_Or): return _ev(n.left) or _ev(n.right)
        raise TypeError(f"Bad node {n}")
    return _ev(node)



###############################################################################
### FILE: search/fuzzy3gram.py
###############################################################################
from __future__ import annotations

from collections import Counter
from math import sqrt, log
from typing import Dict, List, Tuple

from ..db.session import create_connection
from ..db.migrations import apply_search_schema
from .normalize import normalize_basic

_CORPUS: Dict[str, Dict[str, Counter]] = {"var": {}, "class": {}}
_IDF: Dict[str, Dict[str, float]] = {"var": {}, "class": {}}
_BUILT = False

def reset_cache() -> None:
    global _BUILT
    _BUILT = False
    _CORPUS["var"].clear()
    _CORPUS["class"].clear()
    _IDF["var"].clear()
    _IDF["class"].clear()

def _trigrams(s: str) -> Counter:
    s = f"  {s}  "
    return Counter(s[i:i+3] for i in range(len(s)-2))

def _build() -> None:
    global _BUILT
    if _BUILT: return
    conn = create_connection()
    try:
        apply_search_schema(conn)
        var_keys = [normalize_basic(r[0]) for r in conn.execute("SELECT DISTINCT nome FROM variables")]
        class_keys = [normalize_basic(r[0]) for r in conn.execute("SELECT DISTINCT nome FROM classifications")]
        for kind, keys in (("var", var_keys), ("class", class_keys)):
            keys = [k for k in keys if k]
            docs: Dict[str, Counter] = {}
            df: Counter = Counter()
            for key in keys:
                grams = _trigrams(key)
                docs[key] = grams
                for g in grams.keys():
                    df[g] += 1
            N = max(1, len(keys))
            _CORPUS[kind] = docs
            _IDF[kind] = {g: log((N + 1) / (dfg + 1)) + 1.0 for g, dfg in df.items()}
        _BUILT = True
    finally:
        conn.close()

def _vec(kind: str, text: str) -> Dict[str, float]:
    grams = _trigrams(text)
    idf = _IDF[kind]
    return {g: tf * idf.get(g, 1.0) for g, tf in grams.items()}

def _cos(a: Dict[str, float], b: Dict[str, float]) -> float:
    if not a or not b: return 0.0
    dot = sum(va * b.get(k, 0.0) for k, va in a.items())
    na = sqrt(sum(v*v for v in a.values()))
    nb = sqrt(sum(v*v for v in b.values()))
    if na == 0.0 or nb == 0.0: return 0.0
    return dot / (na * nb)

def similar_keys(kind: str, query_raw: str, *, threshold: float, top_k: int = 10) -> List[Tuple[str, float]]:
    assert kind in ("var", "class")
    _build()
    q = normalize_basic(query_raw)
    if not q: return []
    qv = _vec(kind, q)
    out: List[Tuple[str, float]] = []
    for key, grams in _CORPUS[kind].items():
        sv = {g: tf * _IDF[kind].get(g, 1.0) for g, tf in grams.items()}
        s = _cos(qv, sv)
        if s >= threshold:
            out.append((key, s))
    out.sort(key=lambda x: x[1], reverse=True)
    return out[:top_k]



###############################################################################
### FILE: search/normalize.py
###############################################################################
from __future__ import annotations

import re
import unicodedata

_WS = re.compile(r"\s+")
_PUNCT = re.compile(r"[^\w\s]", re.UNICODE)  # keep letters/digits/underscore

def normalize_basic(s: str | None) -> str:
    if not s: return ""
    s = unicodedata.normalize("NFKD", s)
    s = "".join(ch for ch in s if not unicodedata.combining(ch))
    s = s.lower()
    s = s.replace("\u00A0", " ")
    s = _WS.sub(" ", s).strip()
    s = _PUNCT.sub(" ", s)
    s = _WS.sub(" ", s).strip()
    return s



###############################################################################
### FILE: search/title_rank.py
###############################################################################
from __future__ import annotations
from typing import Mapping, Dict

def rrf(ranks: Mapping[int, int], k: float = 60.0) -> Dict[int, float]:
    return {k_: 1.0 / (k + r) for k_, r in ranks.items()}



###############################################################################
### FILE: util/__init__.py
###############################################################################
from __future__ import annotations

import hashlib
from datetime import datetime, timezone

ISO = "%Y-%m-%dT%H:%M:%SZ"

def utcnow_iso() -> str:
    return datetime.now(timezone.utc).strftime(ISO)

def sha256_text(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()



