Project structure for '/c/Users/Galaxy/LEVI/projects/sidra-database/src/sidra_database':
===============================================================================
  __init__.py
  api_client.py
  bulk_ingest.py
  catalog.py
  cli.py
  config.py
  db.py
  diagnostics.py
  discovery.py
  embedding.py
  ingest.py
  schema.py
  search.py



###############################################################################
### FILE: __init__.py
###############################################################################
"""sidra_database package exports."""
from .config import Settings, get_settings
from .api_client import SidraApiClient, SidraApiError
from .ingest import ingest_agregado, ingest_agregado_sync
from .db import ensure_schema, sqlite_session, create_connection, get_database_path
from .embedding import EmbeddingClient
from .catalog import AgregadoRecord, list_agregados
from .bulk_ingest import (
    BulkIngestionReport,
    discover_agregados_by_coverage,
    ingest_by_coverage,
)
from .discovery import CatalogEntry
from .search import (
    ChildMatch,
    SemanticMatch,
    SemanticResult,
    SearchFilters,
    semantic_search,
    semantic_search_with_metadata,
    hybrid_search,
)

__all__ = [
    "Settings",
    "get_settings",
    "SidraApiClient",
    "SidraApiError",
    "ingest_agregado",
    "ingest_agregado_sync",
    "ensure_schema",
    "sqlite_session",
    "create_connection",
    "get_database_path",
    "EmbeddingClient",
    "AgregadoRecord",
    "list_agregados",
    "BulkIngestionReport",
    "CatalogEntry",
    "discover_agregados_by_coverage",
    "ingest_by_coverage",
    "ChildMatch",
    "SemanticMatch",
    "SemanticResult",
    "SearchFilters",
    "semantic_search",
    "semantic_search_with_metadata",
    "hybrid_search",
]






###############################################################################
### FILE: api_client.py
###############################################################################
"""HTTP client for the SIDRA aggregated-data API."""
from __future__ import annotations

import asyncio
from collections.abc import Mapping
from typing import Any, Self

import httpx
import orjson
from tenacity import retry, retry_if_exception_type, stop_after_attempt, wait_exponential

from .config import get_settings


class SidraApiError(RuntimeError):
    """Raised when the SIDRA API returns a non-successful response."""


class SidraApiClient:
    """Asynchronous client wrapping SIDRA aggregated-data API endpoints."""

    def __init__(self, *, base_url: str | None = None, timeout: float | None = None) -> None:
        settings = get_settings()
        self._client = httpx.AsyncClient(
            base_url=base_url or settings.sidra_base_url,
            timeout=timeout or settings.request_timeout,
            headers={"User-Agent": settings.user_agent},
        )
        self._settings = settings

    async def __aenter__(self) -> Self:
        return self

    async def __aexit__(self, exc_type, exc, tb) -> None:  # noqa: ANN001
        await self.close()

    async def close(self) -> None:
        await self._client.aclose()

    @retry(
        stop=stop_after_attempt(get_settings().request_retries),
        wait=wait_exponential(multiplier=1, min=1, max=10),
        retry=retry_if_exception_type((httpx.TransportError, SidraApiError)),
        reraise=True,
    )
    async def _get_json(self, path: str, params: Mapping[str, Any] | None = None) -> Any:
        response = await self._client.get(path, params=params)
        if response.status_code >= 400:
            raise SidraApiError(
                f"SIDRA API request failed ({response.status_code}): {response.text[:200]}"
            )
        return orjson.loads(response.content)

    async def fetch_metadata(self, agregado_id: int) -> Any:
        return await self._get_json(f"/{agregado_id}/metadados")

    async def fetch_periods(self, agregado_id: int) -> Any:
        return await self._get_json(f"/{agregado_id}/periodos")

    async def fetch_localities(self, agregado_id: int, level: str) -> Any:
        return await self._get_json(f"/{agregado_id}/localidades/{level}")

    async def fetch_catalog(
        self,
        *,
        subject_id: int | None = None,
        periodicity: str | None = None,
        levels: list[str] | None = None,
    ) -> Any:
        """Return the agregados catalog grouped by survey/subject."""

        params: dict[str, Any] = {}
        if subject_id is not None:
            params["assunto"] = subject_id
        if periodicity:
            params["periodicidade"] = periodicity
        if levels:
            normalized = [code.upper() for code in levels if code]
            if normalized:
                params["nivel"] = "|".join(normalized)
        return await self._get_json("", params=params or None)

    async def fetch_values(
        self,
        agregado_id: int,
        period: str,
        variable: int,
        *,
        localidades: str | None = None,
        classificacao: str | None = None,
        view: str | None = None,
    ) -> Any:
        params: dict[str, Any] = {}
        if localidades:
            params["localidades"] = localidades
        if classificacao:
            params["classificacao"] = classificacao
        if view:
            params["view"] = view
        return await self._get_json(f"/{agregado_id}/periodos/{period}/variaveis/{variable}", params=params)

    async def fetch_latest(self, agregado_id: int, variable: int, **params: Any) -> Any:
        return await self._get_json(f"/{agregado_id}/variaveis/{variable}", params=params)


def fetch_metadata_sync(agregado_id: int) -> Any:
    """Convenience synchronous helper for quick scripts."""

    async def _runner() -> Any:
        async with SidraApiClient() as client:
            return await client.fetch_metadata(agregado_id)

    return asyncio.run(_runner())


__all__ = ["SidraApiClient", "SidraApiError", "fetch_metadata_sync"]



###############################################################################
### FILE: bulk_ingest.py
###############################################################################
"""Bulk ingestion helpers that discover and ingest SIDRA agregados."""
from __future__ import annotations

import asyncio
import time
from dataclasses import dataclass, field
from collections.abc import Iterable, Sequence
from typing import Any, Callable

from .api_client import SidraApiClient
from .db import ensure_schema, sqlite_session
from .discovery import CatalogEntry, fetch_catalog_entries, filter_catalog_entries
from .embedding import EmbeddingClient
from .ingest import ingest_agregado, generate_embeddings_for_agregado


@dataclass(slots=True)
class BulkIngestionReport:
    """Outcome of a bulk ingestion run."""

    discovered_ids: list[int] = field(default_factory=list)
    scheduled_ids: list[int] = field(default_factory=list)
    skipped_existing: list[int] = field(default_factory=list)
    ingested_ids: list[int] = field(default_factory=list)
    failed: list[tuple[int, str]] = field(default_factory=list)

    def as_dict(self) -> dict[str, Any]:
        """Return a serializable summary of the ingestion run."""

        return {
            "discovered": self.discovered_ids,
            "scheduled": self.scheduled_ids,
            "skipped_existing": self.skipped_existing,
            "ingested": self.ingested_ids,
            "failed": self.failed,
        }


async def discover_agregados_by_coverage(
    *,
    client: SidraApiClient | None = None,
    require_any_levels: Iterable[str] | None = None,
    require_all_levels: Iterable[str] | None = None,
    exclude_levels: Iterable[str] | None = None,
    subject_contains: str | None = None,
    survey_contains: str | None = None,
    limit: int | None = None,
) -> list[CatalogEntry]:
    """Return catalog entries matching the requested territorial filters."""

    level_filter: list[str] | None = None
    if require_any_levels or require_all_levels:
        combined = [
            *(code.upper() for code in (require_any_levels or []) if code),
            *(code.upper() for code in (require_all_levels or []) if code),
        ]
        if combined:
            level_filter = list(dict.fromkeys(combined))

    entries = await fetch_catalog_entries(
        client=client,
        levels=level_filter,
    )
    filtered = filter_catalog_entries(
        entries,
        require_any_levels=require_any_levels,
        require_all_levels=require_all_levels,
        exclude_levels=exclude_levels,
        subject_contains=subject_contains,
        survey_contains=survey_contains,
    )
    if limit is not None and limit >= 0:
        return filtered[:limit]
    return filtered


async def ingest_by_coverage(
    *,
    require_any_levels: Iterable[str] | None = None,
    require_all_levels: Iterable[str] | None = None,
    exclude_levels: Iterable[str] | None = None,
    subject_contains: str | None = None,
    survey_contains: str | None = None,
    limit: int | None = None,
    concurrency: int = 8,
    skip_existing: bool = True,
    dry_run: bool = False,
    client: SidraApiClient | None = None,
    embedding_client: EmbeddingClient | None = None,
    progress_callback: Callable[[str], None] | None = None,
    generate_embeddings: bool = True,
) -> BulkIngestionReport:
    """Discover agregados using coverage filters and ingest them."""

    if concurrency < 1:
        raise ValueError("concurrency must be at least 1")

    ensure_schema()

    def _emit(message: str) -> None:
        if progress_callback is not None:
            progress_callback(message)

    own_client = False
    if client is None:
        client = SidraApiClient()
        own_client = True

    report = BulkIngestionReport()

    try:
        candidates = await discover_agregados_by_coverage(
            client=client,
            require_any_levels=require_any_levels,
            require_all_levels=require_all_levels,
            exclude_levels=exclude_levels,
            subject_contains=subject_contains,
            survey_contains=survey_contains,
            limit=limit,
        )
        report.discovered_ids = [entry.id for entry in candidates]

        existing_ids: set[int] = set()
        if skip_existing:
            with sqlite_session() as conn:
                rows = conn.execute("SELECT id FROM agregados")
                existing_ids = {int(row["id"]) for row in rows}

        to_schedule: list[int] = []
        for entry in candidates:
            if entry.id in existing_ids:
                report.skipped_existing.append(entry.id)
                continue
            to_schedule.append(entry.id)
        report.scheduled_ids = list(to_schedule)

        total_to_schedule = len(to_schedule)
        if total_to_schedule:
            _emit(
                f"Scheduling {total_to_schedule} agregados for ingestion "
                f"with concurrency={concurrency}"
            )
        else:
            _emit("No new agregados matched the requested filters.")

        if dry_run or not to_schedule:
            return report

        semaphore = asyncio.Semaphore(concurrency)
        db_lock = asyncio.Lock() if concurrency > 1 else None

        progress_step = max(1, total_to_schedule // 100) if total_to_schedule > 0 else 1
        progress_time_budget = 15.0
        last_progress_at = time.monotonic()
        completed = 0

        async def _run(agregado_id: int) -> None:
            nonlocal completed
            nonlocal last_progress_at
            async with semaphore:
                try:
                    await ingest_agregado(
                        agregado_id,
                        client=client,
                        embedding_client=embedding_client,
                        generate_embeddings=False,
                        db_lock=db_lock,
                    )
                except Exception as exc:  # noqa: BLE001
                    report.failed.append((agregado_id, str(exc)))
                    _emit(f"Failed agregado {agregado_id}: {exc}")
                else:
                    report.ingested_ids.append(agregado_id)
                finally:
                    completed += 1
                    now = time.monotonic()
                    should_emit = (
                        completed <= 10
                        or completed == total_to_schedule
                        or completed % progress_step == 0
                        or (now - last_progress_at) >= progress_time_budget
                    )
                    if should_emit:
                        last_progress_at = now
                        ingested = len(report.ingested_ids)
                        failed = len(report.failed)
                        _emit(
                            f"Progress {completed}/{total_to_schedule}: "
                            f"ingested={ingested}, failed={failed}"
                        )

        await asyncio.gather(*(_run(agregado_id) for agregado_id in to_schedule))

        if generate_embeddings and report.ingested_ids:
            embed_client = embedding_client or EmbeddingClient()
            embed_lock = asyncio.Lock() if concurrency > 1 else None
            embed_semaphore = asyncio.Semaphore(concurrency)
            embed_total = len(report.ingested_ids)
            embed_step = max(1, embed_total // 100) if embed_total > 0 else 1
            embed_last_progress = time.monotonic()
            completed_embeds = 0

            async def _embed(agregado_id: int) -> None:
                nonlocal completed_embeds
                nonlocal embed_last_progress
                async with embed_semaphore:
                    try:
                        await generate_embeddings_for_agregado(
                            agregado_id,
                            embedding_client=embed_client,
                            db_lock=embed_lock,
                        )
                    except Exception as exc:  # noqa: BLE001
                        report.failed.append((agregado_id, f"embedding: {exc}"))
                        _emit(f"Embedding failed for {agregado_id}: {exc}")
                    finally:
                        completed_embeds += 1
                        now = time.monotonic()
                        if (
                            completed_embeds <= 10
                            or completed_embeds == embed_total
                            or completed_embeds % embed_step == 0
                            or (now - embed_last_progress) >= progress_time_budget
                        ):
                            embed_last_progress = now
                            _emit(
                                f"Embedding progress {completed_embeds}/{embed_total}"
                            )

            _emit(
                f"Generating embeddings for {len(report.ingested_ids)} agregados"
            )
            await asyncio.gather(
                *(_embed(agregado_id) for agregado_id in report.ingested_ids)
            )
        return report
    finally:
        if own_client:
            await client.close()


__all__ = ["BulkIngestionReport", "discover_agregados_by_coverage", "ingest_by_coverage"]



###############################################################################
### FILE: catalog.py
###############################################################################
"""Helpers to enumerate stored agregados and coverage metadata."""
from __future__ import annotations

from dataclasses import dataclass

from .db import sqlite_session


@dataclass(frozen=True)
class AgregadoRecord:
    """Lightweight snapshot of an agregados entry."""

    id: int
    nome: str
    assunto: str | None
    pesquisa: str | None
    municipality_locality_count: int
    covers_national_municipalities: bool
    fetched_at: str


def list_agregados(
    *,
    requires_national_munis: bool = False,
    min_municipality_count: int | None = None,
    limit: int | None = None,
    order_by: str = "municipalities",
) -> list[AgregadoRecord]:
    """Return agregados rows matching basic coverage filters."""

    valid_order = {
        "municipalities": "municipality_locality_count DESC, id ASC",
        "id": "id ASC",
        "name": "nome COLLATE NOCASE ASC",
        "fetched": "fetched_at DESC",
    }
    if order_by not in valid_order:
        raise ValueError(f"Unsupported order_by value: {order_by}")

    conditions: list[str] = []
    params: list[object] = []
    if requires_national_munis:
        conditions.append("covers_national_municipalities = 1")
    if min_municipality_count is not None:
        conditions.append("municipality_locality_count >= ?")
        params.append(int(min_municipality_count))

    where_clause = f"WHERE {' AND '.join(conditions)}" if conditions else ""
    limit_clause = f"LIMIT {int(limit)}" if limit is not None and limit > 0 else ""

    sql = (
        "SELECT id, nome, assunto, pesquisa, municipality_locality_count, "
        "covers_national_municipalities, fetched_at FROM agregados "
        f"{where_clause} ORDER BY {valid_order[order_by]} {limit_clause}"
    ).strip()

    rows: list[AgregadoRecord] = []
    with sqlite_session() as conn:
        for row in conn.execute(sql, params):
            rows.append(
                AgregadoRecord(
                    id=row["id"],
                    nome=row["nome"],
                    assunto=row["assunto"],
                    pesquisa=row["pesquisa"],
                    municipality_locality_count=int(row["municipality_locality_count"] or 0),
                    covers_national_municipalities=bool(row["covers_national_municipalities"]),
                    fetched_at=row["fetched_at"],
                )
            )
    return rows


__all__ = ["AgregadoRecord", "list_agregados"]



###############################################################################
### FILE: cli.py
###############################################################################
"""Command-line interface for sidra-database."""
from __future__ import annotations

import argparse
import asyncio
import json
import time
from dataclasses import asdict
from typing import Mapping, Sequence

from .config import get_settings
from .embedding import EmbeddingClient
from .ingest import ingest_agregado, generate_embeddings_for_agregado
from .bulk_ingest import ingest_by_coverage
from .search import hybrid_search, SearchFilters
from .catalog import list_agregados
from .db import sqlite_session, ensure_schema
from .diagnostics import (
    api_vs_db_spot_check,
    global_health_report,
    repair_missing_variables,
)


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="SIDRA metadata management")
    subparsers = parser.add_subparsers(dest="command", required=True)

    ingest_parser = subparsers.add_parser("ingest", help="Ingest one or more agregados")
    ingest_parser.add_argument(
        "agregado_ids",
        metavar="AGREGADO",
        type=int,
        nargs="+",
        help="One or more agregados IDs to ingest",
    )
    ingest_parser.add_argument(
        "--concurrent",
        type=int,
        default=4,
        help="Number of concurrent ingestion tasks (default: 4)",
    )
    ingest_parser.add_argument(
        "--skip-embeddings",
        action="store_true",
        help="Skip embedding generation during ingestion",
    )

    embed_parser = subparsers.add_parser(
        "embed",
        help="Regenerate embeddings for stored agregados",
    )
    embed_parser.add_argument(
        "agregado_ids",
        metavar="AGREGADO",
        type=int,
        nargs="*",
        help="Optional list of agregados IDs to embed (default: all stored)",
    )
    embed_parser.add_argument(
        "--concurrent",
        type=int,
        default=6,
        help="Number of concurrent embedding tasks (default: 6)",
    )
    embed_parser.add_argument(
        "--model",
        type=str,
        default=None,
        help="Override embedding model identifier for LM Studio",
    )

    bulk_parser = subparsers.add_parser(
        "ingest-coverage",
        help="Discover agregados by territorial coverage and ingest them",
    )
    bulk_parser.add_argument(
        "--any-level",
        dest="any_levels",
        metavar="LEVEL",
        nargs="+",
        default=None,
        help="Require at least one of these territorial level codes (default: N3 N6)",
    )
    bulk_parser.add_argument(
        "--all-level",
        dest="all_levels",
        metavar="LEVEL",
        nargs="+",
        default=None,
        help="Require all of these territorial level codes",
    )
    bulk_parser.add_argument(
        "--exclude-level",
        dest="exclude_levels",
        metavar="LEVEL",
        nargs="+",
        default=None,
        help="Exclude agregados containing any of these level codes",
    )
    bulk_parser.add_argument(
        "--subject-contains",
        dest="subject_contains",
        default=None,
        help="Case-insensitive substring filter for the subject description",
    )
    bulk_parser.add_argument(
        "--survey-contains",
        dest="survey_contains",
        default=None,
        help="Case-insensitive substring filter for the survey description",
    )
    bulk_parser.add_argument(
        "--limit",
        type=int,
        default=None,
        help="Limit the number of discovered agregados to ingest",
    )
    bulk_parser.add_argument(
        "--concurrent",
        type=int,
        default=8,
        help="Number of concurrent ingestion tasks (default: 8)",
    )
    bulk_parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Only discover matching agregados without ingesting them",
    )
    bulk_parser.add_argument(
        "--no-skip-existing",
        dest="skip_existing",
        action="store_false",
        help="Reingest agregados already present in the database",
    )
    bulk_parser.set_defaults(skip_existing=True)
    bulk_parser.add_argument(
        "--skip-embeddings",
        action="store_true",
        help="Skip embedding generation during ingestion",
    )

    diag_parser = subparsers.add_parser(
        "diagnostics",
        help="Run health checks against the SIDRA metadata store",
    )
    diag_sub = diag_parser.add_subparsers(dest="diagnostics_command", required=True)

    diag_health = diag_sub.add_parser("health", help="Print global ingestion health stats")
    diag_health.add_argument(
        "--sample",
        type=int,
        default=50,
        help="Number of missing-agregado IDs to include in the sample (default: 50)",
    )

    diag_spot = diag_sub.add_parser(
        "spot-check",
        help="Compare API metadata for agregados currently missing variables",
    )
    diag_spot.add_argument(
        "--sample",
        type=int,
        default=10,
        help="Number of agregados to sample for live API verification (default: 10)",
    )

    repair_parser = subparsers.add_parser(
        "repair-missing",
        help="Re-ingest agregados that currently have zero variables recorded",
    )
    repair_parser.add_argument(
        "--chunk",
        type=int,
        default=50,
        help="Number of agregados to process per batch (default: 50)",
    )
    repair_parser.add_argument(
        "--concurrent",
        type=int,
        default=6,
        help="Maximum concurrent ingestion tasks per batch (default: 6)",
    )
    repair_parser.add_argument(
        "--limit",
        type=int,
        default=None,
        help="Limit the number of agregados to re-ingest",
    )
    repair_parser.add_argument(
        "--retries",
        type=int,
        default=3,
        help="Number of retry attempts per batch (default: 3)",
    )

    search_parser = subparsers.add_parser("search", help="Run semantic search over stored embeddings")
    search_parser.add_argument("query", help="Search query text")
    search_parser.add_argument(
        "--types",
        nargs="*",
        choices=["agregado", "variable", "classification", "category"],
        help="Child entity types that may contribute lexical evidence",
    )
    search_parser.add_argument(
        "--limit",
        type=int,
        default=5,
        help="Maximum number of results to display (default: 5)",
    )
    search_parser.add_argument(
        "--model",
        type=str,
        default=None,
        help="Override embedding model identifier",
    )
    search_parser.add_argument(
        "--expand",
        type=int,
        default=6,
        help="Maximum number of child matches to show per table (default: 6)",
    )
    search_parser.add_argument(
        "--flat",
        action="store_true",
        help="Print child matches as a flat list instead of per table",
    )
    search_parser.add_argument(
        "--json",
        action="store_true",
        help="Return results as JSON",
    )
    search_parser.add_argument(
        "--weights",
        type=str,
        default=None,
        help="Override scoring weights, e.g. sem=0.6,lex_table=0.2,lex_children=0.2",
    )
    search_parser.add_argument(
        "--min-municipalities",
        type=int,
        default=None,
        help="Only return agregados covering at least this many municipalities",
    )
    search_parser.add_argument(
        "--requires-national-munis",
        action="store_true",
        help="Only return agregados flagged with national municipal coverage",
    )
    search_parser.add_argument(
        "--subject-contains",
        type=str,
        default=None,
        help="Filter agregados whose subject contains this text",
    )
    search_parser.add_argument(
        "--survey-contains",
        type=str,
        default=None,
        help="Filter agregados whose survey contains this text",
    )
    search_parser.add_argument(
        "--period-start",
        type=int,
        default=None,
        help="Require tables with period end >= this year",
    )
    search_parser.add_argument(
        "--period-end",
        type=int,
        default=None,
        help="Require tables with period start <= this year",
    )
    search_parser.add_argument(
        "--has-variable",
        dest="has_variables",
        type=int,
        action="append",
        default=None,
        help="Only return tables containing this variable ID (repeatable)",
    )
    search_parser.add_argument(
        "--has-classification",
        dest="has_classifications",
        type=int,
        action="append",
        default=None,
        help="Only return tables containing this classification ID (repeatable)",
    )
    search_parser.add_argument(
        "--has-category",
        dest="has_categories",
        type=str,
        action="append",
        default=None,
        help="Only return tables containing classification:category pairs",
    )

    list_parser = subparsers.add_parser("list", help="List stored agregados with optional coverage filters")
    list_parser.add_argument(
        "--requires-national-munis",
        action="store_true",
        help="Only show agregados flagged with national municipal coverage",
    )
    list_parser.add_argument(
        "--min-municipalities",
        type=int,
        default=None,
        help="Only show agregados covering at least this many municipalities",
    )
    list_parser.add_argument(
        "--limit",
        type=int,
        default=20,
        help="Maximum number of rows to display (default: 20)",
    )
    list_parser.add_argument(
        "--order-by",
        choices=["municipalities", "id", "name", "fetched"],
        default="municipalities",
        help="Sort order for results (default: municipalities)",
    )

    return parser


async def _run_ids(
    agregado_ids: Sequence[int],
    concurrency: int,
    *,
    generate_embeddings: bool,
) -> None:
    semaphore = asyncio.Semaphore(concurrency)

    async def worker(agregado_id: int) -> None:
        async with semaphore:
            await ingest_agregado(agregado_id, generate_embeddings=generate_embeddings)

    await asyncio.gather(*(worker(agregado_id) for agregado_id in agregado_ids))


async def _embed_ids(
    agregado_ids: Sequence[int],
    concurrency: int,
    *,
    model: str | None = None,
) -> None:
    if concurrency < 1:
        raise ValueError("concurrency must be at least 1")

    semaphore = asyncio.Semaphore(concurrency)
    progress_lock = asyncio.Lock()
    db_lock = asyncio.Lock()

    embed_client = EmbeddingClient(model=model) if model else EmbeddingClient()

    total = len(agregado_ids)
    completed = 0
    failed: list[tuple[int, str]] = []
    progress_step = max(1, total // 100) if total else 1
    last_emit = time.monotonic()
    emit_interval = 15.0

    async def worker(agregado_id: int) -> None:
        nonlocal completed, last_emit
        async with semaphore:
            try:
                await generate_embeddings_for_agregado(
                    agregado_id,
                    embedding_client=embed_client,
                    db_lock=db_lock,
                )
            except Exception as exc:  # noqa: BLE001
                async with progress_lock:
                    failed.append((agregado_id, str(exc)))
            finally:
                async with progress_lock:
                    completed += 1
                    now = time.monotonic()
                    should_emit = (
                        completed <= 10
                        or completed == total
                        or completed % progress_step == 0
                        or (now - last_emit) >= emit_interval
                    )
                    if should_emit:
                        last_emit = now
                        print(
                            f"Embedding progress {completed}/{total}: failed={len(failed)}",
                            flush=True,
                        )

    await asyncio.gather(*(worker(agregado_id) for agregado_id in agregado_ids))

    if failed:
        print("Embedding failures:")
        for agregado_id, message in failed[:10]:
            print(f"   {agregado_id}: {message[:180]}")
        if len(failed) > 10:
            print("   ...")


def _parse_weight_overrides(spec: str | None) -> Mapping[str, float] | None:
    if spec is None:
        return None
    overrides: dict[str, float] = {}
    for part in spec.split(","):
        chunk = part.strip()
        if not chunk:
            continue
        if "=" not in chunk:
            raise ValueError(
                f"Invalid weight override '{chunk}'. Expected format key=value, e.g. sem=0.6."
            )
        key, value = chunk.split("=", 1)
        key = key.strip()
        try:
            overrides[key] = float(value.strip())
        except ValueError as exc:
            raise ValueError(
                f"Invalid numeric weight for '{key}': {value.strip()}"
            ) from exc
    return overrides


def _parse_category_filters(raw: Sequence[str] | None) -> list[tuple[int, int]]:
    if not raw:
        return []
    parsed: list[tuple[int, int]] = []
    for value in raw:
        chunk = value.strip()
        if not chunk:
            continue
        if ":" not in chunk:
            raise ValueError(
                f"Invalid category filter '{value}'. Expected CLASSIFICATION:CATEGORY format."
            )
        left, right = chunk.split(":", 1)
        try:
            parsed.append((int(left), int(right)))
        except ValueError as exc:
            raise ValueError(
                f"Invalid category filter '{value}'. Both parts must be integers."
            ) from exc
    return parsed


def main(argv: Sequence[str] | None = None) -> None:
    parser = build_parser()
    args = parser.parse_args(argv)
    settings = get_settings()
    print(f"Using database at {settings.database_path}")

    if args.command == "ingest":
        if args.skip_embeddings:
            print("Embedding generation skipped; metadata only.")
        asyncio.run(
            _run_ids(
                args.agregado_ids,
                args.concurrent,
                generate_embeddings=not args.skip_embeddings,
            )
        )
        return

    if args.command == "ingest-coverage":
        any_levels = args.any_levels or ["N3", "N6"]

        if args.skip_embeddings:
            print("Embedding generation skipped; metadata only.")

        def _progress(message: str) -> None:
            print(message, flush=True)

        report = asyncio.run(
            ingest_by_coverage(
                require_any_levels=any_levels,
                require_all_levels=args.all_levels,
                exclude_levels=args.exclude_levels,
                subject_contains=args.subject_contains,
                survey_contains=args.survey_contains,
                limit=args.limit,
                concurrency=args.concurrent,
                skip_existing=args.skip_existing,
                dry_run=args.dry_run,
                generate_embeddings=not args.skip_embeddings,
                progress_callback=_progress,
            )
        )
        discovered = len(report.discovered_ids)
        print(f"Discovered {discovered} agregados matching the provided filters.")
        if report.discovered_ids:
            preview = ", ".join(str(tid) for tid in report.discovered_ids[:10])
            suffix = " ..." if discovered > 10 else ""
            print(f"   IDs: {preview}{suffix}")
        if report.skipped_existing:
            print(
                f"Skipped {len(report.skipped_existing)} already ingested agregados"
            )
        if args.dry_run:
            scheduled = len(report.scheduled_ids)
            print(f"Dry run complete: {scheduled} agregados would be ingested.")
            return
        ingested = len(report.ingested_ids)
        print(f"Ingested {ingested} agregados.")
        if report.failed:
            print(f"Failed to ingest {len(report.failed)} agregados:")
            for agregado_id, message in report.failed[:10]:
                print(f"   {agregado_id}: {message[:180]}")
            if len(report.failed) > 10:
                print("   ...")
        return

    if args.command == "diagnostics":
        ensure_schema()
        with sqlite_session() as conn:
            if args.diagnostics_command == "health":
                report = global_health_report(conn, sample_limit=args.sample)
            elif args.diagnostics_command == "spot-check":
                report = asyncio.run(
                    api_vs_db_spot_check(conn, sample_size=args.sample)
                )
            else:
                report = {}
        print(json.dumps(report, indent=2, ensure_ascii=False))
        return

    if args.command == "repair-missing":
        ensure_schema()
        result = asyncio.run(
            repair_missing_variables(
                chunk_size=args.chunk,
                concurrency=args.concurrent,
                limit=args.limit,
                max_retries=args.retries,
            )
        )
        payload = asdict(result)
        print(json.dumps(payload, indent=2, ensure_ascii=False))
        if result.failures:
            preview = result.failures[: min(10, len(result.failures))]
            print("Failures:")
            for agregado_id, message in preview:
                print(f"   {agregado_id}: {message}")
            if len(result.failures) > len(preview):
                print("   ...")
        return

    if args.command == "embed":
        ensure_schema()
        if args.agregado_ids:
            ids = list(dict.fromkeys(args.agregado_ids))
        else:
            with sqlite_session() as conn:
                rows = conn.execute("SELECT id FROM agregados ORDER BY id").fetchall()
                ids = [int(row["id"]) for row in rows]

        if not ids:
            print("No agregados available for embedding.")
            return

        print(f"Embedding {len(ids)} agregados with concurrency={args.concurrent}")
        asyncio.run(
            _embed_ids(
                ids,
                args.concurrent,
                model=args.model,
            )
        )
        return

    if args.command == "search":
        try:
            weight_overrides = _parse_weight_overrides(args.weights)
        except ValueError as exc:
            parser.error(str(exc))

        try:
            category_filters = _parse_category_filters(args.has_categories)
        except ValueError as exc:
            parser.error(str(exc))

        has_variables = args.has_variables or None
        has_classifications = args.has_classifications or None
        has_categories = category_filters or None

        filters = SearchFilters(
            min_municipalities=args.min_municipalities,
            requires_national_munis=args.requires_national_munis,
            subject_contains=args.subject_contains,
            survey_contains=args.survey_contains,
            period_start=args.period_start,
            period_end=args.period_end,
            has_variables=has_variables,
            has_classifications=has_classifications,
            has_categories=has_categories,
        )

        allowed_children = ["variable", "classification", "category"]
        if args.types:
            requested = {item.lower() for item in args.types}
            child_types = [child for child in allowed_children if child in requested]
        else:
            child_types = allowed_children

        expand = max(0, args.expand)

        client = EmbeddingClient(model=args.model) if args.model else EmbeddingClient()
        results = hybrid_search(
            args.query,
            limit=args.limit,
            filters=filters,
            embedding_client=client,
            model=args.model,
            child_types=child_types,
            max_child_matches=expand,
            weights=weight_overrides,
        )
        if not results:
            print("No results found.")
            return
        if args.json:
            payload = []
            for item in results:
                metadata = dict(item.metadata)
                children: list[dict[str, object]] = []
                for child in item.child_matches:
                    child_meta = dict(child.metadata)
                    children.append(
                        {
                            "type": child.entity_type,
                            "id": child.entity_id,
                            "title": child.title,
                            "lexical_score": child.lexical_score,
                            "metadata": child_meta,
                        }
                    )
                payload.append(
                    {
                        "table_id": item.agregado_id,
                        "title": item.title,
                        "description": item.description,
                        "scores": {
                            "combined": item.combined_score,
                            "semantic": item.score,
                            "lexical_table": item.lexical_table_score,
                            "lexical_children": item.lexical_children_score,
                        },
                        "metadata": metadata,
                        "children": children,
                    }
                )
            print(json.dumps(payload, ensure_ascii=False, indent=2))
            return

        if args.flat:
            for item in results:
                base = f"{item.agregado_id}: {item.title}"
                if not item.child_matches:
                    print(f"{base} [score={item.combined_score:.3f}]")
                    continue
                for child in item.child_matches:
                    child_meta = dict(child.metadata)
                    details: list[str] = []
                    unit = child_meta.get("unit")
                    if unit:
                        details.append(f"unit={unit}")
                    classification_id = child_meta.get("classification_id")
                    if classification_id:
                        details.append(f"classification={classification_id}")
                    detail_text = f" ({', '.join(details)})" if details else ""
                    print(
                        f"{item.agregado_id}: [{child.entity_type}] {child.title}{detail_text} "
                        f"lex={child.lexical_score:.3f}"
                    )
            return

        for index, item in enumerate(results, start=1):
            metadata = dict(item.metadata)
            score_line = (
                f"{index}. table={item.agregado_id} "
                f"score={item.combined_score:.3f} "
                f"(sem={item.score:.3f} | lex_table={item.lexical_table_score:.3f} | "
                f"lex_children={item.lexical_children_score:.3f})"
            )
            print(score_line)
            print(f"   {item.title}")
            if item.description:
                print(f"   {item.description}")

            subject = metadata.get("subject")
            survey = metadata.get("survey")
            period_start = metadata.get("period_start")
            period_end = metadata.get("period_end")
            muni_count = metadata.get("municipality_locality_count")
            covers_national = metadata.get("covers_national_municipalities")
            url = metadata.get("url")
            levels_sample = metadata.get("levels_sample")
            levels_count = metadata.get("levels_count")

            info_parts: list[str] = []
            if survey:
                info_parts.append(f"Survey: {survey}")
            if subject:
                info_parts.append(f"Subject: {subject}")
            if period_start or period_end:
                if period_start and period_end and period_start != period_end:
                    info_parts.append(f"Period {period_start}-{period_end}")
                else:
                    info_parts.append(f"Period {period_start or period_end}")
            if muni_count:
                try:
                    formatted = f"{int(muni_count):,}"
                except ValueError:
                    formatted = muni_count
                coverage = f"Municipalities: {formatted}"
                if covers_national in {"1", "True", "true", "yes"}:
                    coverage += " (national)"
                info_parts.append(coverage)
            if levels_sample:
                suffix = f" (total {levels_count})" if levels_count else ""
                info_parts.append(f"Levels: {levels_sample}{suffix}")
            if url:
                info_parts.append(f"URL: {url}")
            if info_parts:
                print(f"   {' | '.join(info_parts)}")

            if item.child_matches:
                print("   Why it matched:")
                for child in item.child_matches:
                    child_meta = dict(child.metadata)
                    details: list[str] = []
                    unit = child_meta.get("unit")
                    if unit:
                        details.append(f"unit={unit}")
                    classification_id = child_meta.get("classification_id")
                    if classification_id:
                        details.append(f"classification={classification_id}")
                    detail_text = f" ({', '.join(details)})" if details else ""
                    print(
                        f"      - [{child.entity_type}] {child.title}{detail_text} "
                        f"lex={child.lexical_score:.3f}"
                    )
            print()
        return

    if args.command == "list":
        rows = list_agregados(
            requires_national_munis=args.requires_national_munis,
            min_municipality_count=args.min_municipalities,
            limit=args.limit,
            order_by=args.order_by,
        )
        if not rows:
            print("No agregados matched the provided filters.")
            return
        for row in rows:
            coverage = f"municipalities={row.municipality_locality_count:,}"
            if row.covers_national_municipalities:
                coverage += " (national)"
            subject = f"assunto={row.assunto}" if row.assunto else "assunto=?"
            survey = f"pesquisa={row.pesquisa}" if row.pesquisa else "pesquisa=?"
            print(f"{row.id}: {row.nome}")
            print(f"   {subject} | {survey} | {coverage} | fetched={row.fetched_at}")
        return

    parser.error("Unknown command")


if __name__ == "__main__":
    main()



###############################################################################
### FILE: config.py
###############################################################################
"""Application configuration via environment variables."""
from __future__ import annotations
try:  # pragma: no cover - exercised indirectly via configuration access
    from pydantic_settings import BaseSettings, SettingsConfigDict
except ImportError:  # pragma: no cover - fallback for minimal test environments
    from pydantic import BaseModel

    class BaseSettings(BaseModel):
        """Fallback minimal BaseSettings replacement."""

        model_config: dict[str, object] = {}

    class SettingsConfigDict(dict):
        """Alias used to mirror the pydantic-settings API."""

        def __init__(self, **kwargs):
            super().__init__(**kwargs)

from pydantic import Field
from functools import lru_cache
import os
import json
from pathlib import Path


class Settings(BaseSettings):
    """Runtime configuration."""

    sidra_base_url: str = Field(
        default="https://servicodados.ibge.gov.br/api/v3/agregados",
        description="Base URL for the IBGE SIDRA aggregated-data API.",
    )
    database_path: str = Field(
        default="sidra.db",
        description="SQLite database filename (relative to project root unless absolute).",
    )
    embedding_api_url: str = Field(
        default="http://127.0.0.1:1234/v1/embeddings",
        description="LM Studio endpoint for embedding generation.",
    )
    embedding_model: str = Field(
        default="text-embedding-qwen3-embedding-0.6b@f16",
        description="Default embedding model identifier used by LM Studio.",
    )
    request_timeout: float = Field(
        default=30.0,
        description="HTTP timeout in seconds for API calls.",
    )
    request_retries: int = Field(
        default=3,
        description="Number of retry attempts for transient API errors.",
    )
    user_agent: str = Field(
        default="sidra-database/0.1",
        description="User agent string for outbound HTTP requests.",
    )
    municipality_national_threshold: int = Field(
        default=4000,
        description="Minimum municipality count (N6) to flag national coverage.",
    )
    database_timeout: float = Field(
        default=30.0,
        description="SQLite busy timeout (seconds) for concurrent writers.",
    )

    model_config = SettingsConfigDict(
        env_prefix="SIDRA_",
        case_sensitive=False,
    )


@lru_cache(maxsize=1)
def get_settings() -> Settings:
    """Return cached Settings instance."""

    override = _load_config_file()
    env_override = _collect_env_overrides()
    if override:
        merged = {**override, **env_override}
        return Settings(**merged)
    if env_override:
        return Settings(**env_override)
    return Settings()


def _collect_env_overrides() -> dict[str, object]:
    overrides: dict[str, object] = {}
    for field_name in Settings.model_fields:
        env_key = f"SIDRA_{field_name.upper()}"
        value = _lookup_environment(env_key)
        if value is not None:
            overrides[field_name] = value
    return overrides


def _lookup_environment(name: str) -> str | None:
    candidates = {name, name.upper(), name.lower()}
    for candidate in candidates:
        if candidate in os.environ:
            return os.environ[candidate]
    return None


def _load_config_file() -> dict[str, object] | None:
    candidates = [
        Path(os.getenv("SIDRA_CONFIG_PATH", "sidra.config.json")),
        Path("sidra.config.json"),
    ]
    for path in candidates:
        if path.is_file():
            try:
                with path.open("r", encoding="utf-8") as handle:
                    return json.load(handle)
            except Exception as exc:  # noqa: BLE001
                raise RuntimeError(f"Failed to load config file at {path}") from exc
    return None


__all__ = ["Settings", "get_settings"]



###############################################################################
### FILE: db.py
###############################################################################
"""SQLite utilities for the SIDRA metadata store."""
from __future__ import annotations

import sqlite3
from contextlib import contextmanager
from pathlib import Path
from typing import Iterator

from .config import get_settings
from .schema import apply_schema


def get_database_path() -> Path:
    settings = get_settings()
    return Path(settings.database_path).expanduser().resolve()


def create_connection() -> sqlite3.Connection:
    settings = get_settings()
    path = get_database_path()
    path.parent.mkdir(parents=True, exist_ok=True)
    timeout = max(float(settings.database_timeout), 60.0)
    connection = sqlite3.connect(path, timeout=timeout, check_same_thread=False)
    connection.execute("PRAGMA journal_mode=WAL")
    connection.execute("PRAGMA synchronous=NORMAL")
    busy_timeout_ms = max(int(timeout * 1000), 60000)
    connection.execute(f"PRAGMA busy_timeout = {busy_timeout_ms}")
    connection.row_factory = sqlite3.Row
    return connection


def ensure_schema(connection: sqlite3.Connection | None = None) -> None:
    close_after = False
    if connection is None:
        connection = create_connection()
        close_after = True
    try:
        apply_schema(connection)
    finally:
        if close_after:
            connection.close()


@contextmanager
def sqlite_session() -> Iterator[sqlite3.Connection]:
    connection = create_connection()
    try:
        yield connection
    finally:
        connection.close()


__all__ = ["create_connection", "ensure_schema", "sqlite_session", "get_database_path"]



###############################################################################
### FILE: diagnostics.py
###############################################################################
"""Diagnostics and repair helpers for SIDRA metadata ingestion."""
from __future__ import annotations

import asyncio
from dataclasses import dataclass
from typing import Any, Iterable, Sequence

from .api_client import SidraApiClient
from .db import sqlite_session
from .ingest import ingest_agregado


def _fetch_scalar(conn, query: str, params: Sequence[Any] | None = None) -> int:
    cursor = conn.execute(query, params or ())
    row = cursor.fetchone()
    return int(row[0]) if row and row[0] is not None else 0


def _get_index_presence(conn, table: str, expected: Iterable[str]) -> dict[str, bool]:
    cursor = conn.execute(f"PRAGMA index_list({table})")
    available = {row[1] for row in cursor.fetchall()}
    return {name: name in available for name in expected}


def collect_missing_variable_ids(conn, *, limit: int | None = None) -> list[int]:
    query = (
        "SELECT a.id FROM agregados AS a "
        "LEFT JOIN (SELECT DISTINCT agregado_id FROM variables) AS v ON v.agregado_id = a.id "
        "WHERE v.agregado_id IS NULL ORDER BY a.id"
    )
    if limit is not None:
        query += f" LIMIT {int(limit)}"
    cursor = conn.execute(query)
    return [int(row[0]) for row in cursor.fetchall()]


def global_health_report(conn, *, sample_limit: int = 50) -> dict[str, Any]:
    total_agregados = _fetch_scalar(conn, "SELECT COUNT(*) FROM agregados")
    agregados_with_variables = _fetch_scalar(
        conn, "SELECT COUNT(DISTINCT agregado_id) FROM variables"
    )
    agregados_with_classifications = _fetch_scalar(
        conn, "SELECT COUNT(DISTINCT agregado_id) FROM classifications"
    )
    agregados_with_categories = _fetch_scalar(
        conn, "SELECT COUNT(DISTINCT agregado_id) FROM categories"
    )

    missing_ids = collect_missing_variable_ids(conn)
    sample_missing = missing_ids[:sample_limit]

    index_status = {
        "variables": _get_index_presence(conn, "variables", ["idx_variables_agregado"]),
        "categories": _get_index_presence(conn, "categories", ["idx_categories_agregado"]),
        "localities": _get_index_presence(conn, "localities", ["idx_localities_agregado"]),
        "embeddings": _get_index_presence(conn, "embeddings", ["idx_embeddings_agregado"]),
    }

    journal_row = conn.execute("PRAGMA journal_mode").fetchone()
    journal_mode = str(journal_row[0]).upper() if journal_row else "UNKNOWN"

    return {
        "counts": {
            "agregados": total_agregados,
            "agregados_with_variables": agregados_with_variables,
            "agregados_with_classifications": agregados_with_classifications,
            "agregados_with_categories": agregados_with_categories,
            "agregados_missing_variables": len(missing_ids),
        },
        "sample_missing_variables": sample_missing,
        "indexes": index_status,
        "journal_mode": journal_mode,
    }


async def api_vs_db_spot_check(
    conn,
    *,
    sample_size: int = 10,
    client: SidraApiClient | None = None,
) -> dict[str, Any]:
    missing_ids = collect_missing_variable_ids(conn, limit=sample_size)
    if not missing_ids:
        return {
            "sampled": 0,
            "api_nonempty": 0,
            "api_empty": 0,
            "errors": 0,
            "details": [],
        }

    own_client = False
    if client is None:
        client = SidraApiClient()
        own_client = True

    results: list[dict[str, Any]] = []
    try:
        for agregado_id in missing_ids:
            try:
                metadata = await client.fetch_metadata(agregado_id)
                variaveis = metadata.get("variaveis") if isinstance(metadata, dict) else None
                if isinstance(variaveis, list):
                    count = len(variaveis)
                else:
                    count = 0
                results.append(
                    {
                        "agregado_id": agregado_id,
                        "variables_in_api": count,
                        "error": None,
                    }
                )
            except Exception as exc:  # noqa: BLE001
                results.append(
                    {
                        "agregado_id": agregado_id,
                        "variables_in_api": None,
                        "error": str(exc)[:200],
                    }
                )
    finally:
        if own_client:
            await client.close()

    api_nonempty = sum(1 for item in results if (item["variables_in_api"] or 0) > 0)
    api_empty = sum(1 for item in results if item["variables_in_api"] == 0)
    errors = sum(1 for item in results if item["error"])

    return {
        "sampled": len(results),
        "api_nonempty": api_nonempty,
        "api_empty": api_empty,
        "errors": errors,
        "details": results,
    }


@dataclass
class RepairResult:
    attempted: int
    succeeded: int
    failed: int
    failures: list[tuple[int, str]]


async def _ingest_chunk(
    ids: Sequence[int],
    *,
    concurrency: int,
    client: SidraApiClient,
) -> list[tuple[int, bool, str | None]]:
    semaphore = asyncio.Semaphore(max(1, concurrency))
    results: list[tuple[int, bool, str | None]] = []

    async def worker(agregado_id: int) -> None:
        async with semaphore:
            try:
                await ingest_agregado(
                    agregado_id,
                    client=client,
                    generate_embeddings=False,
                )
                results.append((agregado_id, True, None))
            except Exception as exc:  # noqa: BLE001
                results.append((agregado_id, False, str(exc)[:200]))

    await asyncio.gather(*(worker(ag_id) for ag_id in ids))
    return results


async def repair_missing_variables(
    *,
    chunk_size: int = 50,
    concurrency: int = 6,
    limit: int | None = None,
    max_retries: int = 3,
) -> RepairResult:
    with sqlite_session() as conn:
        missing_ids = collect_missing_variable_ids(conn, limit=limit)

    if not missing_ids:
        return RepairResult(attempted=0, succeeded=0, failed=0, failures=[])

    attempted = len(missing_ids)
    succeeded = 0
    failures: list[tuple[int, str]] = []

    async with SidraApiClient() as client:
        for start in range(0, len(missing_ids), max(1, chunk_size)):
            chunk = missing_ids[start : start + max(1, chunk_size)]
            remaining = list(chunk)
            attempt = 0
            partial_failures: list[tuple[int, str]] = []
            while remaining and attempt < max(1, max_retries):
                attempt += 1
                results = await _ingest_chunk(
                    remaining,
                    concurrency=concurrency,
                    client=client,
                )
                remaining = [ag_id for ag_id, ok, _ in results if not ok]
                succeeded += sum(1 for _, ok, _ in results if ok)
                partial_failures = [
                    (ag_id, error or "unknown error")
                    for ag_id, ok, error in results
                    if not ok
                ]

            failures.extend(partial_failures)

    failed = len(failures)
    return RepairResult(
        attempted=attempted,
        succeeded=succeeded,
        failed=failed,
        failures=failures,
    )


__all__ = [
    "RepairResult",
    "api_vs_db_spot_check",
    "collect_missing_variable_ids",
    "global_health_report",
    "repair_missing_variables",
]




###############################################################################
### FILE: discovery.py
###############################################################################
"""Catalog discovery helpers for SIDRA agregados."""
from __future__ import annotations

from dataclasses import dataclass
from collections.abc import Iterable, Mapping, Sequence
from typing import Any

from .api_client import SidraApiClient


def _normalize_levels(payload: Any) -> dict[str, list[str]]:
    """Return a mapping of level type -> normalized codes from raw API payload."""

    result: dict[str, list[str]] = {}
    if isinstance(payload, dict):
        for key, values in payload.items():
            values_iter: Iterable[Any]
            if isinstance(values, str):
                values_iter = [values]
            elif isinstance(values, Iterable):
                values_iter = values
            else:
                continue
            normalized = []
            for value in values_iter:
                if isinstance(value, str):
                    normalized.append(value.upper())
                elif isinstance(value, Mapping):
                    code = value.get("codigo") or value.get("nivel") or value.get("id")
                    if isinstance(code, str):
                        normalized.append(code.upper())
            if normalized:
                result[str(key)] = normalized
    elif isinstance(payload, list):
        normalized: list[str] = []
        for value in payload:
            if isinstance(value, str):
                normalized.append(value.upper())
            elif isinstance(value, Mapping):
                code = value.get("codigo") or value.get("nivel") or value.get("id")
                if isinstance(code, str):
                    normalized.append(code.upper())
        if normalized:
            result["codes"] = normalized
    return result


@dataclass(frozen=True)
class CatalogEntry:
    """Lightweight representation of an agregados catalog record."""

    id: int
    nome: str | None
    pesquisa: str | None
    pesquisa_id: int | None
    assunto: str | None
    assunto_id: int | None
    periodicidade: Any
    nivel_territorial: dict[str, list[str]]
    level_hints: frozenset[str] = frozenset()

    @property
    def level_codes(self) -> set[str]:
        """Return all territorial level codes exposed by this agregados."""

        codes: set[str] = set()
        for values in self.nivel_territorial.values():
            for value in values:
                codes.add(value.upper())
        codes.update(self.level_hints)
        return codes

    @classmethod
    def from_api(
        cls,
        payload: dict[str, Any],
        *,
        pesquisa: dict[str, Any] | None = None,
        level_hints: Sequence[str] | None = None,
    ) -> "CatalogEntry":
        """Build a catalog entry from the API payload."""

        level_payload = payload.get("nivelTerritorial")
        nome = payload.get("nome") or payload.get("tabela")
        pesquisa_nome = None
        pesquisa_id = None
        assunto_nome = None
        assunto_id = None
        periodicidade = None

        if pesquisa is None:
            pesquisa = {}

        if isinstance(pesquisa, dict):
            pesquisa_nome = pesquisa.get("pesquisa") or pesquisa.get("nome")
            pesquisa_id = pesquisa.get("idPesquisa") or pesquisa.get("id")
            assunto = pesquisa.get("assunto") or {}
            if isinstance(assunto, dict):
                assunto_nome = assunto.get("nome")
                assunto_id = assunto.get("id")
            else:
                assunto_nome = pesquisa.get("assunto")
                assunto_id = pesquisa.get("idAssunto")
            periodicidade = pesquisa.get("periodicidade")

        return cls(
            id=int(payload.get("id")),
            nome=str(nome) if nome is not None else None,
            pesquisa=str(pesquisa_nome) if pesquisa_nome is not None else None,
            pesquisa_id=int(pesquisa_id) if isinstance(pesquisa_id, int) else None,
            assunto=str(assunto_nome) if assunto_nome is not None else None,
            assunto_id=int(assunto_id) if isinstance(assunto_id, int) else None,
            periodicidade=periodicidade,
            nivel_territorial=_normalize_levels(level_payload),
            level_hints=frozenset(code.upper() for code in level_hints or [] if code),
        )


async def fetch_catalog_entries(
    *,
    client: SidraApiClient | None = None,
    subject_id: int | None = None,
    periodicity: str | None = None,
    levels: Sequence[str] | None = None,
) -> list[CatalogEntry]:
    """Fetch the agregados catalog and normalize it into CatalogEntry rows."""

    own_client = False
    if client is None:
        client = SidraApiClient()
        own_client = True

    normalized_levels = [code.upper() for code in levels or [] if code]

    try:
        catalog = await client.fetch_catalog(
            subject_id=subject_id,
            periodicity=periodicity,
            levels=normalized_levels or None,
        )
    finally:
        if own_client:
            await client.close()

    entries: list[CatalogEntry] = []
    if not isinstance(catalog, Iterable):
        return entries

    for survey in catalog:
        agregados = None
        if isinstance(survey, dict):
            agregados = survey.get("agregados")
        if not isinstance(agregados, Iterable):
            continue
        for agregado in agregados:
            if not isinstance(agregado, dict):
                continue
            try:
                entry = CatalogEntry.from_api(
                    agregado,
                    pesquisa=survey,
                    level_hints=normalized_levels,
                )
            except Exception:  # noqa: BLE001
                continue
            entries.append(entry)
    return entries


def filter_catalog_entries(
    entries: Sequence[CatalogEntry],
    *,
    require_any_levels: Iterable[str] | None = None,
    require_all_levels: Iterable[str] | None = None,
    exclude_levels: Iterable[str] | None = None,
    subject_contains: str | None = None,
    survey_contains: str | None = None,
) -> list[CatalogEntry]:
    """Filter CatalogEntry items by territorial coverage and optional metadata."""

    any_levels = {code.upper() for code in require_any_levels or ()}
    all_levels = {code.upper() for code in require_all_levels or ()}
    excluded = {code.upper() for code in exclude_levels or ()}
    subject_query = subject_contains.lower() if subject_contains else None
    survey_query = survey_contains.lower() if survey_contains else None

    filtered: list[CatalogEntry] = []
    for entry in entries:
        codes = entry.level_codes
        if any_levels and not (codes & any_levels):
            continue
        if all_levels and not all_levels.issubset(codes):
            continue
        if excluded and (codes & excluded):
            continue
        if subject_query and (entry.assunto or "").lower().find(subject_query) == -1:
            continue
        if survey_query and (entry.pesquisa or "").lower().find(survey_query) == -1:
            continue
        filtered.append(entry)
    return filtered


__all__ = [
    "CatalogEntry",
    "fetch_catalog_entries",
    "filter_catalog_entries",
]



###############################################################################
### FILE: embedding.py
###############################################################################
"""Embedding helpers using LM Studio local API."""
from __future__ import annotations

from typing import Iterable, Sequence

import httpx
import orjson

from .config import get_settings


class EmbeddingClient:
    """Synchronous client for LM Studio embedding endpoint."""

    def __init__(self, *, base_url: str | None = None, model: str | None = None, timeout: float | None = None) -> None:
        settings = get_settings()
        self._base_url = base_url or settings.embedding_api_url
        self._model = model or settings.embedding_model
        self._timeout = timeout or settings.request_timeout
        self._headers = {"Content-Type": "application/json", "User-Agent": settings.user_agent}

    @property
    def model(self) -> str:
        """Return the default embedding model configured for this client."""

        return self._model

    def embed_text(self, text: str, *, model: str | None = None) -> Sequence[float]:
        payload = {
            "model": model or self._model,
            "input": text,
        }
        response = httpx.post(
            self._base_url,
            content=orjson.dumps(payload),
            headers=self._headers,
            timeout=self._timeout,
        )
        response.raise_for_status()
        data = response.json()
        return data["data"][0]["embedding"]

    def embed_batch(self, texts: Iterable[str], *, model: str | None = None) -> list[Sequence[float]]:
        payload = {
            "model": model or self._model,
            "input": list(texts),
        }
        response = httpx.post(
            self._base_url,
            content=orjson.dumps(payload),
            headers=self._headers,
            timeout=self._timeout,
        )
        response.raise_for_status()
        data = response.json()
        return [item["embedding"] for item in data["data"]]


__all__ = ["EmbeddingClient"]



###############################################################################
### FILE: ingest.py
###############################################################################
"""Metadata ingestion pipeline."""
from __future__ import annotations

import asyncio
import hashlib
from array import array
from datetime import datetime, timezone
from typing import Any, NamedTuple, Sequence

import orjson

from .api_client import SidraApiClient
from .config import get_settings
from .db import sqlite_session
from .embedding import EmbeddingClient

ISO_FORMAT = "%Y-%m-%dT%H:%M:%SZ"
MUNICIPALITY_LEVEL_CODE = "N6"


class _EmbeddingTarget(NamedTuple):
    entity_type: str
    entity_id: str
    agregado_id: int
    text: str
    text_hash: str


def _utcnow() -> str:
    return datetime.now(timezone.utc).strftime(ISO_FORMAT)


def _hash_text(*parts: str) -> str:
    joined = "||".join(part or "" for part in parts)
    return hashlib.sha256(joined.encode("utf-8")).hexdigest()


def _json_dumps(obj: Any) -> bytes:
    return orjson.dumps(obj)


def _json_dump_text(obj: Any) -> str:
    return orjson.dumps(obj).decode("utf-8")


def _line_or_none(prefix: str, value: Any) -> str | None:
    if value is None:
        return None
    text = str(value).strip()
    if not text:
        return None
    return f"{prefix}{text}"


def _canonical_agregado_text(metadata: dict[str, Any]) -> str:
    periodicidade = metadata.get("periodicidade") or {}
    freq = periodicidade.get("frequencia")
    inicio = periodicidade.get("inicio")
    fim = periodicidade.get("fim")

    if inicio and fim:
        period_line = f"Period: {inicio} - {fim}" if inicio != fim else f"Period: {inicio}"
    elif inicio or fim:
        period_line = f"Period: {inicio or fim}"
    else:
        period_line = None

    nivel = metadata.get("nivelTerritorial") or {}
    level_parts: list[str] = []
    for level_type in sorted(nivel):
        codes = nivel.get(level_type) or []
        if codes:
            level_parts.append(f"{level_type}: {', '.join(codes)}")

    lines = [
        f"Table {metadata.get('id')}: {str(metadata.get('nome') or '').strip()}".strip(),
        _line_or_none("Survey: ", metadata.get("pesquisa")),
        _line_or_none("Subject: ", metadata.get("assunto")),
        _line_or_none("Frequency: ", freq),
        period_line,
        f"Territorial levels: {'; '.join(level_parts)}" if level_parts else None,
        _line_or_none("URL: ", metadata.get("URL")),
    ]
    return "\n".join(line for line in lines if line)


def _canonical_variable_text(metadata: dict[str, Any], variable: dict[str, Any]) -> str:
    lines = [
        f"Table {metadata.get('id')}: {str(metadata.get('nome') or '').strip()}".strip(),
        f"Variable {variable.get('id')}: {str(variable.get('nome') or '').strip()}".strip(),
    ]
    unit_line = _line_or_none("Unit: ", variable.get("unidade"))
    if unit_line:
        lines.append(unit_line)
    subject_line = _line_or_none("Subject: ", metadata.get("assunto"))
    if subject_line:
        lines.append(subject_line)
    survey_line = _line_or_none("Survey: ", metadata.get("pesquisa"))
    if survey_line:
        lines.append(survey_line)
    summary_data = variable.get("sumarizacao")
    if summary_data:
        lines.append(f"Summarization: {_json_dump_text(summary_data)}")
    return "\n".join(line for line in lines if line)


def _canonical_classification_text(metadata: dict[str, Any], classificacao: dict[str, Any]) -> str:
    lines = [
        f"Table {metadata.get('id')}: {str(metadata.get('nome') or '').strip()}".strip(),
        f"Classification {classificacao.get('id')}: {str(classificacao.get('nome') or '').strip()}".strip(),
    ]
    summary_payload = classificacao.get("sumarizacao")
    if isinstance(summary_payload, dict):
        status = summary_payload.get("status")
        if status is not None:
            lines.append(f"Summarization enabled: {bool(status)}")
        excecao = summary_payload.get("excecao")
        if excecao:
            lines.append(f"Exceptions: {_json_dump_text(excecao)}")
    return "\n".join(line for line in lines if line)


def _canonical_category_text(
    metadata: dict[str, Any],
    classificacao: dict[str, Any],
    categoria: dict[str, Any],
) -> str:
    lines = [
        f"Table {metadata.get('id')}: {str(metadata.get('nome') or '').strip()}".strip(),
        f"Classification {classificacao.get('id')}: {str(classificacao.get('nome') or '').strip()}".strip(),
        f"Category {categoria.get('id')}: {str(categoria.get('nome') or '').strip()}".strip(),
    ]
    unit_line = _line_or_none("Unit: ", categoria.get("unidade"))
    if unit_line:
        lines.append(unit_line)
    level = categoria.get("nivel")
    if level is not None:
        lines.append(f"Level: {level}")
    return "\n".join(line for line in lines if line)


def _build_embedding_targets(agregado_id: int, metadata: dict[str, Any]) -> list[_EmbeddingTarget]:
    table_text = _canonical_agregado_text(metadata)
    if not table_text:
        return []
    return [
        _EmbeddingTarget(
            "agregado",
            str(agregado_id),
            agregado_id,
            table_text,
            _hash_text("agregado", str(agregado_id), table_text),
        )
    ]


def _vector_to_blob(vector: Sequence[float]) -> bytes:
    arr = array("f", (float(value) for value in vector))
    return arr.tobytes()


async def _persist_embeddings(
    conn,
    targets: list[_EmbeddingTarget],
    embedding_client: EmbeddingClient,
    timestamp: str,
) -> None:
    if not targets:
        return

    model_name = embedding_client.model
    for target in targets:
        if not target.text.strip():
            continue
        existing = conn.execute(
            "SELECT text_hash FROM embeddings WHERE entity_type = ? AND entity_id = ? AND model = ?",
            (target.entity_type, target.entity_id, model_name),
        ).fetchone()
        if existing and existing["text_hash"] == target.text_hash:
            continue

        vector = await asyncio.to_thread(embedding_client.embed_text, target.text, model=model_name)
        dimension = len(vector)
        if dimension == 0:
            continue

        conn.execute(
            """
            INSERT OR REPLACE INTO embeddings (
                entity_type, entity_id, agregado_id, text_hash, model, dimension, vector, created_at
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """,
            (
                target.entity_type,
                target.entity_id,
                target.agregado_id,
                target.text_hash,
                model_name,
                dimension,
                _vector_to_blob(vector),
                timestamp,
            ),
        )


async def ingest_agregado(
    agregado_id: int,
    *,
    client: SidraApiClient | None = None,
    embedding_client: EmbeddingClient | None = None,
    generate_embeddings: bool = True,
    db_lock: asyncio.Lock | None = None,
) -> None:
    """Fetch and persist metadata for a single agregados table."""

    own_client = False
    settings = get_settings()
    if client is None:
        client = SidraApiClient()
        own_client = True
    if generate_embeddings:
        if embedding_client is None:
            embedding_client = EmbeddingClient()
    else:
        embedding_client = None
    try:
        try:
            raw_metadata = await client.fetch_metadata(agregado_id)
        except Exception:
            _log_ingestion_status(agregado_id, "error", "api")
            raise

        try:
            metadata = _validate_metadata_payload(raw_metadata)
        except IngestionValidationError:
            _log_ingestion_status(agregado_id, "error", "validation")
            raise

        try:
            periods = await client.fetch_periods(agregado_id)
        except Exception:
            _log_ingestion_status(agregado_id, "error", "api")
            raise

        nivel_groups_raw = metadata.get("nivelTerritorial", {}) or {}
        nivel_groups = nivel_groups_raw if isinstance(nivel_groups_raw, dict) else {}

        municipality_locality_count: int = 0
        level_rows: list[tuple[int, str, str | None, str, int]] = []
        locality_rows: list[tuple[int, str, str | None, str | None]] = []
        for level_type, codes in nivel_groups.items():
            if not codes:
                continue
            for level_code in codes:
                try:
                    localities = await client.fetch_localities(agregado_id, level_code)
                except Exception:
                    _log_ingestion_status(agregado_id, "error", "api")
                    raise
                raw_payload = localities or []
                if isinstance(raw_payload, list):
                    payload = raw_payload
                else:
                    try:
                        payload = list(raw_payload)
                    except TypeError:
                        payload = []
                count = len(payload)
                if level_code.upper() == MUNICIPALITY_LEVEL_CODE and count > municipality_locality_count:
                    municipality_locality_count = count
                level_name = None
                if payload:
                    level_name = (payload[0].get("nivel", {}) or {}).get("nome")
                level_rows.append(
                    (
                        agregado_id,
                        level_code,
                        level_name,
                        level_type,
                        count,
                    )
                )
                for loc in payload:
                    locality_rows.append(
                        (
                            agregado_id,
                            level_code,
                            loc.get("id"),
                            loc.get("nome"),
                        )
                    )

        threshold_setting = max(0, int(settings.municipality_national_threshold))
        if threshold_setting == 0:
            covers_national_munis = 1 if municipality_locality_count > 0 else 0
        else:
            covers_national_munis = 1 if municipality_locality_count >= threshold_setting else 0

        variables_payload = metadata.get("variaveis") or []
        variable_rows: list[tuple[Any, int, Any, Any, str, str]] = []
        for variable in variables_payload:
            variable_rows.append(
                (
                    variable.get("id"),
                    agregado_id,
                    variable.get("nome"),
                    variable.get("unidade"),
                    _json_dump_text(variable.get("sumarizacao", [])),
                    _hash_text(
                        str(variable.get("id")),
                        variable.get("nome", ""),
                        variable.get("unidade", ""),
                    ),
                )
            )

        classification_rows: list[tuple[int, int, str | None, int, str]] = []
        category_rows: list[tuple[int, int, int, str | None, str | None, Any, str]] = []
        for classificacao in metadata.get("classificacoes", []) or []:
            cid = classificacao.get("id")
            classification_rows.append(
                (
                    cid,
                    agregado_id,
                    classificacao.get("nome"),
                    1 if (classificacao.get("sumarizacao", {}).get("status")) else 0,
                    _json_dump_text(classificacao.get("sumarizacao", {}).get("excecao", [])),
                )
            )
            for categoria in classificacao.get("categorias", []) or []:
                category_rows.append(
                    (
                        agregado_id,
                        cid,
                        categoria.get("id"),
                        categoria.get("nome"),
                        categoria.get("unidade"),
                        categoria.get("nivel"),
                        _hash_text(
                            str(cid),
                            str(categoria.get("id")),
                            categoria.get("nome", ""),
                            categoria.get("unidade", ""),
                        ),
                    )
                )

        period_rows: list[tuple[int, str, str, Any]] = []
        for period in periods or []:
            pid = period.get("id") if isinstance(period, dict) else period
            literals = period.get("literals", [pid]) if isinstance(period, dict) else [period]
            modificacao = period.get("modificacao") if isinstance(period, dict) else None
            period_rows.append((agregado_id, str(pid), _json_dump_text(literals), modificacao))

        embedding_targets: list[_EmbeddingTarget] = []
        if generate_embeddings:
            embedding_targets = _build_embedding_targets(agregado_id, metadata)

        fetched_at = _utcnow()

        async def _write_to_database() -> None:
            with sqlite_session() as conn:
                conn.execute("BEGIN")
                try:
                    conn.execute(
                        """
                        INSERT OR REPLACE INTO agregados (
                            id, nome, pesquisa, assunto, url, freq, periodo_inicio, periodo_fim, raw_json, fetched_at,
                            municipality_locality_count, covers_national_municipalities
                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                        """,
                        (
                            metadata.get("id"),
                            metadata.get("nome"),
                            metadata.get("pesquisa"),
                            metadata.get("assunto"),
                            metadata.get("URL"),
                            metadata.get("periodicidade", {}).get("frequencia"),
                            metadata.get("periodicidade", {}).get("inicio"),
                            metadata.get("periodicidade", {}).get("fim"),
                            _json_dumps(metadata),
                            fetched_at,
                            municipality_locality_count,
                            covers_national_munis,
                        ),
                    )

                    if level_rows:
                        conn.execute("DELETE FROM agregados_levels WHERE agregado_id = ?", (agregado_id,))
                        conn.executemany(
                            """
                            INSERT OR REPLACE INTO agregados_levels (
                                agregado_id, level_id, level_name, level_type, locality_count
                            ) VALUES (?, ?, ?, ?, ?)
                            """,
                            level_rows,
                        )

                    if variable_rows:
                        conn.execute("DELETE FROM variables WHERE agregado_id = ?", (agregado_id,))
                        conn.executemany(
                            """
                            INSERT OR REPLACE INTO variables (
                                id, agregado_id, nome, unidade, sumarizacao, text_hash
                            ) VALUES (?, ?, ?, ?, ?, ?)
                            """,
                            variable_rows,
                        )

                    if classification_rows:
                        conn.execute("DELETE FROM classifications WHERE agregado_id = ?", (agregado_id,))
                        conn.executemany(
                            """
                            INSERT OR REPLACE INTO classifications (
                                id, agregado_id, nome, sumarizacao_status, sumarizacao_excecao
                            ) VALUES (?, ?, ?, ?, ?)
                            """,
                            classification_rows,
                        )

                    if category_rows:
                        conn.execute("DELETE FROM categories WHERE agregado_id = ?", (agregado_id,))
                        conn.executemany(
                            """
                            INSERT OR REPLACE INTO categories (
                                agregado_id, classification_id, categoria_id, nome, unidade, nivel, text_hash
                            ) VALUES (?, ?, ?, ?, ?, ?, ?)
                            """,
                            category_rows,
                        )

                    if period_rows:
                        conn.execute("DELETE FROM periods WHERE agregado_id = ?", (agregado_id,))
                        conn.executemany(
                            """
                            INSERT OR REPLACE INTO periods (
                                agregado_id, periodo_id, literals, modificacao
                            ) VALUES (?, ?, ?, ?)
                            """,
                            period_rows,
                        )

                    if locality_rows:
                        conn.execute("DELETE FROM localities WHERE agregado_id = ?", (agregado_id,))
                        conn.executemany(
                            """
                            INSERT OR REPLACE INTO localities (
                                agregado_id, level_id, locality_id, nome
                            ) VALUES (?, ?, ?, ?)
                            """,
                            locality_rows,
                        )

                    if generate_embeddings and embedding_targets and embedding_client is not None:
                        await _persist_embeddings(conn, embedding_targets, embedding_client, fetched_at)

                    conn.execute(
                        """
                        INSERT INTO ingestion_log (agregado_id, stage, status, detail, run_at)
                        VALUES (?, ?, ?, ?, ?)
                        """,
                        (agregado_id, "metadata", "success", None, fetched_at),
                    )

                    conn.commit()
                except Exception:
                    conn.rollback()
                    raise

        try:
            if db_lock is not None:
                async with db_lock:
                    await _write_to_database()
            else:
                await _write_to_database()
        except Exception:
            _log_ingestion_status(agregado_id, "error", "db")
            raise

    finally:
        if own_client:
            await client.close()


async def generate_embeddings_for_agregado(
    agregado_id: int,
    *,
    embedding_client: EmbeddingClient | None = None,
    db_lock: asyncio.Lock | None = None,
) -> None:
    """Regenerate embeddings for an already ingested agregado."""

    if embedding_client is None:
        embedding_client = EmbeddingClient()

    with sqlite_session() as conn:
        row = conn.execute(
            "SELECT raw_json FROM agregados WHERE id = ?",
            (agregado_id,),
        ).fetchone()
    if row is None:
        raise ValueError(f"Agregado {agregado_id} not found in database")

    metadata = orjson.loads(row["raw_json"])
    if not metadata or not metadata.get("nome"):
        # Synthetic fixtures may not provide the full metadata payload. Skip
        # embedding generation if the required context is missing to avoid
        # spurious network calls during tests.
        return
    embedding_targets = _build_embedding_targets(agregado_id, metadata)
    if not embedding_targets:
        return

    fetched_at = _utcnow()

    async def _write() -> None:
        with sqlite_session() as conn:
            conn.execute("BEGIN")
            await _persist_embeddings(conn, embedding_targets, embedding_client, fetched_at)
            conn.commit()

    if db_lock is not None:
        async with db_lock:
            await _write()
    else:
        await _write()


def ingest_agregado_sync(agregado_id: int) -> None:
    asyncio.run(ingest_agregado(agregado_id))


__all__ = ["ingest_agregado", "ingest_agregado_sync", "generate_embeddings_for_agregado"]
class IngestionValidationError(RuntimeError):
    """Raised when the SIDRA API returns an incomplete metadata payload."""


def _validate_metadata_payload(metadata: Any) -> dict[str, Any]:
    if not isinstance(metadata, dict):
        raise IngestionValidationError("metadata payload is not a JSON object")

    nome = metadata.get("nome")
    if not isinstance(nome, str) or not nome.strip():
        raise IngestionValidationError("metadata payload missing 'nome'")

    variaveis = metadata.get("variaveis")
    if variaveis is not None and not isinstance(variaveis, list):
        raise IngestionValidationError("metadata field 'variaveis' is not a list")
    classificacoes = metadata.get("classificacoes")
    if classificacoes is not None and not isinstance(classificacoes, list):
        raise IngestionValidationError("metadata field 'classificacoes' is not a list")
    has_variables = isinstance(variaveis, list) and any(variaveis)
    has_classifications = isinstance(classificacoes, list) and any(classificacoes)
    if not has_variables and not has_classifications:
        raise IngestionValidationError("metadata payload missing both 'variaveis' and 'classificacoes'")

    return metadata


def _log_ingestion_status(agregado_id: int, status: str, detail: str) -> None:
    timestamp = _utcnow()

    with sqlite_session() as conn:
        with conn:
            conn.execute(
                """
                INSERT INTO ingestion_log (agregado_id, stage, status, detail, run_at)
                VALUES (?, ?, ?, ?, ?)
                """,
                (agregado_id, "metadata", status, detail, timestamp),
            )





###############################################################################
### FILE: schema.py
###############################################################################
"""SQLite schema creation helpers."""
from __future__ import annotations

SCHEMA_STATEMENTS: tuple[str, ...] = (
    """
    CREATE TABLE IF NOT EXISTS agregados (
        id INTEGER PRIMARY KEY,
        nome TEXT NOT NULL,
        pesquisa TEXT,
        assunto TEXT,
        url TEXT,
        freq TEXT,
        periodo_inicio TEXT,
        periodo_fim TEXT,
        raw_json BLOB NOT NULL,
        fetched_at TEXT NOT NULL,
        municipality_locality_count INTEGER DEFAULT 0,
        covers_national_municipalities INTEGER DEFAULT 0
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS agregados_levels (
        agregado_id INTEGER NOT NULL,
        level_id TEXT NOT NULL,
        level_name TEXT,
        level_type TEXT NOT NULL,
        locality_count INTEGER DEFAULT 0,
        PRIMARY KEY (agregado_id, level_id, level_type),
        FOREIGN KEY (agregado_id) REFERENCES agregados(id)
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS variables (
        id INTEGER PRIMARY KEY,
        agregado_id INTEGER NOT NULL,
        nome TEXT NOT NULL,
        unidade TEXT,
        sumarizacao TEXT,
        text_hash TEXT NOT NULL,
        FOREIGN KEY (agregado_id) REFERENCES agregados(id)
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS classifications (
        id INTEGER NOT NULL,
        agregado_id INTEGER NOT NULL,
        nome TEXT NOT NULL,
        sumarizacao_status INTEGER,
        sumarizacao_excecao TEXT,
        PRIMARY KEY (agregado_id, id),
        FOREIGN KEY (agregado_id) REFERENCES agregados(id)
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS categories (
        agregado_id INTEGER NOT NULL,
        classification_id INTEGER NOT NULL,
        categoria_id INTEGER NOT NULL,
        nome TEXT NOT NULL,
        unidade TEXT,
        nivel INTEGER,
        text_hash TEXT NOT NULL,
        PRIMARY KEY (agregado_id, classification_id, categoria_id),
        FOREIGN KEY (agregado_id, classification_id) REFERENCES classifications(agregado_id, id)
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS periods (
        agregado_id INTEGER NOT NULL,
        periodo_id TEXT NOT NULL,
        literals TEXT NOT NULL,
        modificacao TEXT,
        PRIMARY KEY (agregado_id, periodo_id),
        FOREIGN KEY (agregado_id) REFERENCES agregados(id)
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS localities (
        agregado_id INTEGER NOT NULL,
        level_id TEXT NOT NULL,
        locality_id TEXT NOT NULL,
        nome TEXT NOT NULL,
        PRIMARY KEY (agregado_id, level_id, locality_id),
        FOREIGN KEY (agregado_id, level_id) REFERENCES agregados_levels(agregado_id, level_id)
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS embeddings (
        entity_type TEXT NOT NULL,
        entity_id TEXT NOT NULL,
        agregado_id INTEGER,
        text_hash TEXT NOT NULL,
        model TEXT NOT NULL,
        dimension INTEGER NOT NULL,
        vector BLOB NOT NULL,
        created_at TEXT NOT NULL,
        PRIMARY KEY (entity_type, entity_id, model)
    )
    """,
    """
    CREATE TABLE IF NOT EXISTS ingestion_log (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        agregado_id INTEGER NOT NULL,
        stage TEXT NOT NULL,
        status TEXT NOT NULL,
        detail TEXT,
        run_at TEXT NOT NULL
    )
    """,
    """
    CREATE INDEX IF NOT EXISTS idx_variables_agregado ON variables(agregado_id)
    """,
    """
    CREATE INDEX IF NOT EXISTS idx_categories_agregado ON categories(agregado_id, classification_id)
    """,
    """
    CREATE INDEX IF NOT EXISTS idx_localities_agregado ON localities(agregado_id, level_id)
    """,
    """
    CREATE INDEX IF NOT EXISTS idx_embeddings_agregado ON embeddings(agregado_id, model)
    """
)

ADDITIONAL_COLUMNS: tuple[tuple[str, str, str], ...] = (
    ("agregados", "municipality_locality_count", "INTEGER DEFAULT 0"),
    ("agregados", "covers_national_municipalities", "INTEGER DEFAULT 0"),
    ("agregados_levels", "locality_count", "INTEGER DEFAULT 0"),
)


def _column_exists(connection, table: str, column: str) -> bool:
    cursor = connection.execute(f"PRAGMA table_info({table})")
    return any(row[1] == column for row in cursor.fetchall())


def apply_schema(connection) -> None:
    """Execute schema statements on the provided SQLite connection."""
    cursor = connection.cursor()
    for stmt in SCHEMA_STATEMENTS:
        cursor.execute(stmt)
    for table, column, definition in ADDITIONAL_COLUMNS:
        if not _column_exists(connection, table, column):
            cursor.execute(f"ALTER TABLE {table} ADD COLUMN {column} {definition}")
    connection.commit()

__all__ = ["SCHEMA_STATEMENTS", "apply_schema"]



###############################################################################
### FILE: search.py
###############################################################################
"""Semantic search helpers over stored embeddings."""
from __future__ import annotations

from array import array
from dataclasses import dataclass, replace
import math
import re
import unicodedata
from typing import Mapping, Sequence

from .db import sqlite_session
from .embedding import EmbeddingClient


@dataclass(frozen=True)
class SemanticMatch:
    """Result item returned by semantic_search."""

    entity_type: str
    entity_id: str
    agregado_id: int | None
    score: float
    model: str


@dataclass(frozen=True)
class SemanticResult:
    """Semantic match enriched with human-readable metadata."""

    entity_type: str
    entity_id: str
    agregado_id: int | None
    score: float
    model: str
    title: str
    description: str | None
    metadata: Mapping[str, str]
    lexical_score: float = 0.0
    combined_score: float = 0.0
    lexical_table_score: float = 0.0
    lexical_children_score: float = 0.0
    child_matches: Sequence["ChildMatch"] = ()


@dataclass(frozen=True)
class SearchFilters:
    """Structured filters to refine search results."""

    min_municipalities: int | None = None
    requires_national_munis: bool = False
    subject_contains: str | None = None
    survey_contains: str | None = None
    period_start: int | None = None
    period_end: int | None = None
    has_variables: Sequence[int] | None = None
    has_classifications: Sequence[int] | None = None
    has_categories: Sequence[tuple[int, int]] | None = None


@dataclass(frozen=True)
class ChildMatch:
    """Lexical evidence from child entities used to justify a table hit."""

    entity_type: str
    entity_id: str
    title: str
    lexical_score: float
    metadata: Mapping[str, str]


def _decode_vector(blob: bytes, dimension: int) -> list[float]:
    values = array("f")
    values.frombytes(blob)
    if len(values) < dimension:
        return []
    if len(values) > dimension:
        values = values[:dimension]
    return [float(v) for v in values]


def _vector_norm(vector: Sequence[float]) -> float:
    return math.sqrt(sum(value * value for value in vector))


def _cosine_similarity(query: Sequence[float], candidate: Sequence[float], query_norm: float) -> float:
    candidate_norm = _vector_norm(candidate)
    if query_norm == 0 or candidate_norm == 0:
        return 0.0
    dot = sum(q * c for q, c in zip(query, candidate))
    return dot / (query_norm * candidate_norm)


def _strip_accents(value: str) -> str:
    normalized = unicodedata.normalize("NFKD", value)
    return "".join(ch for ch in normalized if ord(ch) < 128)


def _tokenize(text: str) -> list[str]:
    folded = _strip_accents(text.lower())
    tokens = [token for token in re.findall(r"\w+", folded) if len(token) >= 3]
    return tokens


def _normalize_substring(value: str | None) -> str:
    return (value or "").strip().lower()


def semantic_search(
    query: str,
    *,
    entity_types: Sequence[str] | None = None,
    limit: int = 10,
    embedding_client: EmbeddingClient | None = None,
    model: str | None = None,
) -> list[SemanticMatch]:
    """Return the best-matching stored embedding rows for the given query text."""

    if limit <= 0:
        return []

    client = embedding_client or EmbeddingClient(model=model)
    model_name = model or client.model
    query_vector = [float(value) for value in client.embed_text(query, model=model_name)]
    if not query_vector:
        return []

    query_norm = _vector_norm(query_vector)
    if query_norm == 0:
        return []

    sql = (
        "SELECT entity_type, entity_id, agregado_id, model, dimension, vector "
        "FROM embeddings WHERE model = ?"
    )
    params: list[object] = [model_name]
    if entity_types:
        placeholders = ", ".join("?" for _ in entity_types)
        sql += f" AND entity_type IN ({placeholders})"
        params.extend(entity_types)

    matches: list[SemanticMatch] = []
    with sqlite_session() as conn:
        for row in conn.execute(sql, params):
            candidate = _decode_vector(row["vector"], row["dimension"])
            if not candidate or len(candidate) != len(query_vector):
                continue
            score = _cosine_similarity(query_vector, candidate, query_norm)
            matches.append(
                SemanticMatch(
                    entity_type=row["entity_type"],
                    entity_id=row["entity_id"],
                    agregado_id=row["agregado_id"],
                    score=score,
                    model=row["model"],
                )
            )

    matches.sort(key=lambda item: item.score, reverse=True)
    return matches[:limit]


def _filters_to_sql(
    filters: SearchFilters | None,
    *,
    table_alias: str | None = None,
) -> tuple[list[str], list[object]]:
    if filters is None:
        return [], []
    conditions: list[str] = []
    params: list[object] = []
    prefix = f"{table_alias}." if table_alias else ""
    id_ref = f"{table_alias}.id" if table_alias else "id"
    if filters.min_municipalities is not None:
        conditions.append(f"{prefix}municipality_locality_count >= ?")
        params.append(int(filters.min_municipalities))
    if filters.requires_national_munis:
        conditions.append(f"{prefix}covers_national_municipalities = 1")
    if filters.subject_contains:
        conditions.append(f"LOWER({prefix}assunto) LIKE ?")
        params.append(f"%{filters.subject_contains.lower()}%")
    if filters.survey_contains:
        conditions.append(f"LOWER({prefix}pesquisa) LIKE ?")
        params.append(f"%{filters.survey_contains.lower()}%")
    if filters.period_start is not None:
        conditions.append(
            f"({prefix}periodo_fim IS NULL OR CAST({prefix}periodo_fim AS INTEGER) >= ?)"
        )
        params.append(int(filters.period_start))
    if filters.period_end is not None:
        conditions.append(
            f"({prefix}periodo_inicio IS NULL OR CAST({prefix}periodo_inicio AS INTEGER) <= ?)"
        )
        params.append(int(filters.period_end))
    for variable_id in filters.has_variables or ():
        conditions.append(
            f"EXISTS (SELECT 1 FROM variables v WHERE v.agregado_id = {id_ref} AND v.id = ?)"
        )
        params.append(int(variable_id))
    for classification_id in filters.has_classifications or ():
        conditions.append(
            f"EXISTS (SELECT 1 FROM classifications c WHERE c.agregado_id = {id_ref} AND c.id = ?)"
        )
        params.append(int(classification_id))
    for category in filters.has_categories or ():
        classification_id, category_id = category
        conditions.append(
            "EXISTS (SELECT 1 FROM categories cat WHERE cat.agregado_id = {id_ref} "
            "AND cat.classification_id = ? AND cat.categoria_id = ?)".format(id_ref=id_ref)
        )
        params.extend([int(classification_id), int(category_id)])
    return conditions, params


def _lexical_candidates(
    conn,
    tokens: Sequence[str],
    filters: SearchFilters | None,
    limit: int,
) -> list[tuple[int, float]]:
    if not tokens or limit <= 0:
        return []

    conditions, params = _filters_to_sql(filters, table_alias="a")
    token_clauses: list[str] = []
    for token in tokens:
        pattern = f"%{token}%"
        token_clauses.append(
            "(LOWER(a.nome) LIKE ? OR LOWER(a.pesquisa) LIKE ? OR LOWER(a.assunto) LIKE ?)"
        )
        params.extend([pattern, pattern, pattern])

    where_parts = conditions[:]
    if token_clauses:
        where_parts.append(" AND ".join(token_clauses))

    sql = "SELECT a.id, a.nome, a.pesquisa, a.assunto FROM agregados a"
    if where_parts:
        sql += " WHERE " + " AND ".join(where_parts)
    sql += " LIMIT ?"
    params.append(int(limit))

    rows = conn.execute(sql, params).fetchall()
    results: list[tuple[int, float]] = []
    for row in rows:
        blob = " ".join(
            part for part in [row["nome"], row["pesquisa"], row["assunto"]] if part
        ).lower()
        if not blob:
            continue
        hits = sum(1 for token in tokens if token in blob)
        if hits == 0:
            continue
        lexical_score = hits / len(tokens)
        results.append((int(row["id"]), float(lexical_score)))
    return results


def _compute_lexical_score(
    tokens: Sequence[str],
    text_parts: Sequence[str],
) -> float:
    if not tokens:
        return 0.0
    blob = " ".join(part for part in text_parts if part).lower()
    if not blob:
        return 0.0
    hits = sum(1 for token in tokens if token in blob)
    return hits / len(tokens)


def _combine_scores(semantic_score: float, lexical_score: float) -> float:
    if semantic_score <= 0 and lexical_score <= 0:
        return 0.0
    if semantic_score <= 0:
        return 0.45 * lexical_score
    if lexical_score <= 0:
        return semantic_score
    return 0.65 * semantic_score + 0.35 * lexical_score


DEFAULT_WEIGHTS: Mapping[str, float] = {
    "sem": 0.62,
    "lex_table": 0.18,
    "lex_children": 0.20,
}


def _resolve_weights(overrides: Mapping[str, float] | None) -> Mapping[str, float]:
    if not overrides:
        return DEFAULT_WEIGHTS
    merged = dict(DEFAULT_WEIGHTS)
    for key, value in overrides.items():
        if key in merged and value >= 0:
            merged[key] = float(value)
    return merged


def _lexical_children(
    conn,
    tokens: Sequence[str],
    filters: SearchFilters | None,
    limit: int,
    child_types: Sequence[str],
) -> list[tuple[int, ChildMatch]]:
    if not tokens or not child_types or limit <= 0:
        return []

    overfetch = max(limit, 50)
    results: list[tuple[int, ChildMatch]] = []
    normalized_types = [child.lower() for child in child_types]

    if "variable" in normalized_types:
        results.extend(
            _lexical_child_query(
                conn,
                tokens,
                filters,
                overfetch,
                sql="""
                    SELECT v.agregado_id AS table_id,
                           v.id AS child_id,
                           v.nome,
                           v.unidade,
                           NULL AS classification_id,
                           NULL AS category_id
                    FROM variables v
                    JOIN agregados a ON a.id = v.agregado_id
                """,
                fields=["v.nome", "v.unidade"],
                entity_type="variable",
            )
        )

    if "classification" in normalized_types:
        results.extend(
            _lexical_child_query(
                conn,
                tokens,
                filters,
                overfetch,
                sql="""
                    SELECT c.agregado_id AS table_id,
                           c.id AS child_id,
                           c.nome,
                           NULL AS unidade,
                           NULL AS classification_id,
                           NULL AS category_id
                    FROM classifications c
                    JOIN agregados a ON a.id = c.agregado_id
                """,
                fields=["c.nome"],
                entity_type="classification",
            )
        )

    if "category" in normalized_types:
        results.extend(
            _lexical_child_query(
                conn,
                tokens,
                filters,
                overfetch,
                sql="""
                    SELECT cat.agregado_id AS table_id,
                           cat.categoria_id AS child_id,
                           cat.nome,
                           cat.unidade,
                           cat.classification_id,
                           NULL AS category_id
                    FROM categories cat
                    JOIN agregados a ON a.id = cat.agregado_id
                """,
                fields=["cat.nome", "cat.unidade"],
                entity_type="category",
            )
        )

    return results


def _lexical_child_query(
    conn,
    tokens: Sequence[str],
    filters: SearchFilters | None,
    limit: int,
    sql: str,
    fields: Sequence[str],
    entity_type: str,
) -> list[tuple[int, ChildMatch]]:
    conditions, params = _filters_to_sql(filters, table_alias="a")
    token_clauses: list[str] = []
    for token in tokens:
        pattern = f"%{token}%"
        field_clauses = [f"LOWER({field}) LIKE ?" for field in fields]
        token_clauses.append("(" + " OR ".join(field_clauses) + ")")
        params.extend([pattern] * len(fields))

    where_segments = conditions[:]
    if token_clauses:
        where_segments.append(" AND ".join(token_clauses))

    query = sql
    if where_segments:
        query += " WHERE " + " AND ".join(where_segments)
    query += " LIMIT ?"
    params.append(int(limit))

    rows = conn.execute(query, params).fetchall()
    hits: list[tuple[int, ChildMatch]] = []
    for row in rows:
        name = row["nome"] or ""
        keys = set(row.keys())
        unit = row["unidade"] if "unidade" in keys else None
        score = _compute_lexical_score(tokens, [name, unit or ""])
        if score <= 0:
            continue
        metadata: dict[str, str] = {}
        if unit:
            metadata["unit"] = str(unit)
        classification_id = row["classification_id"] if "classification_id" in keys else None
        if classification_id is not None:
            metadata["classification_id"] = str(classification_id)
        child_id = str(row["child_id"])
        match = ChildMatch(
            entity_type=entity_type,
            entity_id=child_id,
            title=str(name).strip() or f"{entity_type.title()} {child_id}",
            lexical_score=float(score),
            metadata=metadata,
        )
        hits.append((int(row["table_id"]), match))
    return hits


def _aggregate_children_by_table(
    child_hits: Sequence[tuple[int, ChildMatch]],
    tokens: Sequence[str],
    max_matches: int,
) -> dict[int, dict[str, object]]:
    if not child_hits:
        return {}

    aggregated: dict[int, dict[str, object]] = {}
    for table_id, match in child_hits:
        bucket = aggregated.setdefault(table_id, {"matches": []})
        bucket["matches"].append(match)

    token_count = max(1, len(tokens))
    for table_id, payload in aggregated.items():
        matches: list[ChildMatch] = sorted(
            payload["matches"],
            key=lambda item: item.lexical_score,
            reverse=True,
        )
        top_matches = matches[:max_matches] if max_matches > 0 else matches
        total = sum(item.lexical_score for item in top_matches)
        denom = max(1, len(top_matches))
        payload["score"] = total / denom
        payload["matches"] = top_matches
    return aggregated


def _build_agregado_summary(row) -> tuple[str, str | None, dict[str, str]]:
    title = str(row["nome"] or "").strip() or f"Table {row['id']}"
    parts: list[str] = []
    metadata: dict[str, str] = {"table_id": str(row["id"])}
    if row["pesquisa"]:
        metadata["survey"] = row["pesquisa"]
        parts.append(str(row["pesquisa"]))
    if row["assunto"]:
        metadata["subject"] = row["assunto"]
        parts.append(str(row["assunto"]))
    freq = row["freq"]
    if freq:
        metadata["frequency"] = str(freq)
        parts.append(f"Frequency: {freq}")
    start = row["periodo_inicio"]
    end = row["periodo_fim"]
    if start or end:
        metadata["period_start"] = str(start) if start is not None else ""
        metadata["period_end"] = str(end) if end is not None else ""
        if start and end and start != end:
            parts.append(f"Period {start}–{end}")
        elif start or end:
            parts.append(f"Period {start or end}")
    if row["url"]:
        metadata["url"] = row["url"]
    muni_count = row["municipality_locality_count"] if "municipality_locality_count" in row.keys() else None
    if muni_count is not None and int(muni_count) > 0:
        metadata["municipality_locality_count"] = str(int(muni_count))
        parts.append(f"Municipalities: {int(muni_count):,}")
    covers_national = (
        row["covers_national_municipalities"] if "covers_national_municipalities" in row.keys() else None
    )
    if covers_national is not None:
        metadata["covers_national_municipalities"] = str(int(covers_national))
        if int(covers_national):
            parts.append("National municipal coverage")
    description = " | ".join(filter(None, (part.strip() for part in parts))) or None
    return title, description, metadata


def _build_variable_summary(row) -> tuple[str, str | None, dict[str, str]]:
    variable_name = str(row["nome"] or "").strip() or f"Variable {row['id']}"
    title = f"{variable_name}"
    parts: list[str] = []
    metadata: dict[str, str] = {
        "variable_id": str(row["id"]),
        "table_id": str(row["agregado_id"]),
    }
    if row["agregado_nome"]:
        parts.append(f"Table {row['agregado_id']}: {row['agregado_nome']}")
        metadata["table_name"] = row["agregado_nome"]
    if row["unidade"]:
        parts.append(f"Unit: {row['unidade']}")
        metadata["unit"] = row["unidade"]
    if row["pesquisa"]:
        metadata["survey"] = row["pesquisa"]
    if "municipality_locality_count" in row.keys() and int(row["municipality_locality_count"] or 0) > 0:
        metadata["municipality_locality_count"] = str(int(row["municipality_locality_count"]))
        parts.append(f"Municipalities: {int(row['municipality_locality_count']):,}")
    if "covers_national_municipalities" in row.keys() and int(row["covers_national_municipalities"] or 0):
        metadata["covers_national_municipalities"] = str(int(row["covers_national_municipalities"]))
        parts.append("National municipal coverage")
    description = " | ".join(parts) or None
    return title, description, metadata


def _build_classification_summary(row) -> tuple[str, str | None, dict[str, str]]:
    name = str(row["nome"] or "").strip() or f"Classification {row['id']}"
    title = name
    metadata: dict[str, str] = {
        "classification_id": str(row["id"]),
        "table_id": str(row["agregado_id"]),
    }
    parts: list[str] = []
    if row["agregado_nome"]:
        parts.append(f"Table {row['agregado_id']}: {row['agregado_nome']}")
        metadata["table_name"] = row["agregado_nome"]
    if "municipality_locality_count" in row.keys() and int(row["municipality_locality_count"] or 0) > 0:
        metadata["municipality_locality_count"] = str(int(row["municipality_locality_count"]))
    if "covers_national_municipalities" in row.keys() and int(row["covers_national_municipalities"] or 0):
        metadata["covers_national_municipalities"] = str(int(row["covers_national_municipalities"]))
    if row["sumarizacao_status"] is not None:
        metadata["summary_enabled"] = str(bool(row["sumarizacao_status"]))
        parts.append(f"Summarization: {'on' if row['sumarizacao_status'] else 'off'}")
    description = " | ".join(parts) or None
    return title, description, metadata


def _build_category_summary(row) -> tuple[str, str | None, dict[str, str]]:
    name = str(row["nome"] or "").strip() or f"Category {row['categoria_id']}"
    title = name
    metadata: dict[str, str] = {
        "category_id": str(row["categoria_id"]),
        "classification_id": str(row["classification_id"]),
        "table_id": str(row["agregado_id"]),
    }
    parts: list[str] = []
    if row["classification_nome"]:
        parts.append(f"Classification {row['classification_id']}: {row['classification_nome']}")
        metadata["classification_name"] = row["classification_nome"]
    if row["agregado_nome"]:
        parts.append(f"Table {row['agregado_id']}: {row['agregado_nome']}")
        metadata["table_name"] = row["agregado_nome"]
    if "municipality_locality_count" in row.keys() and int(row["municipality_locality_count"] or 0) > 0:
        metadata["municipality_locality_count"] = str(int(row["municipality_locality_count"]))
        parts.append(f"Municipalities: {int(row['municipality_locality_count']):,}")
    if "covers_national_municipalities" in row.keys() and int(row["covers_national_municipalities"] or 0):
        metadata["covers_national_municipalities"] = str(int(row["covers_national_municipalities"]))
        parts.append("National municipal coverage")
    if row["unidade"]:
        parts.append(f"Unit: {row['unidade']}")
        metadata["unit"] = row["unidade"]
    if row["nivel"] is not None:
        metadata["level"] = str(row["nivel"])
        parts.append(f"Level: {row['nivel']}")
    description = " | ".join(parts) or None
    return title, description, metadata


def _enrich_match(conn, match: SemanticMatch) -> tuple[SemanticResult, Mapping[str, object] | None]:
    if match.entity_type == "agregado" and match.agregado_id is not None:
        row = conn.execute(
            """
            SELECT id, nome, pesquisa, assunto, url, freq, periodo_inicio, periodo_fim,
                   municipality_locality_count, covers_national_municipalities
            FROM agregados
            WHERE id = ?
            """,
            (match.agregado_id,),
        ).fetchone()
        if row:
            title, description, metadata = _build_agregado_summary(row)
            return (
                SemanticResult(
                    **match.__dict__,
                    title=title,
                    description=description,
                    metadata=metadata,
                ),
                row,
            )

    if match.entity_type == "variable" and match.agregado_id is not None:
        parts = match.entity_id.split(":")
        variable_id = int(parts[-1]) if parts and parts[-1].isdigit() else None
        if variable_id is not None:
            row = conn.execute(
                """
                SELECT v.id,
                       v.nome,
                       v.unidade,
                       v.agregado_id,
                       a.nome AS agregado_nome,
                       a.pesquisa,
                       a.covers_national_municipalities,
                       a.municipality_locality_count
                FROM variables v
                JOIN agregados a ON a.id = v.agregado_id
                WHERE v.agregado_id = ? AND v.id = ?
                """,
                (match.agregado_id, variable_id),
            ).fetchone()
            if row:
                title, description, metadata = _build_variable_summary(row)
                return (
                    SemanticResult(
                        **match.__dict__,
                        title=title,
                        description=description,
                        metadata=metadata,
                    ),
                    None,
                )

    if match.entity_type == "classification" and match.agregado_id is not None:
        parts = match.entity_id.split(":")
        classification_id = int(parts[-1]) if parts and parts[-1].isdigit() else None
        if classification_id is not None:
            row = conn.execute(
                """
                SELECT c.id,
                       c.nome,
                       c.agregado_id,
                       c.sumarizacao_status,
                       a.nome AS agregado_nome,
                       a.covers_national_municipalities,
                       a.municipality_locality_count
                FROM classifications c
                JOIN agregados a ON a.id = c.agregado_id
                WHERE c.agregado_id = ? AND c.id = ?
                """,
                (match.agregado_id, classification_id),
            ).fetchone()
            if row:
                title, description, metadata = _build_classification_summary(row)
                return (
                    SemanticResult(
                        **match.__dict__,
                        title=title,
                        description=description,
                        metadata=metadata,
                    ),
                    None,
                )

    if match.entity_type == "category" and match.agregado_id is not None:
        parts = match.entity_id.split(":")
        if len(parts) >= 3 and parts[-1].isdigit() and parts[-2].isdigit():
            classification_id = int(parts[-2])
            category_id = int(parts[-1])
            row = conn.execute(
                """
                SELECT cat.agregado_id,
                       cat.classification_id,
                       cat.categoria_id,
                       cat.nome,
                       cat.unidade,
                       cat.nivel,
                       cls.nome AS classification_nome,
                       ag.nome AS agregado_nome,
                       ag.covers_national_municipalities,
                       ag.municipality_locality_count
                FROM categories cat
                JOIN classifications cls
                  ON cls.agregado_id = cat.agregado_id AND cls.id = cat.classification_id
                JOIN agregados ag ON ag.id = cat.agregado_id
                WHERE cat.agregado_id = ? AND cat.classification_id = ? AND cat.categoria_id = ?
                """,
                (match.agregado_id, classification_id, category_id),
            ).fetchone()
            if row:
                title, description, metadata = _build_category_summary(row)
                return (
                    SemanticResult(
                        **match.__dict__,
                        title=title,
                        description=description,
                        metadata=metadata,
                    ),
                    None,
                )

    # Fallback when we could not resolve metadata
    return (
        SemanticResult(
            **match.__dict__,
            title=f"{match.entity_type.title()} {match.entity_id}",
            description=None,
            metadata={},
        ),
        None,
    )


def semantic_search_with_metadata(
    query: str,
    *,
    entity_types: Sequence[str] | None = None,
    limit: int = 10,
    embedding_client: EmbeddingClient | None = None,
    model: str | None = None,
) -> list[SemanticResult]:
    """Run semantic search and attach friendly metadata for presentation."""

    matches = semantic_search(
        query,
        entity_types=entity_types,
        limit=limit,
        embedding_client=embedding_client,
        model=model,
    )
    if not matches:
        return []

    enriched: list[SemanticResult] = []
    with sqlite_session() as conn:
        for match in matches:
            result, _ = _enrich_match(conn, match)
            enriched.append(replace(result, combined_score=result.score))
    return enriched


def _augment_agregado_metadata(conn, agregado_id: int, metadata: dict[str, str]) -> None:
    variable_rows = conn.execute(
        "SELECT nome FROM variables WHERE agregado_id = ? ORDER BY id LIMIT 5",
        (agregado_id,),
    ).fetchall()
    if variable_rows:
        metadata["variables_sample"] = "; ".join(row["nome"] for row in variable_rows if row["nome"])
        count = conn.execute(
            "SELECT COUNT(*) FROM variables WHERE agregado_id = ?",
            (agregado_id,),
        ).fetchone()[0]
        metadata["variables_count"] = str(int(count))

    classification_rows = conn.execute(
        "SELECT nome FROM classifications WHERE agregado_id = ? ORDER BY id LIMIT 5",
        (agregado_id,),
    ).fetchall()
    if classification_rows:
        metadata["classifications_sample"] = "; ".join(
            row["nome"] for row in classification_rows if row["nome"]
        )
        count = conn.execute(
            "SELECT COUNT(*) FROM classifications WHERE agregado_id = ?",
            (agregado_id,),
        ).fetchone()[0]
        metadata["classifications_count"] = str(int(count))

    level_rows = conn.execute(
        """
        SELECT level_id, level_name, level_type, locality_count
        FROM agregados_levels
        WHERE agregado_id = ?
        ORDER BY locality_count DESC, level_id
        """,
        (agregado_id,),
    ).fetchall()
    if level_rows:
        samples: list[str] = []
        for row in level_rows[:5]:
            level_id = row["level_id"]
            level_name = row["level_name"] or ""
            level_type = row["level_type"] or ""
            locality_count = row["locality_count"]
            snippet_parts = [level_id]
            if level_name:
                snippet_parts.append(level_name)
            if level_type:
                snippet_parts.append(level_type)
            if locality_count is not None:
                snippet_parts.append(f"{int(locality_count):,}")
            samples.append(" / ".join(str(part) for part in snippet_parts if part))
        metadata["levels_sample"] = "; ".join(samples)
        metadata["levels_count"] = str(len(level_rows))


def _fetch_agregado_row(conn, agregado_id: int | None):
    if agregado_id is None:
        return None
    return conn.execute(
        """
        SELECT id, nome, pesquisa, assunto, url, freq, periodo_inicio, periodo_fim,
               municipality_locality_count, covers_national_municipalities
        FROM agregados
        WHERE id = ?
        """,
        (agregado_id,),
    ).fetchone()


def _passes_filters(row, filters: SearchFilters | None) -> bool:
    if filters is None or row is None:
        return True

    def _to_int(value) -> int | None:
        if value is None:
            return None
        try:
            return int(str(value))
        except (TypeError, ValueError):
            return None

    if filters.min_municipalities is not None:
        count = _to_int(row["municipality_locality_count"])
        if count is None or count < filters.min_municipalities:
            return False
    if filters.requires_national_munis and int(row["covers_national_municipalities"] or 0) != 1:
        return False
    if filters.subject_contains and filters.subject_contains.lower() not in _normalize_substring(row["assunto"]):
        return False
    if filters.survey_contains and filters.survey_contains.lower() not in _normalize_substring(row["pesquisa"]):
        return False

    start = _to_int(row["periodo_inicio"])
    end = _to_int(row["periodo_fim"])
    if filters.period_start is not None and end is not None and end < filters.period_start:
        return False
    if filters.period_end is not None and start is not None and start > filters.period_end:
        return False
    return True


def hybrid_search(
    query: str,
    *,
    limit: int = 10,
    filters: SearchFilters | None = None,
    embedding_client: EmbeddingClient | None = None,
    model: str | None = None,
    child_types: Sequence[str] | None = None,
    max_child_matches: int = 6,
    weights: Mapping[str, float] | None = None,
) -> list[SemanticResult]:
    if limit <= 0:
        return []

    child_types = list(child_types) if child_types is not None else [
        "variable",
        "classification",
        "category",
    ]
    tokens = _tokenize(query)
    weight_map = _resolve_weights(weights)
    overfetch = max(limit * 5, 50)

    matches = semantic_search(
        query,
        entity_types=["agregado"],
        limit=overfetch,
        embedding_client=embedding_client,
        model=model,
    )

    table_candidates: dict[int, dict[str, object]] = {}

    def _ensure_entry(table_id: int) -> dict[str, object]:
        entry = table_candidates.get(table_id)
        if entry is None:
            entry = {
                "match": SemanticMatch(
                    entity_type="agregado",
                    entity_id=str(table_id),
                    agregado_id=table_id,
                    score=0.0,
                    model=model or "lexical",
                ),
                "semantic": 0.0,
                "lex_table": 0.0,
            }
            table_candidates[table_id] = entry
        return entry

    for match in matches:
        if match.agregado_id is None:
            continue
        entry = _ensure_entry(match.agregado_id)
        if match.score > entry["semantic"]:
            entry["match"] = match
        entry["semantic"] = max(entry["semantic"], match.score)

    with sqlite_session() as conn:
        lexical_candidates = _lexical_candidates(
            conn,
            tokens,
            filters,
            max(overfetch, 50),
        )
        for agregado_id, lexical_score in lexical_candidates:
            entry = _ensure_entry(agregado_id)
            entry["lex_table"] = max(float(entry.get("lex_table", 0.0)), float(lexical_score))

        child_hits = _lexical_children(
            conn,
            tokens,
            filters,
            max(overfetch, 200),
            child_types,
        )
        child_map = _aggregate_children_by_table(
            child_hits,
            tokens,
            max_child_matches,
        )

        for agregado_id in child_map.keys():
            _ensure_entry(agregado_id)

        enriched: list[SemanticResult] = []
        for table_id, payload in table_candidates.items():
            match: SemanticMatch = payload["match"]  # type: ignore[assignment]
            semantic_score = float(payload.get("semantic", 0.0))
            result, aggregator_row = _enrich_match(conn, match)

            if aggregator_row is None:
                aggregator_row = _fetch_agregado_row(conn, table_id)

            if not _passes_filters(aggregator_row, filters):
                continue

            if match.entity_type == "agregado" and aggregator_row is not None:
                metadata = dict(result.metadata)
                _augment_agregado_metadata(conn, table_id, metadata)
                result = replace(result, metadata=metadata)

            lexical_table_score = float(payload.get("lex_table", 0.0))
            lexical_from_text = _compute_lexical_score(
                tokens,
                [result.title or "", result.description or ""],
            )
            lexical_table_score = max(lexical_table_score, lexical_from_text)

            child_payload = child_map.get(table_id, {})
            lexical_children_score = float(child_payload.get("score", 0.0))
            child_matches = tuple(child_payload.get("matches", ()))

            if semantic_score <= 0:
                semantic_score = result.score
            result = replace(result, score=semantic_score)

            boost = 0.0
            if aggregator_row is not None:
                covers_national = int(aggregator_row["covers_national_municipalities"] or 0)
                if covers_national:
                    boost += 0.03
                if filters and filters.period_start is not None:
                    try:
                        period_end = int(str(aggregator_row["periodo_fim"])) if aggregator_row["periodo_fim"] else None
                    except (TypeError, ValueError):
                        period_end = None
                    if period_end is not None and period_end >= filters.period_start:
                        boost += 0.02

            combined = (
                weight_map.get("sem", 0.0) * semantic_score
                + weight_map.get("lex_table", 0.0) * lexical_table_score
                + weight_map.get("lex_children", 0.0) * lexical_children_score
                + boost
            )

            lexical_total = lexical_table_score + lexical_children_score
            result = replace(
                result,
                lexical_table_score=lexical_table_score,
                lexical_children_score=lexical_children_score,
                lexical_score=lexical_total,
                combined_score=combined,
                child_matches=child_matches,
            )
            enriched.append(result)

    enriched.sort(key=lambda item: item.combined_score, reverse=True)
    return enriched[:limit]


__all__ = [
    "SemanticMatch",
    "SemanticResult",
    "SearchFilters",
    "ChildMatch",
    "semantic_search",
    "semantic_search_with_metadata",
    "hybrid_search",
]



